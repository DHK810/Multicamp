{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 설정\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 엑셀파일 불러오기\n",
    "code_df = pd.read_csv('code_a.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장파일 함수 지정\n",
    "def news_crawling(new_code): #new_code = 종목코드\n",
    "    df_crawling = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    temp_page_url = 'https://finance.naver.com/item/news_news.nhn?code={}&page={}&sm=title_entity_id.basic&clusterId='.format(new_code, 1) \n",
    "    #request\n",
    "    \n",
    "    lastPageResp = requests.get(temp_page_url)\n",
    "    lastPagehtml = lastPageResp.text\n",
    "    \n",
    "    #soup초기화\n",
    "    soup = BeautifulSoup(lastPagehtml)\n",
    "    \n",
    "    href = soup.find('td', class_='pgRR').a['href']\n",
    "    url = parse.urlparse(href)\n",
    "    \n",
    "    #Http URL Parameter를 다루려면 parse_qs() 메서드를 사용\n",
    "    #parse_qs() 는 지정된 쿼리스트링을 해석하여 dict() 로 반환\n",
    "    url_query = parse.parse_qs(url.query)\n",
    "    \n",
    "    pg = url_query['page'][0]\n",
    "        \n",
    "    ##400page가 넘는 경우 반복되는 news만 나오므로 400 = pg\n",
    "    if int(pg) >= int(401):\n",
    "        pg = int(400)\n",
    "    else:\n",
    "        pg = pg\n",
    "    \n",
    "    ###      \n",
    "\n",
    "    for page in range(1,int(pg)+1):\n",
    "        #종목에서 더 많은 페이지를 설정하는 경우, 마지막페이지만 계속 반복\n",
    "        page_url = 'https://finance.naver.com/item/news_news.nhn?code={}&page={}&sm=title_entity_id.basic&clusterId='.format(new_code,page)\n",
    "        resp = requests.get(page_url)\n",
    "        html = resp.text\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        a_tags = soup.select('a.tit')\n",
    "        title_text=[]    \n",
    "        link_text=[]\n",
    "        for a_tag in a_tags:\n",
    "            title_text.append(a_tag.text)\n",
    "            link_text.append(a_tag['href'])\n",
    "\n",
    "        td_tags = soup.select('td.date')   \n",
    "        date_text=[]             \n",
    "        for td_tag in td_tags:\n",
    "            date_text.append(td_tag.string)  #.string하니까 시간만 정제됨!!!\n",
    "\n",
    "        test_dict={}\n",
    "        test_dict['date']=date_text\n",
    "        test_dict['link']=link_text\n",
    "        test_dict['title']=title_text  #title이라는 키값을 가지는 title_text 리스트의 요소들은  dict로 전환\n",
    "    \n",
    "        df_crawling = df_crawling.append(pd.DataFrame(test_dict), ignore_index=True)\n",
    "    return df_crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_crawling(new_code):\n",
    "    \n",
    "    period_year = 1      #오늘로부터 기간(year) \n",
    "    end_day = str(datetime.date.today())[:10]\n",
    "    start_day = str(int(end_day[:4])-period_year)+end_day[4:]\n",
    "    domain_url = 'https://www.wisereport.co.kr/wiseReport/reports/company.aspx'\n",
    "    cookies = {\n",
    "        'ASP.NET_SessionId': ''\n",
    "    }\n",
    "\n",
    "    page_num=0\n",
    "    report_list = []\n",
    "    for i in range(1,10000):\n",
    "        form_data = {\n",
    "            'startDT': start_day,\n",
    "            'endDt': end_day,\n",
    "            'langTyp': 1,\n",
    "            'searchTyp': None,\n",
    "            'searchVal': None,\n",
    "            'reVal': 'and a.cmp_cd = |{}|'.format(new_code),\n",
    "            'perPage': 25,\n",
    "            'curPage': i,\n",
    "            'sortCol': 'anl_dt',\n",
    "            'sortTyp': 'd',\n",
    "            'flag': None,\n",
    "            'flashYN': 1,\n",
    "            'userId': 'gaps5_000543'\n",
    "        }\n",
    "        resp = requests.post(domain_url, cookies = cookies, data = form_data)\n",
    "        soup = BeautifulSoup(resp.content,'html.parser')\n",
    "        tg_contents = soup.find('tbody')\n",
    "\n",
    "        if soup.find('tbody').find('tr').find('td').attrs['class'] == ['lst_center', 'end']:\n",
    "            break\n",
    "        else:\n",
    "            page_num+=1\n",
    "#             print('{}페이지를 가져오겠습니다.'.format(page_num))\n",
    "    \n",
    "        new_list=[]\n",
    "        for content in tg_contents.contents[:-2]:\n",
    "        \n",
    "            new_dict = {}\n",
    "\n",
    "            new_dict['title'] = content.find_all('td')[1].find('div').find('a').text\n",
    "\n",
    "            new_dict['time']  = content.find('td').text\n",
    "\n",
    "            summary_tag = content.find_all('td')[1].attrs['data-content']\n",
    "            summary_soup = BeautifulSoup(summary_tag)\n",
    "            summary_chunk = summary_soup.text\n",
    "            summary_list = summary_chunk.split('▶')[1:]\n",
    "            for i in range(len(summary_list)-1):\n",
    "                summary_list[i] = summary_list[i][:-2]\n",
    "            new_dict['summary'] = summary_list\n",
    "\n",
    "            new_list.append(new_dict)\n",
    "\n",
    "        report_list.extend(new_list)\n",
    "        \n",
    "    report_list = pd.DataFrame(report_list)\n",
    "    return report_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_crawling(new_code):\n",
    "    price_list = []\n",
    "    url = 'https://finance.naver.com/item/sise_day.nhn'\n",
    "    for num in range(1,25):\n",
    "        params = {\n",
    "            'code' : new_code,\n",
    "            'page' : '{}'.format(num)\n",
    "        }\n",
    "        resp = requests.get(url, params = params)\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        for tr in soup.find_all('tr')[2:14]:\n",
    "            new_dict = {}\n",
    "            td_list = tr.find_all('td')\n",
    "            if not td_list[0].text.strip():\n",
    "                continue\n",
    "            tdtext_list = ['날짜', '종가', '전일비', '시가', '고가', '저가', '거래량']\n",
    "            for i in range(7):\n",
    "                new_dict[tdtext_list[i]] = td_list[i].text.strip()\n",
    "            price_list.append(new_dict)\n",
    "\n",
    "    return pd.DataFrame(price_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "회사이름을 적어주세요 : 삼성전기\n",
      "저장하시겠습니까?(1-저장, 2-종료) : 1\n",
      "1\n",
      "저장 완료. 프로그램을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "# 회사이름 쳐서 코드 불러오기\n",
    "name = input('회사이름을 적어주세요 : ')\n",
    "new_name = name.upper()\n",
    "a = code_df[code_df['회사명'].isin([new_name])]\n",
    "\n",
    "while True:\n",
    "    if a.size == 0:\n",
    "        print('정확한 회사명을 입력하세요')\n",
    "        # 특정 문자열이 포함된 리스트 뽑기\n",
    "        compony_ex = code_df[code_df.회사명.str.contains(new_name) == True]\n",
    "        print(compony_ex)\n",
    "        name = input('회사이름을 적어주세요 : ')\n",
    "        new_name = name.upper()\n",
    "        a = code_df[code_df['회사명'].isin([new_name])]\n",
    "    else:\n",
    "        code = a['종목코드'].values[0]\n",
    "        new_code = '%06d' %code\n",
    "        break\n",
    "\n",
    "    \n",
    "#뉴스크롤링\n",
    "news = news_crawling(new_code)\n",
    "\n",
    "#가격크롤링\n",
    "price = price_crawling(new_code)\n",
    "\n",
    "# 리포트크롤링\n",
    "report = report_crawling(new_code)\n",
    "\n",
    "\n",
    "#저장, 종료 여부 묻기\n",
    "while True:\n",
    "    try:\n",
    "        num=int(input(\"저장하시겠습니까?(1-저장, 2-종료) : \"))\n",
    "        print(num)\n",
    "    \n",
    "        if not num in [1,2]:\n",
    "            print(\"잘못된 명령입니다. 다시 입력해주세요.\")\n",
    "            continue\n",
    "        \n",
    "        elif num == 1 :\n",
    "\n",
    "            BASE_DIR = os.getcwd()\n",
    "            SAVE_DB_DIR = os.path.join(BASE_DIR, new_name)\n",
    "    \n",
    "            if not os.path.exists(SAVE_DB_DIR):\n",
    "                os.makedirs(SAVE_DB_DIR)\n",
    "             \n",
    "            temp_name = f\"{new_name}_{new_code}_{str(datetime.date.today())}_report.csv\"\n",
    "            txt_name = os.path.join(SAVE_DB_DIR, temp_name)\n",
    "            report.to_csv(txt_name, encoding='cp949')\n",
    "            \n",
    "            temp_name = f\"{new_name}_{new_code}_{str(datetime.date.today())}_price.csv\"\n",
    "            txt_name = os.path.join(SAVE_DB_DIR, temp_name)\n",
    "            price.to_csv(txt_name, encoding='cp949')\n",
    "            \n",
    "            temp_name = f\"{new_name}_{new_code}_{str(datetime.date.today())}_news.xlsx\"\n",
    "            txt_name = os.path.join(SAVE_DB_DIR, temp_name)\n",
    "            news.to_excel(txt_name, sheet_name='sheet1')\n",
    "        \n",
    "            print(\"저장 완료. 프로그램을 종료합니다.\")\n",
    "            break    \n",
    "    \n",
    "        else:\n",
    "            print(\"프로그램을 종료합니다.\")\n",
    "            break\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"잘못된 명령입니다. 다시 입력해주세요.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
