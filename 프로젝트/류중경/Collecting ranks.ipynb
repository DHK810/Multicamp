{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나라별 유명 검색 사이트에서 선택한 나라들의 탑 순위 및 원하는 검색들의 결과 보여주는. \n",
    "\n",
    "# naver.com네이버 - 한국 \n",
    "# https://tieba.baidu.com/index.html - 중국\n",
    "# https://www.yahoo.com/ - 미국\n",
    "# https://www.yahoo.co.jp/ - 일본\n",
    "\n",
    "# 위의 각 검색사이트의 현재 검색순위1~10등, 그리고 그것을 구글에서 검색한 이미지를 같이 보여준다.\n",
    "# cvc파일로 저장한다.\n",
    "# 중국 미국 일본에서 검색한 것은 번역기로 번역한 것도 같이 보여준다.\n",
    "# 원문을 컴퓨터가 읽어주기.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from gtts import gTTS\n",
    "from selenium import webdriver\n",
    "new_ul_naver = []\n",
    "new_ul_yahoo = []\n",
    "naver_list = []\n",
    "baidu_list = []\n",
    "yahoo_list = []\n",
    "new_ul_yahoo = []\n",
    "yahoo_jp_list = []\n",
    "rank = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Naver searching\n",
    "NAVER_URL = \"https://www.naver.com\"\n",
    "resp_naver = requests.get(NAVER_URL)\n",
    "soup_naver = BeautifulSoup(resp_naver.content, 'html.parser')\n",
    "ul_contents_naver = soup_naver.find('div', class_='ah_roll_area PM_CL_realtimeKeyword_rolling').find('ul')\n",
    "for content in ul_contents_naver.contents:\n",
    "    if not str(content).strip():\n",
    "        continue\n",
    "    new_ul_naver.append(content)\n",
    "for li in range(0,10):\n",
    "    a_tag = new_ul_naver[li].find('a').find('span')\n",
    "    naver_list.append(a_tag.find_next().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baidu searching\n",
    "BAIDU_URL = \"https://tieba.baidu.com/hottopic/browse/topicList\"\n",
    "resp = requests.get(BAIDU_URL)\n",
    "temp1 = resp.json()\n",
    "temp2 = temp1.get(\"data\")\n",
    "temp3 = temp2.get(\"bang_topic\")\n",
    "temp4 = temp3.get(\"topic_list\")\n",
    "for counting in range(0,10):\n",
    "    temp5 = temp4[counting]\n",
    "    baidu_list.append(temp5.get(\"topic_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yahoo searching\n",
    "YAHOO_URL = \"https://www.yahoo.com/\"\n",
    "resp = requests.get(YAHOO_URL)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "ul_contents = soup.find('ul', class_='Pos(r) Mt(10px)').find('li').find('ul')\n",
    "for content in ul_contents.contents:\n",
    "    if not str(content).strip():\n",
    "        continue\n",
    "    new_ul_yahoo.append(content)\n",
    "for li in new_ul_yahoo:\n",
    "    a_tag = li.find('a').find('span')\n",
    "    yahoo_list.append(a_tag.find_next().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yahoo_jp searching\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\dty\\\\Downloads\\\\chromedriver_win32\\\\chromedriver\")\n",
    "driver.get(\"https://search.yahoo.co.jp/realtime\")\n",
    "\n",
    "for x in range(1,3):\n",
    "    for y in range(1,6):\n",
    "        daily1 = 'daily_raking_' + str(5*(x-1) + y)\n",
    "        rank.append(daily1)\n",
    "        rank[(5*(x-1) + y - 1)] = driver.find_element_by_xpath('//*[@id=\"Te\"]/div[2]/div[' + str(x) + ']/div[' + str(y) + ']/p/a')\n",
    "        yahoo_jp_list.append(rank[(5*(x-1) + y - 1)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching_images_and_save(address, name_list, image_ad, start_from, image_ad2, saving_name):\n",
    "    for listing in range(0,10):\n",
    "        driver.get(address+ name_list[listing])\n",
    "        driver.implicitly_wait(10)\n",
    "        for counting in range(0,4):\n",
    "            elem = driver.find_element_by_xpath(image_ad+ str(counting+start_from) + image_ad2)\n",
    "            elem.screenshot_as_png\n",
    "            filename = saving_name+ str(listing)+ str(counting) +'.png'\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(elem.screenshot_as_png)\n",
    "\n",
    "searching_images_and_save(\"https://search.yahoo.co.jp/image/search?p=\", yahoo_jp_list, '//*[@id=\"gridlist\"]/div[', 1, ']','yahoo_jp_img')\n",
    "searching_images_and_save(\"https://images.search.yahoo.com/search/images?p=\", yahoo_list, '//*[@id=\"resitem-', 0, '\"]', 'yahoo_img')\n",
    "searching_images_and_save(\"http://image.baidu.com/search/index?tn=baiduimage&word=\", baidu_list, '//*[@id=\"imgid\"]/div/ul/li[', 1, ']', 'baidu_img')\n",
    "searching_images_and_save(\"https://search.naver.com/search.naver?where=image&query=\", naver_list, '//*[@id=\"_sau_imageTab\"]/div[2]/div[2]/div[', 1, ']', 'naver_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_list(name_list, lg, name):\n",
    "    for ct in range(0,10):\n",
    "        text = name_list[ct]\n",
    "        tts = gTTS(text=text, lang= \"{}\".format(lg))\n",
    "        tts.save(str(name) + str(ct) +\".mp3\")\n",
    "        \n",
    "reading_list(yahoo_list, 'en', 'yahoo')\n",
    "reading_list(naver_list, 'ko', 'naver')\n",
    "reading_list(baidu_list, 'zh-cn', 'baidu')\n",
    "reading_list(yahoo_jp_list, 'ja', 'yahoo_jr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_jp = []\n",
    "tr_y = []\n",
    "tr_b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct in range(0,10):\n",
    "    driver.get(\"https://translate.google.co.kr/?hl=ko\")\n",
    "    driver.implicitly_wait(10)\n",
    "    search_elem = driver.find_element_by_xpath('//*[@id=\"source\"]')\n",
    "    search_elem.send_keys(yahoo_jp_list[ct])\n",
    "    elem = driver.find_element_by_xpath('/html/body/div[2]/div[1]/div[2]/div[1]/div[1]/div[2]/div[3]/div[1]/div[2]/div/span[1]/span[1]')\n",
    "    tr_jp.append(elem.text)\n",
    "for ct in range(0,10):\n",
    "    driver.get(\"https://translate.google.co.kr/?hl=ko\")\n",
    "    driver.implicitly_wait(10)\n",
    "    search_elem = driver.find_element_by_xpath('//*[@id=\"source\"]')\n",
    "    search_elem.send_keys(yahoo_list[ct])\n",
    "    elem = driver.find_element_by_xpath('/html/body/div[2]/div[1]/div[2]/div[1]/div[1]/div[2]/div[3]/div[1]/div[2]/div/span[1]/span[1]')\n",
    "    tr_y.append(elem.text)\n",
    "for ct in range(0,10):\n",
    "    driver.get(\"https://translate.google.co.kr/?hl=ko\")\n",
    "    driver.implicitly_wait(10)\n",
    "    search_elem = driver.find_element_by_xpath('//*[@id=\"source\"]')\n",
    "    search_elem.send_keys(baidu_list[ct])\n",
    "    elem = driver.find_element_by_xpath('/html/body/div[2]/div[1]/div[2]/div[1]/div[1]/div[2]/div[3]/div[1]/div[2]/div/span[1]/span[1]')\n",
    "    tr_b.append(elem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Naver': naver_list, 'Baidu' : baidu_list, 'Yahoo': yahoo_list, 'Yahoo_jp': yahoo_jp_list, 'tr_jp': tr_jp, 'tr_y': tr_y, 'tr_b': tr_b},index = range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naver</th>\n",
       "      <th>Baidu</th>\n",
       "      <th>Yahoo</th>\n",
       "      <th>Yahoo_jp</th>\n",
       "      <th>tr_jp</th>\n",
       "      <th>tr_y</th>\n",
       "      <th>tr_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이희호</td>\n",
       "      <td>曝林峯求婚张馨月</td>\n",
       "      <td>Xbox One X</td>\n",
       "      <td>ドクターX</td>\n",
       "      <td>닥터</td>\n",
       "      <td>Xbox One X</td>\n",
       "      <td>노출 Lin Feng는 Zhang Xinyue를 제시한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>최홍만</td>\n",
       "      <td>降低论文查重率</td>\n",
       "      <td>Killing Eve</td>\n",
       "      <td>馬場雄大</td>\n",
       "      <td>바바</td>\n",
       "      <td>이브 살해</td>\n",
       "      <td>서류의 무게를 줄이십시오.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>김대중</td>\n",
       "      <td>如何对付火车上的熊孩子</td>\n",
       "      <td>Cynthia Erivo</td>\n",
       "      <td>早坂つむぎ</td>\n",
       "      <td>하야사카</td>\n",
       "      <td>신시아 에리 보</td>\n",
       "      <td>열차에 곰 아이들을 대하는 방법</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>홍자</td>\n",
       "      <td>iG两连败</td>\n",
       "      <td>Britney Spears</td>\n",
       "      <td>恒松祐里</td>\n",
       "      <td>츠네 마츠</td>\n",
       "      <td>브리트니 스피어스</td>\n",
       "      <td>iG 두 게임 패배</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>양재선</td>\n",
       "      <td>国际空间站旅游</td>\n",
       "      <td>Trisha Yearwood</td>\n",
       "      <td>ウォッチドッグス</td>\n",
       "      <td>와치 독스</td>\n",
       "      <td>Trisha Yearwood</td>\n",
       "      <td>국제 우주 정거장 관광</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>이다지</td>\n",
       "      <td>男子篡改存款单</td>\n",
       "      <td>Insurance Companies</td>\n",
       "      <td>二ノ国</td>\n",
       "      <td>노국</td>\n",
       "      <td>보험 회사</td>\n",
       "      <td>보증금 전표로 변경하는 사람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019 싸이 흠뻑쇼</td>\n",
       "      <td>秦明生死语者</td>\n",
       "      <td>Donny Deutsch</td>\n",
       "      <td>鈴木杏樹</td>\n",
       "      <td>스즈키</td>\n",
       "      <td>도니 독일어</td>\n",
       "      <td>진 Mingsheng의 죽음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>한국 이란</td>\n",
       "      <td>2018届毕业生平均月薪</td>\n",
       "      <td>Electronic Logging Devices</td>\n",
       "      <td>ViVi</td>\n",
       "      <td>ViVi 게재</td>\n",
       "      <td>전자 로깅 장치</td>\n",
       "      <td>2018 년 졸업생의 평균 월급</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>천렵질</td>\n",
       "      <td>高考失利要不要复读</td>\n",
       "      <td>Bryce Harper</td>\n",
       "      <td>オリコンランキング</td>\n",
       "      <td>오리콘</td>\n",
       "      <td>브라이스 하퍼</td>\n",
       "      <td>대학 입학 시험에 실패하면, 그것을 반복하십시오.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>홍문종</td>\n",
       "      <td>科学睡眠</td>\n",
       "      <td>Acuvue Oasys</td>\n",
       "      <td>亀山蘭子</td>\n",
       "      <td>카메</td>\n",
       "      <td>Acuvue Oasys</td>\n",
       "      <td>과학적 수면</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Naver         Baidu                        Yahoo   Yahoo_jp  \\\n",
       "1           이희호      曝林峯求婚张馨月                   Xbox One X      ドクターX   \n",
       "2           최홍만       降低论文查重率                  Killing Eve       馬場雄大   \n",
       "3           김대중   如何对付火车上的熊孩子                Cynthia Erivo      早坂つむぎ   \n",
       "4            홍자         iG两连败               Britney Spears       恒松祐里   \n",
       "5           양재선       国际空间站旅游              Trisha Yearwood   ウォッチドッグス   \n",
       "6           이다지       男子篡改存款单          Insurance Companies        二ノ国   \n",
       "7   2019 싸이 흠뻑쇼        秦明生死语者                Donny Deutsch       鈴木杏樹   \n",
       "8         한국 이란  2018届毕业生平均月薪   Electronic Logging Devices       ViVi   \n",
       "9           천렵질     高考失利要不要复读                 Bryce Harper  オリコンランキング   \n",
       "10          홍문종          科学睡眠                 Acuvue Oasys       亀山蘭子   \n",
       "\n",
       "      tr_jp             tr_y                             tr_b  \n",
       "1        닥터       Xbox One X  노출 Lin Feng는 Zhang Xinyue를 제시한다  \n",
       "2        바바            이브 살해                   서류의 무게를 줄이십시오.  \n",
       "3      하야사카         신시아 에리 보                열차에 곰 아이들을 대하는 방법  \n",
       "4     츠네 마츠        브리트니 스피어스                       iG 두 게임 패배  \n",
       "5     와치 독스  Trisha Yearwood                     국제 우주 정거장 관광  \n",
       "6        노국            보험 회사                  보증금 전표로 변경하는 사람  \n",
       "7       스즈키           도니 독일어                  진 Mingsheng의 죽음  \n",
       "8   ViVi 게재         전자 로깅 장치                2018 년 졸업생의 평균 월급  \n",
       "9       오리콘          브라이스 하퍼      대학 입학 시험에 실패하면, 그것을 반복하십시오.  \n",
       "10       카메     Acuvue Oasys                           과학적 수면  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"temp.csv\", encoding=\"utf8\")\n",
    "df.to_excel(\"temp1.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 번외. 셀레니움 감추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\dty\\\\Downloads\\\\chromedriver_win32\\\\chromedriver\", options=options)\n",
    "driver.get('https://www.google.com/doodles')\n",
    "\n",
    "print('Title: \"{}\"'.format(driver.title))\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
