{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5_토픽모델링_Topic Modeling.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Jyp5nHXQsXez","colab_type":"text"},"source":["# 1 잠재의미분석 (LSA)"]},{"cell_type":"markdown","metadata":{"id":"c0Ah5QgSqEce","colab_type":"text"},"source":["## 1.1 실습 템플릿"]},{"cell_type":"code","metadata":{"id":"_Dsdajm39w73","colab_type":"code","colab":{}},"source":["class LSA :\n","  def __init__(self, doc_ls, topic_num):\n","    pass\n","  \n","  # tdm matrix 생성\n","  def TDM(self, doc_ls):\n","    pass\n","  \n","  # tdm matrix 특이값 분해(SVD)\n","  # U, s, Vt로 분해\n","  def SVD(self, tdm):\n","    pass\n","  \n","  # 토픽별 주요 키워드 출력\n","  def TopicModeling(self) :\n","    pass\n","  \n","  # 단어 벡터 행렬 생성 dot(U,s)  \n","  def TermVectorMatrix(self, u, s):\n","    pass\n","  \n","  # 문서 벡터 행렬 생성 dot(s,Vt).T \n","  def DocVectorMatrix(self, s, vt):\n","    pass\n","  \n","  # 키워드를 입력했을 때 단어 벡터 반환\n","  def GetTermVector(self, term):\n","    pass\n","  \n","  # 문서를 입력했을 때 문서 벡터 반환\n","  def GetDocVector(self, doc):\n","    pass\n","  \n","  # 단어-문서 벡터 행렬 생성\n","  def TermDocVectorMatrix(self, u, s, vt):\n","    pass\n","  \n","  # 단어 벡터 행렬에서 단어 간 코사인 유사도 측정하여 행렬형태로 반환\n","  def TermSimilarityMatrix(self, term_vec_matrix):\n","    pass\n","  \n","  # 두개 단어를 입력했을 때 코사인 유사도 반환\n","  def GetTermSimilarity(self, term1, term2):\n","    pass\n","  \n","  # 문서 벡터 행렬에서 문서 간 코사인 유사도 측정하여 행렬형태로 반환\n","  def DocSimilarityMartrix(self, doc_vec_matrix):\n","    pass\n","  \n","  # 두개 문서를 입력했을 때 코사인 유사도 반환\n","  def GetDocSimilarity(self, doc1, doc2):\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHB9A2VjGE0j","colab_type":"code","colab":{}},"source":["doc_ls = ['바나나 사과 포도 포도',\n","         '사과 포도',\n","         '포도 바나나',\n","         '짜장면 짬뽕 탕수욕',\n","         '볶음밥 탕수욕',\n","         '짜장면 짬뽕',\n","         '라면 스시',\n","         '스시',\n","         '가츠동 스시 소바',\n","         '된장찌개 김치찌개 김치',\n","         '김치 된장',\n","         '비빔밥 김치'\n","         ]\n","\n","lsa = LSA(doc_ls, 3)\n","lsa.TopicModeling()\n","lsa.GetTermSimilarity('사과','바나나')\n","lsa.GetTermSimilarity('사과','짜장면')\n","lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n","lsa.GetDocSimilarity('사과 포도', '라면 스시')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAQyWNVQWAh8","colab_type":"text"},"source":["## 1.2 실습 예제 코드 "]},{"cell_type":"code","metadata":{"id":"MFgOmDD84TkZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","from collections import defaultdict\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.decomposition import randomized_svd\n","\n","class LSA :\n","  def __init__(self, doc_ls, topic_num):\n","    self.doc_ls = doc_ls\n","    self.topic_num = topic_num\n","    self.term2idx, self.idx2term = self.toIdxDict(' '.join(doc_ls).split())\n","    self.doc2idx, self.idx2doc = self.toIdxDict(doc_ls)\n","    \n","    self.tdm = self.TDM(doc_ls)\n","    self.U, self.s, self.VT = self.SVD(self.tdm)\n","    \n","    #print(self.s)\n","    #print(self.U[:,:1].round(2))\n","    \n","    self.term_mat = self.TermVectorMatrix(self.U, self.s, topic_num)\n","    self.doc_mat = self.DocVectorMatrix(self.s, self.VT, topic_num)\n","    self.term_doc_mat = self.TermDocVectorMatrix(self.U, self.s, self.VT, topic_num)\n","    \n","    #print(self.term2idx.keys())\n","    #print(self.term_mat.round(2))\n","    \n","    self.term_sim = self.TermSimilarityMatrix(self.term_mat)\n","    self.doc_sim = self.DocSimilarityMartrix(self.doc_mat)\n","    \n","  # 리스트내 값을 index로 변환하는 dict과 \n","  # index를 리스트내 값으로 변환하는 dict\n","  def toIdxDict(self, ls) :\n","    any2idx = defaultdict(lambda : len(any2idx))\n","    idx2any = defaultdict()\n","\n","    for item in ls:\n","        any2idx[item]\n","        idx2any[any2idx[item]] = item\n","        \n","    return any2idx, idx2any\n","  \n","  def TDM(self, doc_ls):\n","    # 행(토큰크기), 열(문서갯수)로 TDM 생성\n","    tdm = np.zeros([len(self.term2idx.keys()), len(doc_ls)])\n","    \n","    for doc_idx, doc in enumerate(doc_ls) :\n","      for term in doc.split() :\n","        #등장한 단어를 dictionary에서 위치를 탐색하여 빈도수 세기\n","        tdm[self.term2idx[term], doc_idx] += 1\n","    \n","    return tdm\n","  \n","  # 특이값 분해\n","  def SVD(self, tdm):\n","    U, s, VT = randomized_svd(tdm, \n","                              n_components=15,\n","                              n_iter=5,\n","                              random_state=None)\n","    \n","    #U, s, VT = np.linalg.svd(tdm)\n","    return U, s, VT\n","  \n","  # 토픽별 주요 키워드 출력\n","  def TopicModeling(self, count = 3) :\n","    topic_num = self.topic_num\n","    \n","    for i in range(topic_num) :\n","      score = self.U[:,i:i+1].T\n","      sorted_index = np.flip(np.argsort(-score),0)\n","      \n","      a = []\n","      for j in sorted_index[0,: count] :\n","        a.append((self.idx2term[j], score[0,j].round(3)))\n","      \n","      print(\"Topic {} - {}\".format(i+1,a ))\n","  \n","  def vectorSimilarity(self, matrix) :\n","    similarity = np.zeros([matrix.shape[1], matrix.shape[1]])\n","    \n","    for i in range(matrix.shape[1]) :\n","      for j in range(matrix.shape[1]) :\n","        similarity[i,j] =  cosine_similarity(matrix[:,i].T, matrix[:,j].T)\n","       \n","    return similarity\n","  \n","  # 키워드를 입력했을 때 단어 벡터 반환\n","  def GetTermVector(self, term):\n","    vec = self.term_mat[self.term2idx[term]:self.term2idx[term]+1,:]\n","    print('{} = {}'.format(term, vec))\n","    return vec\n","  \n","  # 문서를 입력했을 때 문서 벡터 반환\n","  def GetDocVector(self, doc):\n","    vec = self.doc_mat.T[self.doc2idx[doc]:self.doc2idx[doc]+1,:]\n","    print('{} = {}'.format(doc, vec))\n","    return vec\n","  \n","  def Compression(self, round_num=0) :\n","    print(self.tdm)\n","    print(self.term_doc_mat.round(round_num))\n","   \n","  def TermVectorMatrix(self, u, s, topic_num):\n","    term_mat = np.matrix(u[:, :topic_num])# * np.diag(s[:topic_num])\n","    return term_mat\n","  \n","  def DocVectorMatrix(self, s, vt, topic_num):\n","    doc_mat = np.diag(s[:topic_num]) * np.matrix(vt[:topic_num,:])\n","    return doc_mat\n","  \n","  def TermDocVectorMatrix(self, u, s, vt, topic_num):\n","    term_doc_mat = np.matrix(u[:, :topic_num]) * np.diag(s[:topic_num])  * np.matrix(vt[:topic_num,:])\n","    return term_doc_mat\n","  \n","  def TermSimilarityMatrix(self, termVectorMatrix):\n","    return self.vectorSimilarity(termVectorMatrix.T)\n","  \n","  def GetTermSimilarity(self, term1, term2):\n","    sim = self.term_sim[self.term2idx[term1], self.term2idx[term2]]\n","    print(\"({},{}) term similarity = {}\".format(term1, term2, sim))\n","    return sim \n","  \n","  def DocSimilarityMartrix(self,docVectorMatrix):    \n","    return self.vectorSimilarity(docVectorMatrix) \n","  \n","  def GetDocSimilarity(self, doc1, doc2):\n","    sim = self.doc_sim[self.doc2idx[doc1], self.doc2idx[doc2]]\n","    print(\"('{}','{}') doc similarity = {}\".format(doc1, doc2, sim))\n","    return sim "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqZnr8SX86qt","colab_type":"code","outputId":"615aea84-b74b-40eb-e878-0f6768a794a7","executionInfo":{"status":"ok","timestamp":1565739269482,"user_tz":-540,"elapsed":608,"user":{"displayName":"이민호","photoUrl":"https://lh5.googleusercontent.com/-DyV97Vqqbdg/AAAAAAAAAAI/AAAAAAAAADk/S243P6JSRhs/s64/photo.jpg","userId":"15829449822908558555"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["doc_ls = ['바나나 사과 포도 포도 짜장면',\n","         '사과 포도',\n","         '포도 바나나',\n","         '짜장면 짬뽕 탕수육',\n","         '볶음밥 탕수육',\n","         '짜장면 짬뽕',\n","         '라면 스시',\n","         '스시 짜장면',\n","         '가츠동 스시 소바',\n","         '된장찌개 김치찌개 김치',\n","         '김치 된장 짜장면',\n","         '비빔밥 김치'\n","         ]\n","\n","lsa = LSA(doc_ls, 4)\n","X = lsa.TDM(doc_ls)\n","print('== 토픽 모델링 ==')\n","lsa.TopicModeling(4)\n","print('\\n== 단어 벡터 ==')\n","lsa.GetTermVector('사과')\n","lsa.GetTermVector('짜장면')\n","print('\\n== 단어 유사도 ==')\n","lsa.GetTermSimilarity('사과','바나나')\n","lsa.GetTermSimilarity('사과','짜장면')\n","lsa.GetTermSimilarity('포도','짜장면')\n","lsa.GetTermSimilarity('사과','스시')\n","print('\\n== 문서 벡터 ==')\n","lsa.GetDocVector('사과 포도')\n","lsa.GetDocVector('짜장면 짬뽕')\n","print('\\n== 문서 유사도 ==')\n","lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n","lsa.GetDocSimilarity('사과 포도', '라면 스시')\n","print('\\n== 토픽 차원수로 압축 ==')\n","lsa.Compression(0)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["== 토픽 모델링 ==\n","Topic 1 - [('포도', 0.697), ('짜장면', 0.486), ('바나나', 0.348), ('사과', 0.348)]\n","Topic 2 - [('짜장면', 0.584), ('짬뽕', 0.356), ('김치', 0.337), ('스시', 0.256)]\n","Topic 3 - [('김치', 0.611), ('된장찌개', 0.264), ('김치찌개', 0.264), ('비빔밥', 0.185)]\n","Topic 4 - [('스시', 0.552), ('김치', 0.371), ('가츠동', 0.277), ('소바', 0.277)]\n","\n","== 단어 벡터 ==\n","사과 = [[ 0.34843127 -0.19370961  0.01592593  0.03744775]]\n","짜장면 = [[ 0.48563449  0.58415588 -0.07468389 -0.18737521]]\n","\n","== 단어 유사도 ==\n","(사과,바나나) term similarity = 0.9999999999999999\n","(사과,짜장면) term similarity = 0.15191335214806936\n","(포도,짜장면) term similarity = 0.15191335214806956\n","(사과,스시) term similarity = -0.04097825202732765\n","\n","== 문서 벡터 ==\n","사과 포도 = [[ 1.04529381 -0.58112882  0.04777778  0.11234324]]\n","짜장면 짬뽕 = [[ 0.61011838  0.93971158 -0.17760018 -0.53682795]]\n","\n","== 문서 유사도 ==\n","('사과 포도','포도 바나나') doc similarity = 1.0\n","('사과 포도','라면 스시') doc similarity = -0.038506882113500465\n","\n","== 토픽 차원수로 압축 ==\n","[[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.]\n"," [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n","[[ 1.  0.  0. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n"," [ 1.  0.  0. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n"," [ 2.  1.  1. -0. -0.  0. -0.  0. -0. -0.  0. -0.]\n"," [ 1.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.]\n"," [ 0. -0. -0.  1.  0.  1. -0.  0. -0. -0.  0. -0.]\n"," [ 0. -0. -0.  1.  0.  0. -0.  0. -0. -0.  0. -0.]\n"," [-0. -0. -0.  0.  0.  0. -0.  0. -0. -0.  0. -0.]\n"," [-0. -0. -0. -0. -0. -0.  0.  0.  0. -0. -0. -0.]\n"," [ 0. -0. -0.  0. -0.  0.  1.  1.  1. -0.  0. -0.]\n"," [-0. -0. -0. -0. -0. -0.  0.  0.  1. -0. -0. -0.]\n"," [-0. -0. -0. -0. -0. -0.  0.  0.  1. -0. -0. -0.]\n"," [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]\n"," [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]\n"," [ 0. -0. -0. -0. -0.  0. -0.  0. -0.  1.  1.  1.]\n"," [ 0. -0. -0.  0.  0.  0. -0.  0. -0.  0.  0.  0.]\n"," [-0. -0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"31Kn3iEWLDz9","colab_type":"text"},"source":["## 1.3 sklearn LSA 구현 "]},{"cell_type":"code","metadata":{"id":"2LVVXaR4EJCZ","colab_type":"code","colab":{}},"source":["doc_ls = ['바나나 사과 포도 포도',\n","         '사과 포도',\n","         '포도 바나나',\n","         '짜장면 짬뽕 탕수욕',\n","         '볶음밥 탕수욕',\n","         '짜장면 짬뽕',\n","         '라면 스시',\n","         '스시',\n","         '가츠동 스시 소바',\n","         '된장찌개 김치찌개 김치',\n","         '김치 된장',\n","         '비빔밥 김치'\n","         ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOPy5VKzKr8_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"a5496207-6485-475d-835e-700c40cfb4f6","executionInfo":{"status":"ok","timestamp":1565739272437,"user_tz":-540,"elapsed":660,"user":{"displayName":"이민호","photoUrl":"https://lh5.googleusercontent.com/-DyV97Vqqbdg/AAAAAAAAAAI/AAAAAAAAADk/S243P6JSRhs/s64/photo.jpg","userId":"15829449822908558555"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(\n","max_features= 1000, # 상위 1,000개의 단어를 보존 \n","max_df = 0.5, \n","smooth_idf=True)\n","\n","X = vectorizer.fit_transform(doc_ls)\n","\n","from sklearn.decomposition import TruncatedSVD\n","svd_model = TruncatedSVD(n_components=4, algorithm='randomized', n_iter=100)\n","svd_model.fit(X)\n","\n","np.shape(svd_model.components_)\n","\n","terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n","\n","def get_topics(components, feature_names, n=3):\n","    for idx, topic in enumerate(components):\n","        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n","get_topics(svd_model.components_,terms)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Topic 1: [('포도', 0.78069), ('바나나', 0.44189), ('사과', 0.44189)]\n","Topic 2: [('스시', 0.8864), ('라면', 0.33189), ('소바', 0.2282)]\n","Topic 3: [('짬뽕', 0.6258), ('짜장면', 0.6258), ('탕수욕', 0.43614)]\n","Topic 4: [('김치', 0.76421), ('비빔밥', 0.37119), ('된장', 0.37119)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ATfiKPg2wHkQ","colab_type":"text"},"source":["# 2 LDA 실습"]},{"cell_type":"markdown","metadata":{"id":"kTdsTBi5qO6x","colab_type":"text"},"source":["## 2.1 실습 템플릿"]},{"cell_type":"code","metadata":{"id":"O3F7SDbJsrve","colab_type":"code","colab":{}},"source":["class LDA :\n","  def __init__(self, doc_ls, topic_num, alpha = 0.1, beta = 0.001):\n","    self.alpha = alpha\n","    self.beta = beta\n","    self.k = topic_num\n","  \n","  def RandomlyAssignTopic(self, doc_ls):\n","    pass\n","  \n","\n","  def IterateAssignTopic(self) :\n","    pass\n","  \n","  \n","  # 토픽별 주요 키워드 출력\n","  def TopicModeling(self) :\n","    pass\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RS7TS1QP9Q7e","colab_type":"text"},"source":["##2.2  실습 예제 코드"]},{"cell_type":"code","metadata":{"id":"Qj6SMDULtUgd","colab_type":"code","outputId":"7de5f132-a725-491f-b11c-2a7c7858cd5b","executionInfo":{"status":"ok","timestamp":1565739276705,"user_tz":-540,"elapsed":2278,"user":{"displayName":"이민호","photoUrl":"https://lh5.googleusercontent.com/-DyV97Vqqbdg/AAAAAAAAAAI/AAAAAAAAADk/S243P6JSRhs/s64/photo.jpg","userId":"15829449822908558555"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"iTfiF5E7VDk3","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"Qxl6Bat79QQk","colab_type":"code","colab":{}},"source":["import random\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords  \n","from collections import defaultdict\n","\n","\n","\n","class LDA :\n","  def __init__(self, docs, topic_num, alpha = 0.1, beta = 0.001):\n","    self.alpha = alpha\n","    self.beta = beta\n","    self.k = topic_num\n","    self.docs = docs\n","    \n"," \n","  def RandomlyAssignTopic(self, docs) :\n","    dic = defaultdict()\n","    t2i = defaultdict(lambda : len(t2i))\n","    i2t = defaultdict()\n","    d = 0\n","    w = 0\n","    \n","    wnl = WordNetLemmatizer()\n","    stopword = stopwords.words('english')\n","    stopword.append(',')\n","    \n","    # 임의의 토픽을 할당\n","    for tokens in [word_tokenize(doc) for doc in docs] :\n","      for token in [wnl.lemmatize(token.lower()) for token in tokens \n","                    if token not in stopword] :\n","        i2t[t2i[token]] = token\n","        dic[(d, t2i[token], w)] = random.randint(0,self.k-1)\n","        w += 1\n","      d += 1\n","  \n","    print(dic)\n","    return dic, t2i, i2t\n","  \n","  \n","  def CountDocTopic(self, t2i) :\n","    docs = np.zeros((self.k, len(self.docs)))\n","    terms = np.zeros((self.k, len(t2i.keys())))\n","\n","    \n","    #문서별 토큰별 빈도수 계산\n","    for (d, n, w) in self.term_topic.keys() :\n","      topic = self.term_topic[(d, n, w)]\n","      docs[topic, d] += 1 + self.alpha\n","      terms[topic, n] += 1 + self.beta\n","    \n","    #비어있는 값는 값에 alpha, beta 설정\n","    docs = np.where(docs==0.0, self.alpha, docs) \n","    terms = np.where(terms==0.0, self.beta, terms)\n","    \n","    print(\"단어 토픽별 빈도\")\n","    print(terms.round(2))\n","    print(\"문서 토픽별 빈도\")\n","    print(docs.round(1))\n","      \n","    return docs, terms\n","  \n","  \n","  def IterateAssignTopic(self, docs, terms, i2t) :\n","    #한개의 단어씩 주제 배정\n","    prev = {}\n","    \n","    while prev != self.term_topic:\n","      for (d, n, w) in self.term_topic.keys() :\n","        topic = [0, 0]\n","\n","        docs[self.term_topic[(d, n, w)], d] -= (1 + self.alpha)\n","        terms[self.term_topic[(d, n, w)], n] -= (1 + self.beta)\n","        \n","        docs = np.where(docs==0.0, self.alpha, docs) \n","        terms = np.where(terms==0.0, self.beta, terms)\n","\n","        print()\n","        print(\"{}(d:{}, n:{}, w:{}) = topic:{}\".format(i2t[n], d, n, w, self.term_topic[(d, n, w)]))\n","        print(\"문서 토픽별 빈도\")\n","        print(docs.round(1))\n","        print(\"단어 토픽별 빈도\")\n","        print(terms.round(3))\n","        \n","        \n","        prev = self.term_topic\n","        \n","        for t in range(self.k) :\n","          p_t_d = docs[t, d]/docs[:,d:d+1].sum()\n","          p_w_t = terms[t, n]/terms[t:t+1,:].sum()\n","          prob = p_t_d * p_w_t\n","\n","          if topic[1] < prob :\n","            topic = [t, prob]\n","            \n","          print(\"topic {} 일 확률 = {}/{} * {}/{} = {} * {} = {}\".format(t\n","              ,docs[t, d].round(1) , docs[:,d:d+1].sum().round(1)\n","              ,terms[t, d].round(3) , terms[t:t+1,:].sum().round(3)\n","              , p_t_d.round(3), p_w_t.round(4), prob.round(4)))\n","        \n","        \n","        if docs[topic[0], d] < 1 : docs[topic[0], d] = 0;\n","        if terms[topic[0], n] < 1 : terms[topic[0], n] = 0;\n","          \n","        #확률이 가장 큰 토픽을 할당  \n","        self.term_topic[(d, n, w)] = topic[0]\n","        docs[topic[0], d] += (1 + self.alpha)\n","        terms[topic[0], n] += (1 + self.beta)\n","        \n","        print(\"할당된 토픽:{}\".format(self.term_topic[(d, n, w)]))\n","        print(\"=\"*50)\n","        \n","    return terms\n","\n","  \n","  # 토픽별 주요 키워드 출력\n","  def TopicModeling(self, count=3) :\n","    self.term_topic, t2i, i2t = self.RandomlyAssignTopic(self.docs)\n","    docs, terms = self.CountDocTopic(t2i)\n","    terms = self.IterateAssignTopic(docs, terms, i2t)\n","    \n","    score = terms / terms.sum(axis=1, keepdims=True)\n","    \n","    for i in range(self.k) :\n","      print(\"\\nTopic {}\".format(i+1))\n","      sorted_index = np.flip(np.argsort(score[i]),0)[:count]\n","      for j in sorted_index :\n","        #pass\n","        print(\"({}={})\".format(i2t[j], score[i,j].round(3)), end = ' ')\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0sfF7Quwrv9m","colab_type":"code","outputId":"2a72f700-b4f2-41c8-e7b9-ec43b5398799","executionInfo":{"status":"ok","timestamp":1565672105614,"user_tz":-540,"elapsed":2580,"user":{"displayName":"이민호","photoUrl":"https://lh5.googleusercontent.com/-DyV97Vqqbdg/AAAAAAAAAAI/AAAAAAAAADk/S243P6JSRhs/s64/photo.jpg","userId":"15829449822908558555"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["doc_ls = [\"Cute kitty\",\n","\"Eat rice or cake\",\n","\"Kitty and hamster\",\n","\"Eat bread\",\n","\"Rice, bread and cake\",\n","\"Cute hamster eats bread and cake\"]\n","\n","lda = LDA(doc_ls, 2)\n","lda.TopicModeling(5)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["defaultdict(None, {(0, 0, 0): 1, (0, 1, 1): 1, (1, 2, 2): 1, (1, 3, 3): 0, (1, 4, 4): 0, (2, 1, 5): 1, (2, 5, 6): 1, (3, 2, 7): 0, (3, 6, 8): 0, (4, 3, 9): 1, (4, 6, 10): 1, (4, 4, 11): 1, (5, 0, 12): 0, (5, 5, 13): 1, (5, 7, 14): 0, (5, 6, 15): 1, (5, 4, 16): 0})\n","단어 토픽별 빈도\n","[[1. 0. 1. 1. 2. 0. 1. 1.]\n"," [1. 2. 1. 1. 1. 2. 2. 0.]]\n","문서 토픽별 빈도\n","[[0.1 2.2 0.1 2.2 0.1 3.3]\n"," [2.2 1.1 2.2 0.1 3.3 2.2]]\n","\n","cute(d:0, n:0, w:0) = topic:1\n","문서 토픽별 빈도\n","[[0.1 2.2 0.1 2.2 0.1 3.3]\n"," [1.1 1.1 2.2 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[1.001e+00 1.000e-03 1.001e+00 1.001e+00 2.002e+00 1.000e-03 1.001e+00\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.001e+00 1.001e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 0.1/1.2 * 1.001/7.009 = 0.083 * 0.1428 = 0.0119\n","topic 1 일 확률 = 1.1/1.2 * 0.001/9.011 = 0.917 * 0.0001 = 0.0001\n","할당된 토픽:0\n","==================================================\n","\n","kitty(d:0, n:1, w:1) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 2.2 0.1 3.3]\n"," [0.1 1.1 2.2 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 1.001e+00 1.001e+00 2.002e+00 1.000e-03 1.001e+00\n","  1.001e+00]\n"," [1.000e-03 1.001e+00 1.001e+00 1.001e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 1.1/1.2 * 2.002/8.01 = 0.917 * 0.0001 = 0.0001\n","topic 1 일 확률 = 0.1/1.2 * 0.001/8.01 = 0.083 * 0.125 = 0.0104\n","할당된 토픽:1\n","==================================================\n","\n","eat(d:1, n:2, w:2) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 2.2 0.1 3.3]\n"," [1.1 0.1 2.2 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 1.001e+00 1.001e+00 2.002e+00 1.000e-03 1.001e+00\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 1.001e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 2.2/2.3 * 0.001/8.01 = 0.957 * 0.125 = 0.1195\n","topic 1 일 확률 = 0.1/2.3 * 2.002/8.011 = 0.043 * 0.0001 = 0.0\n","할당된 토픽:0\n","==================================================\n","\n","rice(d:1, n:3, w:3) = topic:0\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 2.2 0.1 3.3]\n"," [1.1 0.1 2.2 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.001e+00\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 1.001e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 2.2/2.3 * 0.001/8.011 = 0.957 * 0.0001 = 0.0001\n","topic 1 일 확률 = 0.1/2.3 * 2.002/8.011 = 0.043 * 0.125 = 0.0054\n","할당된 토픽:1\n","==================================================\n","\n","cake(d:1, n:4, w:4) = topic:0\n","문서 토픽별 빈도\n","[[1.1 1.1 0.1 2.2 0.1 3.3]\n"," [1.1 1.1 2.2 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.001e+00 1.000e-03 1.001e+00\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 1.1/2.2 * 0.001/7.01 = 0.5 * 0.1428 = 0.0714\n","topic 1 일 확률 = 1.1/2.2 * 2.002/9.012 = 0.5 * 0.1111 = 0.0555\n","할당된 토픽:0\n","==================================================\n","\n","kitty(d:2, n:1, w:5) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 2.2 0.1 3.3]\n"," [1.1 1.1 1.1 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.001e+00\n","  1.001e+00]\n"," [1.000e-03 1.001e+00 1.000e-03 2.002e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 0.1/1.2 * 2.002/8.011 = 0.083 * 0.0001 = 0.0\n","topic 1 일 확률 = 1.1/1.2 * 0.001/8.011 = 0.917 * 0.125 = 0.1145\n","할당된 토픽:1\n","==================================================\n","\n","hamster(d:2, n:5, w:6) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 2.2 0.1 3.3]\n"," [1.1 1.1 1.1 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.001e+00\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.001e+00 1.001e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 0.1/1.2 * 2.002/8.011 = 0.083 * 0.0001 = 0.0\n","topic 1 일 확률 = 1.1/1.2 * 0.001/8.011 = 0.917 * 0.125 = 0.1145\n","할당된 토픽:1\n","==================================================\n","\n","eat(d:3, n:2, w:7) = topic:0\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 0.1 3.3]\n"," [1.1 1.1 2.2 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 1.001e+00 1.000e-03 2.002e+00 1.000e-03 1.001e+00\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 1.1/1.2 * 0.001/7.01 = 0.917 * 0.1428 = 0.1309\n","topic 1 일 확률 = 0.1/1.2 * 2.002/9.012 = 0.083 * 0.0001 = 0.0\n","할당된 토픽:0\n","==================================================\n","\n","bread(d:3, n:6, w:8) = topic:0\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 0.1 3.3]\n"," [1.1 1.1 2.2 0.1 3.3 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.000e-03\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 1.1/1.2 * 0.001/7.011 = 0.917 * 0.0001 = 0.0001\n","topic 1 일 확률 = 0.1/1.2 * 2.002/9.012 = 0.083 * 0.2221 = 0.0185\n","할당된 토픽:1\n","==================================================\n","\n","rice(d:4, n:3, w:9) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 0.1 3.3]\n"," [1.1 1.1 2.2 1.1 2.2 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.000e-03\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 1.001e+00 1.001e+00 2.002e+00 3.003e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 0.1/2.3 * 2.002/7.011 = 0.043 * 0.0001 = 0.0\n","topic 1 일 확률 = 2.2/2.3 * 1.001/9.012 = 0.957 * 0.1111 = 0.1062\n","할당된 토픽:1\n","==================================================\n","\n","bread(d:4, n:6, w:10) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 0.1 3.3]\n"," [1.1 1.1 2.2 1.1 2.2 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.000e-03\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.001e+00 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 0.1/2.3 * 2.002/7.011 = 0.043 * 0.0001 = 0.0\n","topic 1 일 확률 = 2.2/2.3 * 1.001/9.012 = 0.957 * 0.2221 = 0.2125\n","할당된 토픽:1\n","==================================================\n","\n","cake(d:4, n:4, w:11) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 0.1 3.3]\n"," [1.1 1.1 2.2 1.1 2.2 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.000e-03\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 3.003e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 0.1/2.3 * 2.002/7.011 = 0.043 * 0.2856 = 0.0124\n","topic 1 일 확률 = 2.2/2.3 * 0.001/9.013 = 0.957 * 0.0001 = 0.0001\n","할당된 토픽:0\n","==================================================\n","\n","cute(d:5, n:0, w:12) = topic:0\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 1.1 2.2]\n"," [1.1 1.1 2.2 1.1 2.2 2.2]]\n","단어 토픽별 빈도\n","[[1.001e+00 1.000e-03 2.002e+00 1.000e-03 3.003e+00 1.000e-03 1.000e-03\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 3.003e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 2.2/4.4 * 0.001/7.011 = 0.5 * 0.1428 = 0.0714\n","topic 1 일 확률 = 2.2/4.4 * 2.002/9.013 = 0.5 * 0.0001 = 0.0001\n","할당된 토픽:0\n","==================================================\n","\n","hamster(d:5, n:5, w:13) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 1.1 3.3]\n"," [1.1 1.1 2.2 1.1 2.2 1.1]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 3.003e+00 1.000e-03 1.000e-03\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.001e+00 3.003e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 3.3/4.4 * 0.001/8.012 = 0.75 * 0.0001 = 0.0001\n","topic 1 일 확률 = 1.1/4.4 * 1.001/8.012 = 0.25 * 0.1249 = 0.0312\n","할당된 토픽:1\n","==================================================\n","\n","eats(d:5, n:7, w:14) = topic:0\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 1.1 2.2]\n"," [1.1 1.1 2.2 1.1 2.2 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 3.003e+00 1.000e-03 1.000e-03\n","  1.000e-03]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 3.003e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 2.2/4.4 * 0.001/7.012 = 0.5 * 0.0001 = 0.0001\n","topic 1 일 확률 = 2.2/4.4 * 2.002/9.013 = 0.5 * 0.0001 = 0.0001\n","할당된 토픽:0\n","==================================================\n","\n","bread(d:5, n:6, w:15) = topic:1\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 1.1 3.3]\n"," [1.1 1.1 2.2 1.1 2.2 1.1]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 3.003e+00 1.000e-03 1.000e-03\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 2.002e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 3.3/4.4 * 0.001/8.012 = 0.75 * 0.0001 = 0.0001\n","topic 1 일 확률 = 1.1/4.4 * 2.002/8.012 = 0.25 * 0.2499 = 0.0625\n","할당된 토픽:1\n","==================================================\n","\n","cake(d:5, n:4, w:16) = topic:0\n","문서 토픽별 빈도\n","[[1.1 2.2 0.1 1.1 1.1 2.2]\n"," [1.1 1.1 2.2 1.1 2.2 2.2]]\n","단어 토픽별 빈도\n","[[2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 1.000e-03\n","  1.001e+00]\n"," [1.000e-03 2.002e+00 1.000e-03 2.002e+00 1.000e-03 2.002e+00 3.003e+00\n","  1.000e-03]]\n","topic 0 일 확률 = 2.2/4.4 * 0.001/7.011 = 0.5 * 0.2856 = 0.1428\n","topic 1 일 확률 = 2.2/4.4 * 2.002/9.013 = 0.5 * 0.0001 = 0.0001\n","할당된 토픽:0\n","==================================================\n","\n","Topic 1\n","(cake=0.375) (eat=0.25) (cute=0.25) (eats=0.125) (bread=0.0) \n","Topic 2\n","(bread=0.333) (hamster=0.222) (rice=0.222) (kitty=0.222) (eats=0.0) "],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mi0kPw1isUwj","colab_type":"text"},"source":["##2.3 pyLDAvis 를 이용한 LDA 시각화"]},{"cell_type":"code","metadata":{"id":"C3aI6QjjtFDT","colab_type":"code","colab":{}},"source":["!pip install pyLDAvis"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hza4_0k8skKB","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups\n","dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n","documents = dataset.data\n","len(documents)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJMQKp0pssrE","colab_type":"code","colab":{}},"source":["news_df = pd.DataFrame({'document':documents})\n","# 특수 문자 제거\n","news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n","# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","# 전체 단어에 대한 소문자 변환\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XAbHXJQsvGp","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n","tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n","tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhbqVu5esxdW","colab_type":"code","colab":{}},"source":["from gensim import corpora\n","dictionary = corpora.Dictionary(tokenized_doc)\n","corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n","print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVMkTWb0tCEW","colab_type":"code","colab":{}},"source":["import gensim\n","NUM_TOPICS = 20 #20개의 토픽, k=20\n","ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n","topics = ldamodel.print_topics(num_words=4)\n","for topic in topics:\n","    print(topic)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AP20CapFtOD5","colab_type":"code","colab":{}},"source":["import pyLDAvis.gensim\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n","pyLDAvis.display(vis)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJjzXbWrtOkr","colab_type":"code","colab":{}},"source":["for i, topic_list in enumerate(ldamodel[corpus]):\n","    if i==5:\n","        break\n","    print(i,'번째 문서의 topic 비율은',topic_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rxkdBgjStRfV","colab_type":"code","colab":{}},"source":["def make_topictable_per_doc(ldamodel, corpus, texts):\n","    topic_table = pd.DataFrame()\n","\n","    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n","    for i, topic_list in enumerate(ldamodel[corpus]):\n","        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n","        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n","        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n","        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n","        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n","        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n","\n","        # 모든 문서에 대해서 각각 아래를 수행\n","        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n","            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n","                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n","                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n","            else:\n","                break\n","    return(topic_table)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSFPSOXztUH0","colab_type":"code","colab":{}},"source":["topictable = make_topictable_per_doc(ldamodel, corpus, tokenized_doc)\n","topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n","topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n","topictable[:10]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmsz2JZmKlPZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}