{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LDA:\n",
    "    def __init__(self, doc_ls, topic_num, alpha = 0.1, beta = 0.001):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = topic_num\n",
    "        self.docs = doc_ls\n",
    "    \n",
    "    def RandomlyAssignTopic(self, doc_ls):\n",
    "        dic = defaultdict(dict)\n",
    "        t2i = defaultdict(lambda : len(t2i))\n",
    "        i2t = defaultdict()\n",
    "        d= 0\n",
    "        w = 0\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stopword = stopwords.words('english')\n",
    "        stopword.append(',')\n",
    "\n",
    "\n",
    "\n",
    "        for tokens in [word_tokenize(doc) for doc in doc_ls]:\n",
    "            for token in [wnl.lemmatize(token.lower()) for token in tokens if token not in stopword]:\n",
    "                i2t[t2i[token]] = token\n",
    "                dic[(d, t2i[token], w)] = random.randint(0,self.k-1)\n",
    "                w += 1\n",
    "            d += 1\n",
    "        #print(dic)\n",
    "        return dic, t2i, i2t\n",
    "    def CountDocTopic(self) :\n",
    "        docs = np.zeros((self.k, len(self.docs)))\n",
    "        terms = np.zeros((self.k, len(self.t2i.keys())))\n",
    "        #문서별 토큰별 빈도수 계산\n",
    "        for (d, n, w) in self.term_topic.keys() :\n",
    "            docs[self.term_topic[(d, n, w)], d] += 1 + self.alpha\n",
    "            terms[self.term_topic[(d, n, w)], d] += 1 + self.beta\n",
    "        #print(doc_m)\n",
    "        #비어있는 값는 값에 alpha, beta 설정\n",
    "        docs = np.where(docs==0.0, self.alpha, docs)\n",
    "        terms = np.where(terms==0.0, self.beta, terms)\n",
    "        return docs, terms    \n",
    "    def IterateAssignTopic(self, docs, terms) :\n",
    "        #한개의 단어씩 주제 배정\n",
    "        prev = {}\n",
    "        while prev != self.term_topic:\n",
    "            for (d, n, w) in self.term_topic.keys() :\n",
    "                topic = [0, 0]\n",
    "                docs[self.term_topic[(d, n, w)], d] -= (1 + self.alpha)\n",
    "                terms[self.term_topic[(d, n, w)], n] -= (1 + self.beta)\n",
    "                #print(doc_m)\n",
    "                prev = self.term_topic\n",
    "                for t in range(self.k) :\n",
    "                    p_t_d = docs[t, d]/docs[:,d:d+1].sum()\n",
    "                    p_w_t = terms[t, n]/terms[t:t+1,:].sum()\n",
    "                    prob = p_t_d * p_w_t\n",
    "                    if topic[1] < prob :\n",
    "                        topic = [t, prob]\n",
    "                #확률이 가장 큰 토픽을 할당\n",
    "                self.term_topic[(d, n, w)] = topic[0]\n",
    "                docs[self.term_topic[(d, n, w)], d] += (1 + self.alpha)\n",
    "                terms[self.term_topic[(d, n, w)], t] += (1 + self.beta)\n",
    "                #print(self.term_topic)\n",
    "            return terms\n",
    "# 토픽별 주요 키워드 출력\n",
    "    def TopicModeling(self, count=5) :\n",
    "        self.term_topic, self.t2i, self.i2t = self.RandomlyAssignTopic(self.docs)\n",
    "        docs, terms = self.CountDocTopic()\n",
    "        terms = self.IterateAssignTopic(docs, terms)\n",
    "        score = terms / terms.sum(axis=1, keepdims=True)\n",
    "        for i in range(self.k) :\n",
    "            print(\"\\nTopic {}\".format(i+1))\n",
    "            sorted_index = np.flip(np.argsort(score[i]),0)[:count]\n",
    "            for j in sorted_index :\n",
    "                print(\"({}={})\".format(self.i2t[j], score[i,j].round(3)), end = ' ')    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1\n",
      "(idea=1.039) (imagine=0.476) (waking=0.39) (people=0.0) (lack=0.0) \n",
      "Topic 2\n",
      "(idea=1.028) (imagine=0.405) (waking=0.343) (people=0.0) ('s=0.0) \n",
      "Topic 3\n",
      "(idea=0.998) (waking=0.304) (imagine=0.195) (something=0.065) (every=0.0) \n",
      "Topic 4\n",
      "(waking=1.185) (idea=0.948) (imagine=0.711) (bedroom=0.047) (something=0.047) "
     ]
    }
   ],
   "source": [
    "\n",
    "doc_ls = [\"Imagine waking up in your bedroom with no idea of where you are. Something feels familiar about the cotton sheets, the pictures on the wall, the sheer curtains, but you can't place it. Minutes later, you feel that same sensation of waking up, but this time you're standing at your dresser, wearing a T-shirt and jeans with no recollection of ever having been in bed. It's as though your consciousness lacks a past and a future, like a stop-motion film in which every previous frame is destroyed.\",\n",
    "'''But while people seem to easily remember tragic events and the seemingly insignificant details associated with them, many would be hard-pressed to recall the minutia of their happy times. For example, mothers often have trouble summoning the specifics of their children's birth, but are amazingly accurate in recounting the duration and intensity of the labor process. It begs the question, \"Do we remember the bad times better than the good?\" Before answering, it's helpful to know a bit about the process of memory formation and the factors that influence it.''',\n",
    "\"Kitty and hamster\",\n",
    "\"Eat bread\",\n",
    "\"Rice, bread and cake\",\n",
    "\"Cute hamster eats bread and cake\"]\n",
    "lda = LDA(doc_ls, 4)\n",
    "lda.TopicModeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\",\n",
       " \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\",\n",
       " \"Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to continue this think tank charade of yours, your\\nfixation on Israel must stop.  You might have to start asking the\\nsame sort of questions of Arab countries as well.  You realize it\\nwould not work, as the Arab countries' treatment of Jews over the\\nlast several decades is so bad that your fixation on Israel would\\nbegin to look like the biased attack that it is.\\n\\n   Everyone in this group recognizes that your stupid 'Center for\\nPolicy Research' is nothing more than a fancy name for some bigot\\nwho hates Israel.\",\n",
       " 'Notwithstanding all the legitimate fuss about this proposal, how much\\nof a change is it?  ATT\\'s last product in this area (a) was priced over\\n$1000, as I suspect \\'clipper\\' phones will be; (b) came to the customer \\nwith the key automatically preregistered with government authorities. Thus,\\naside from attempting to further legitimize and solidify the fed\\'s posture,\\nClipper seems to be \"more of the same\", rather than a new direction.\\n   Yes, technology will eventually drive the cost down and thereby promote\\nmore widespread use- but at present, the man on the street is not going\\nto purchase a $1000 crypto telephone, especially when the guy on the other\\nend probably doesn\\'t have one anyway.  Am I missing something?\\n   The real question is what the gov will do in a year or two when air-\\ntight voice privacy on a phone line is as close as your nearest pc.  That\\nhas got to a problematic scenario for them, even if the extent of usage\\nnever surpasses the \\'underground\\' stature of PGP.',\n",
       " \"Well, I will have to change the scoring on my playoff pool.  Unfortunately\\nI don't have time right now, but I will certainly post the new scoring\\nrules by tomorrow.  Does it matter?  No, you'll enter anyway!!!  Good!\\n\\n--\\n    Keith Keller\\t\\t\\t\\tLET'S GO RANGERS!!!!!\\n\\t\\t\\t\\t\\t\\tLET'S GO QUAKERS!!!!!\\n\\tkkeller@mail.sas.upenn.edu\\t\\tIVY LEAGUE CHAMPS!!!!\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  Well i'm not sure about the story nad it did s...   \n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2  Although I realize that principle is not one o...   \n",
       "3  Notwithstanding all the legitimate fuss about ...   \n",
       "4  Well, I will have to change the scoring on my ...   \n",
       "\n",
       "                                           clean_doc  \n",
       "0  well sure about story seem biased what disagre...  \n",
       "1  yeah expect people read actually accept hard a...  \n",
       "2  although realize that principle your strongest...  \n",
       "3  notwithstanding legitimate fuss about this pro...  \n",
       "4  well will have change scoring playoff pool unf...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'sure',\n",
       " 'story',\n",
       " 'seem',\n",
       " 'biased',\n",
       " 'disagree',\n",
       " 'statement',\n",
       " 'media',\n",
       " 'ruin',\n",
       " 'israels',\n",
       " 'reputation',\n",
       " 'rediculous',\n",
       " 'media',\n",
       " 'israeli',\n",
       " 'media',\n",
       " 'world',\n",
       " 'lived',\n",
       " 'europe',\n",
       " 'realize',\n",
       " 'incidences',\n",
       " 'described',\n",
       " 'letter',\n",
       " 'occured',\n",
       " 'media',\n",
       " 'whole',\n",
       " 'seem',\n",
       " 'ignore',\n",
       " 'subsidizing',\n",
       " 'israels',\n",
       " 'existance',\n",
       " 'europeans',\n",
       " 'least',\n",
       " 'degree',\n",
       " 'think',\n",
       " 'might',\n",
       " 'reason',\n",
       " 'report',\n",
       " 'clearly',\n",
       " 'atrocities',\n",
       " 'shame',\n",
       " 'austria',\n",
       " 'daily',\n",
       " 'reports',\n",
       " 'inhuman',\n",
       " 'acts',\n",
       " 'commited',\n",
       " 'israeli',\n",
       " 'soldiers',\n",
       " 'blessing',\n",
       " 'received',\n",
       " 'government',\n",
       " 'makes',\n",
       " 'holocaust',\n",
       " 'guilt',\n",
       " 'away',\n",
       " 'look',\n",
       " 'jews',\n",
       " 'treating',\n",
       " 'races',\n",
       " 'power',\n",
       " 'unfortunate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.014*\"compass\" + 0.013*\"scores\" + 0.009*\"toyota\" + 0.007*\"idle\"')\n",
      "(1, '0.020*\"wiring\" + 0.011*\"helmet\" + 0.009*\"polygon\" + 0.008*\"cmos\"')\n",
      "(2, '0.016*\"radar\" + 0.011*\"autos\" + 0.010*\"circuits\" + 0.010*\"detector\"')\n",
      "(3, '0.014*\"available\" + 0.010*\"version\" + 0.009*\"also\" + 0.009*\"data\"')\n",
      "(4, '0.039*\"file\" + 0.022*\"output\" + 0.021*\"program\" + 0.020*\"window\"')\n",
      "(5, '0.014*\"guns\" + 0.010*\"control\" + 0.009*\"crime\" + 0.009*\"states\"')\n",
      "(6, '0.010*\"jesus\" + 0.007*\"believe\" + 0.007*\"would\" + 0.006*\"people\"')\n",
      "(7, '0.039*\"color\" + 0.022*\"colors\" + 0.013*\"colormap\" + 0.012*\"mask\"')\n",
      "(8, '0.011*\"year\" + 0.011*\"game\" + 0.010*\"team\" + 0.008*\"good\"')\n",
      "(9, '0.032*\"jpeg\" + 0.019*\"format\" + 0.018*\"image\" + 0.012*\"shareware\"')\n",
      "(10, '0.019*\"printf\" + 0.010*\"soon\" + 0.009*\"gordon\" + 0.009*\"pitt\"')\n",
      "(11, '0.011*\"information\" + 0.009*\"space\" + 0.008*\"mail\" + 0.007*\"send\"')\n",
      "(12, '0.019*\"period\" + 0.015*\"play\" + 0.011*\"power\" + 0.010*\"pittsburgh\"')\n",
      "(13, '0.028*\"health\" + 0.021*\"medical\" + 0.016*\"disease\" + 0.013*\"patients\"')\n",
      "(14, '0.016*\"would\" + 0.012*\"people\" + 0.007*\"like\" + 0.007*\"think\"')\n",
      "(15, '0.028*\"president\" + 0.013*\"ground\" + 0.011*\"jobs\" + 0.011*\"going\"')\n",
      "(16, '0.016*\"navy\" + 0.009*\"fingers\" + 0.008*\"speakers\" + 0.008*\"commentary\"')\n",
      "(17, '0.010*\"drive\" + 0.010*\"would\" + 0.010*\"like\" + 0.010*\"know\"')\n",
      "(18, '0.012*\"armenian\" + 0.010*\"turkish\" + 0.009*\"armenians\" + 0.007*\"russian\"')\n",
      "(19, '0.016*\"said\" + 0.012*\"israel\" + 0.010*\"people\" + 0.008*\"went\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 20 #20개의 토픽, k=20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
