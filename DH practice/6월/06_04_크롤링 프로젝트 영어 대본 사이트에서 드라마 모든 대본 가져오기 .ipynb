{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (++추가할 것: 단어 또는 문장 빈도 수 체크 & 사진 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "url = '''https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=american-horror-story'''\n",
    "\n",
    "resp = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "soup\n",
    "find_frame = soup.find('div', class_ = 'main-content-left')  #최고 프레임 설정\n",
    "#print(find_frame)\n",
    "\n",
    "find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "#print(find_seasons)\n",
    "seasons_scripts = []\n",
    "\n",
    "for season in find_seasons:\n",
    "    #시즌 이름 추출\n",
    "    seasons_list = {}                           \n",
    "    \n",
    "    seasons_list['season'] = season.find('h3').text  # 몇번째 시즌    \n",
    "    a_tags = season.find_all('a')\n",
    "    \n",
    "    a_tag_list = []\n",
    "    \n",
    "    for a in a_tags:\n",
    "        \n",
    "        parse_link = urllib.parse.urlparse(a.attrs['href'])  \n",
    "        episodes = parse_link.query.split('&')[1]\n",
    "        #print(episodes[8:])  \n",
    "        seasons_list['episodes'] = episodes[8:]           #에피소드 순서  # = 뒤로 단어 끄집어오기\n",
    "\n",
    "        a_tag_list.append(a.attrs['href'])          #에피소드 별 특별 주소\n",
    "        \n",
    "        for url_tag in a_tag_list:\n",
    "            url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "            url_base = url_base + url_tag            # 대본으로 가는 url 합치기\n",
    "            seasons_list['episodes_url'] = url_base     #에피소드 대본 url\n",
    "        \n",
    "print(seasons_list)\n",
    "\n",
    "#log_str = parse_link.query.split('&')[1]\n",
    "    \n",
    "# url을 합치고 모든 url 들어가서 대본만 가져오기\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "\n",
    "    \n",
    "url = '''https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=american-horror-story'''\n",
    "\n",
    "resp = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "\n",
    "find_frame = soup.find('div', class_ = 'main-content-left')  #최고 프레임 설정\n",
    "#print(find_frame)\n",
    "\n",
    "find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "#print(find_seasons)\n",
    "\n",
    "\n",
    "seasons_list = []                 #시즌 순서와 url 모두 이곳으로\n",
    "\n",
    "for season in find_seasons:\n",
    "    #시즌 이름 추출\n",
    "                              \n",
    "    \n",
    "    seasons_list.append(season.find('h3').text)  # 몇번째 시즌  \n",
    "    \n",
    "#    seasons_scripts.append(season.find('h3').text)\n",
    "#    print(seasons_scripts)\n",
    "                           \n",
    "    a_tags = season.find_all('a')\n",
    "    \n",
    "    url_list = []\n",
    "    \n",
    "    for a in a_tags:\n",
    "        url_tag = a.attrs['href']          #에피소드 별 특별 주소\n",
    "        url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "        url_tag = url_base + url_tag            # 대본으로 가는 url 합치기\n",
    "        \n",
    "        url_list.append(url_tag)          #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "    seasons_list.append(url_list)  #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "    \n",
    "#print(seasons_list)\n",
    "print(seasons_list)\n",
    "\n",
    "script_list = []\n",
    "count = 1\n",
    "for script_url in seasons_list[count]:\n",
    "    script_resp = requests.get(script_url)\n",
    "    script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "    #script_list.append(script_soup.find('div', class_ = 'scrolling-script-container'))\n",
    "    print(seasons_list[count])\n",
    "    count += 2\n",
    "    if count == 9:\n",
    "        break\n",
    "#log_str = parse_link.query.split('&')[1]\n",
    "    \n",
    "# url을 합치고 모든 url 들어가서 대본만 가져오기\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "  \n",
    "url = '''https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=american-horror-story'''\n",
    "\n",
    "resp = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "find_frame = soup.find('div', class_ = 'main-content-left')  #최고 프레임 설정\n",
    "\n",
    "find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "\n",
    "seasons_list = []                 #시즌 순서와 url 모두 이곳으로\n",
    "\n",
    "for season in find_seasons:\n",
    "    #시즌 이름 추출                               \n",
    "    seasons_list.append(season.find('h3').text)  # 몇번째 시즌  \n",
    "                       \n",
    "    a_tags = season.find_all('a')\n",
    "    \n",
    "    url_list = []\n",
    "    \n",
    "    for a in a_tags:\n",
    "        url_tag = a.attrs['href']          #에피소드 별 특별 주소\n",
    "        url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "        url_tag = url_base + url_tag            # 대본으로 가는 url 합치기\n",
    "        \n",
    "        url_list.append(url_tag)          #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "    seasons_list.append(url_list)  #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "    \n",
    "print(seasons_list)\n",
    "\n",
    "#에피소드별 url에 접속 후 대본 가져오기\n",
    "script_list = []\n",
    "count = 1\n",
    "for script_url in seasons_list[count]:         #seasons_list 각 인덱스에 있는 url에 접속\n",
    "    script_resp = requests.get(script_url)     \n",
    "    script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "    script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #본문 긁어 오기\n",
    "    count += 2\n",
    "    if count == 9:\n",
    "        break\n",
    "        \n",
    "\n",
    "#pd.DataFrame(script_list).to_excel('americanhorrorstory.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자로부터 미드 입력받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "user_input = input('원하는 미드를 입력하세요')\n",
    "series = user_input.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "series.lower()                       #소문자로 바꾸기\n",
    "print(series)\n",
    "\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "params = {\n",
    "    'tv-show' : series\n",
    "}\n",
    "\n",
    "resp = requests.get(url, params = params)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "find_frame = soup.find('div', class_ = 'main-content-left')  #최고 프레임 설정\n",
    "find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "\n",
    "seasons_list = []                 #시즌 순서와 url 모두 이곳으로\n",
    "\n",
    "for season in find_seasons:\n",
    "    #시즌 이름 추출                               \n",
    "    seasons_list.append(season.find('h3').text)  # 몇번째 시즌  \n",
    "                       \n",
    "    a_tags = season.find_all('a')\n",
    "    \n",
    "    url_list = []\n",
    "    \n",
    "    for a in a_tags:\n",
    "        url_tag = a.attrs['href']          #에피소드 별 특별 주소\n",
    "        url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "        url_tag = url_base + url_tag            # 대본으로 가는 url 합치기\n",
    "        \n",
    "        url_list.append(url_tag)          #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "    seasons_list.append(url_list)  #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "#에피소드별 url에 접속 후 대본 가져오기\n",
    "script_list = []\n",
    "count = 1\n",
    "print(seasons_list)\n",
    "for script_url in seasons_list[count]:         #seasons_list 각 인덱스에 있는 url에 접속\n",
    "    script_resp = requests.get(script_url)\n",
    "    script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "    script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #본문 긁어 오기\n",
    "    count += 2\n",
    "    if count == 9:\n",
    "        break\n",
    "        \n",
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "\n",
    "with open('output.txt', 'w', -1, 'utf-8') as f:\n",
    "    f.write('%s\\n' % user_input)\n",
    "    for scripts in script_list:\n",
    "        f.write('%s\\n' % scripts)\n",
    "#pd.DataFrame(script_list).to_excel('americanhorrorstory.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image 다운 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import base64\n",
    "\n",
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "\n",
    "user_input = input('원하는 미드를 입력하세요')\n",
    "series = user_input.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "series.lower()                       #소문자로 바꾸기\n",
    "print(series)\n",
    "\n",
    "\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "params = {\n",
    "    'tv-show' : series\n",
    "}\n",
    "\n",
    "resp = requests.get(url, params = params)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "#미드 이미지 가져오기/et(img_url)\n",
    "driver_path = 'C:\\\\Users\\\\15Z970-GA5BK\\\\Downloads\\\\chromedriver.exe'\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "driver.get('https://www.google.com')\n",
    "search_input = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "search_input.send_keys(series)\n",
    "\n",
    "driver.implicitly_wait(10)\n",
    "search_btn = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[3]/center/input[1]').click()\n",
    "image_btn = driver.find_element_by_link_text('이미지').click()\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "find_frame = soup.find('div', class_ = 'main-content-left')  #가장 바깥 프레임 설정\n",
    "find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "\n",
    "seasons_list = []                 #시즌 순서와 url 모두 이곳으로\n",
    "\n",
    "for season in find_seasons:\n",
    "    #시즌 이름 추출                               \n",
    "    seasons_list.append(season.find('h3').text)  # 시즌 numbering   \n",
    "                       \n",
    "    a_tags = season.find_all('a')\n",
    "    \n",
    "    url_list = []\n",
    "    \n",
    "    for a in a_tags:\n",
    "        url_tag = a.attrs['href']          #에피소드 별 특별 주소\n",
    "        url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "        url_tag = url_base + url_tag            # 대본으로 가는 url 합치기\n",
    "        \n",
    "        url_list.append(url_tag)          #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "    seasons_list.append(url_list)  #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "#에피소드별 url에 접속 후 대본 가져오기\n",
    "script_list = []\n",
    "count = 1\n",
    "print(seasons_list)\n",
    "for script_url in seasons_list[count]:         #seasons_list 각 인덱스에 있는 url에 접속\n",
    "    script_resp = requests.get(script_url)\n",
    "    script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "    script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #본문 긁어 오기\n",
    "    count += 2\n",
    "    if count == 5:\n",
    "        break\n",
    "        \n",
    "# 이름대로 파일 저장하기\n",
    "file_name = series + '_script.txt'\n",
    "with open(file_name, 'w', -1, 'utf-8') as f:\n",
    "    f.write('%s\\n' % user_input)\n",
    "    for scripts in script_list:\n",
    "        f.write('%s\\n' % scripts)\n",
    "#pd.DataFrame(script_list).to_excel('americanhorrorstory.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import base64\n",
    "\n",
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "\n",
    "user_input = input('원하는 미드를 입력하세요')\n",
    "series = user_input.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "series.lower()                       #소문자로 바꾸기\n",
    "print(series)\n",
    "\n",
    "\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "params = {\n",
    "    'tv-show' : series\n",
    "}\n",
    "\n",
    "resp = requests.get(url, params = params)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "soup\n",
    "\n",
    "#미드 이미지 가져오기/et(img_url)\n",
    "driver_path = 'C:\\\\Users\\\\15Z970-GA5BK\\\\Downloads\\\\chromedriver.exe'\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "driver.get('https://www.google.com')\n",
    "\n",
    "search_input = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "search_input.send_keys(series)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "search_btn = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[3]/center/input[1]').click()\n",
    "image_btn = driver.find_element_by_link_text('이미지').click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ################################################\n",
    "\n",
    "# find_frame = soup.find('div', class_ = 'main-content-left')  #가장 바깥 프레임 설정\n",
    "# find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "\n",
    "# seasons_list = []                 #시즌 순서와 url 모두 이곳으로\n",
    "\n",
    "# for season in find_seasons:\n",
    "#     #시즌 이름 추출                               \n",
    "#     seasons_list.append(season.find('h3').text)  # 시즌 numbering   \n",
    "                       \n",
    "#     a_tags = season.find_all('a')\n",
    "    \n",
    "#     url_list = []\n",
    "    \n",
    "#     for a in a_tags:\n",
    "#         url_tag = a.attrs['href']          #에피소드 별 특별 주소\n",
    "#         url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "#         url_tag = url_base + url_tag            # 대본으로 가는 url 합치기\n",
    "        \n",
    "#         url_list.append(url_tag)          #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "#     seasons_list.append(url_list)  #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "# #에피소드별 url에 접속 후 대본 가져오기\n",
    "# script_list = []\n",
    "# count = 1\n",
    "# print(seasons_list)\n",
    "# for script_url in seasons_list[count]:         #seasons_list 각 인덱스에 있는 url에 접속\n",
    "#     script_resp = requests.get(script_url)\n",
    "#     script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "#     script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #본문 긁어 오기\n",
    "#     count += 2\n",
    "#     if count == 5:\n",
    "#         break\n",
    "        \n",
    "# # 이름대로 파일 저장하기\n",
    "# file_name = series + '_script.txt'\n",
    "# with open(file_name, 'w', -1, 'utf-8') as f:\n",
    "#     f.write('%s\\n' % user_input)\n",
    "#     for scripts in script_list:\n",
    "#         f.write('%s\\n' % scripts)\n",
    "# #pd.DataFrame(script_list).to_excel('americanhorrorstory.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 페이지 html로 변환\n",
    "img_url = 'https://www.google.com/search?q=' + series + '&source=lnms&tbm=isch&sa=X&ved=0'\n",
    "resp2 = requests.get(img_url)\n",
    "soup2 = BeautifulSoup(resp2.content, 'html.parser')\n",
    "print(soup2)\n",
    "find_img = soup2.find_all('table', class_ = 'images_table')\n",
    "print(find_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수\n",
    "def series_input\n",
    "\n",
    "def script_crawling\n",
    "\n",
    "def save_to_txt\n",
    "\n",
    "def img_crawling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대본 프로젝트 함수로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "#전역 변수\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "\n",
    "#입력 받은 값에서 띄어쓰기가 있는 경우 '-'로 변환\n",
    "def replace_space(series_name):  \n",
    "    \n",
    "    series_name = series_name.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "    series_name.lower()    #소문자로 바꾸기\n",
    "    return series_name\n",
    "\n",
    "\n",
    "\n",
    "#대본 크롤링\n",
    "def script_crawling(series_name):\n",
    "    params = {\n",
    "        'tv-show' : series_name\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params = params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    find_frame = soup.find('div', class_ = 'main-content-left')           #가장 바깥 프레임 설정\n",
    "    find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "\n",
    "    seasons_list = []                                #시즌 태그와 하위 태그 값들을 모두 이곳에 저장\n",
    "\n",
    "    for season in find_seasons:\n",
    "        #시즌 이름 추출                               \n",
    "        seasons_list.append(season.find('h3').text)  # 시즌 numbering   \n",
    "        #에피소드 태그 추출               \n",
    "        a_tags = season.find_all('a')    \n",
    "        url_list = []                                #각 에피소드들의 주소가 담겨 있는 리스트\n",
    "        for a in a_tags:\n",
    "            url_tag = a.attrs['href']                #에피소드 별 주소 추출\n",
    "            url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "            url_tag = url_base + url_tag             # 대본으로 가는 url 합치기\n",
    "        \n",
    "            url_list.append(url_tag)                 #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "        seasons_list.append(url_list)                #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "    #에피소드별 url에 접속 후 대본 가져오기\n",
    "    script_list = []\n",
    "    count = 1\n",
    "    print(seasons_list)\n",
    "    for script_url in seasons_list[count]:           #seasons_list 각 인덱스에 있는 url에 접속\n",
    "        script_resp = requests.get(script_url)\n",
    "        script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "        script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #대본 페이지 본문 긁어 오기\n",
    "        \n",
    "        #제한 설정: 시즌 2 대본까지만 긁어오기\n",
    "        count += 2\n",
    "        if count == 5:\n",
    "            break\n",
    "    \n",
    "    #텍스트 파일 하나로 몰아넣기\n",
    "    def save_to_txt(series_name):\n",
    "        # 처음에 입력 받은 이름으로 파일 저장하기\n",
    "        file_name = series_name + '_script.txt'\n",
    "        with open(file_name, 'w', -1, 'utf-8') as f:   #'cp949' codec can't encode character '\\xe8' in position 11518: \n",
    "                                                       #illegal multibyte sequence 에러 때문에 'utf-8' 인코딩 수정\n",
    "\n",
    "            f.write('%s\\n' % series)         #시즌 몇번째인지\n",
    "            for scripts in script_list:      #해당 시즌 모든 에피소드 출력\n",
    "                f.write('%s\\n' % scripts)\n",
    "    save_to_txt(series_name)                 #텍스트 파일로 저장\n",
    "##script crwaling end\n",
    "\n",
    "#미드 이미지 가져오기/et(img_url)\n",
    "def get_img(series_name):\n",
    "\n",
    "    driver_path = 'C:\\\\Users\\\\15Z970-GA5BK\\\\Downloads\\\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    driver.get('https://www.google.com')\n",
    "\n",
    "    search_input = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "    search_input.send_keys(series_name)\n",
    "\n",
    "\n",
    "    search_btn = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[3]/center/input[1]').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    image_btn = driver.find_element_by_link_text('이미지').click()\n",
    "    \n",
    "    #구글 이미지 페이지 사진 긁어오기\n",
    "    img_url = 'https://www.google.com/search?q=' + series_name + '&source=lnms&tbm=isch&sa=X&ved=0'\n",
    "    resp2 = requests.get(img_url)\n",
    "    soup2 = BeautifulSoup(resp2.content, 'html.parser')\n",
    "    #print(soup2)\n",
    "    find_img = soup2.find('table', class_ = 'images_table')\n",
    "    #print(find_img)\n",
    "    find_img2 = find_img.find_all('tr')\n",
    "    \n",
    "    \n",
    "    img_url_list = []\n",
    "    for td in find_img2:\n",
    "        a_tag = td.find('img')\n",
    "        \n",
    "        img_src = a_tag.attrs['src']\n",
    "        img_url_list.append(img_src)\n",
    "    \n",
    "    img_url_list2 = []\n",
    "    #image source 가져오기\n",
    "    for i in range(len(img_url_list)):\n",
    "        \n",
    "        img_url_list2.append(img_url_list[i])\n",
    "    \n",
    "    print(img_url_list2)\n",
    "    \n",
    "    #image source에 접근한 후 이미지 다운로드\n",
    "    count = 1\n",
    "    for i in img_url_list2:\n",
    "        img_url = i\n",
    "        resp = requests.get(i)\n",
    "        file_name = series_name  + str(count) + '.jpeg'  #번호 올리면서 이미지 저장\n",
    "        \n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "        \n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "            \n",
    "#replace_space(), script_crawling(), #get_img()\n",
    "series = replace_space(input('원하는 미드를 입력하세요: '))\n",
    "print(series) \n",
    "script_crawlingrawling(series)\n",
    "get_img(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 이미지는 disable javescript 한 후에 태그 찾아야 가능#이미지 페이지 html로 변환\n",
    "\n",
    "img_url = 'https://www.google.com/search?q=' + series + '&source=lnms&tbm=isch&sa=X&ved=0'\n",
    "resp2 = requests.get(img_url)\n",
    "soup2 = BeautifulSoup(resp2.content, 'html.parser')\n",
    "print(soup2)\n",
    "find_img = soup2.find_all('table', class_ = 'images_table')\n",
    "print(find_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_07_크롤링 프로젝트 영어 대본 사이트에서 드라마 모든 대본 가져오기 & 구글 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "#전역 변수\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "\n",
    "#입력 받은 값에서 띄어쓰기가 있는 경우 '-'로 변환\n",
    "def replace_space(series_name):  \n",
    "    \n",
    "    series_name = series_name.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "    series_name.lower()    #소문자로 바꾸기\n",
    "    return series_name\n",
    "\n",
    "\n",
    "\n",
    "#대본 크롤링\n",
    "def script_crawling(series_name):\n",
    "    params = {\n",
    "        'tv-show' : series_name\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params = params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    find_frame = soup.find('div', class_ = 'main-content-left')           #가장 바깥 프레임 설정\n",
    "    find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "\n",
    "    seasons_list = []                                #시즌 태그와 하위 태그 값들을 모두 이곳에 저장\n",
    "\n",
    "    for season in find_seasons:\n",
    "        #시즌 이름 추출                               \n",
    "        seasons_list.append(season.find('h3').text)  # 시즌 numbering   \n",
    "        #에피소드 태그 추출               \n",
    "        a_tags = season.find_all('a')    \n",
    "        url_list = []                                #각 에피소드들의 주소가 담겨 있는 리스트\n",
    "        for a in a_tags:\n",
    "            url_tag = a.attrs['href']                #에피소드 별 주소 추출\n",
    "            url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "            url_tag = url_base + url_tag             # 대본으로 가는 url 합치기\n",
    "        \n",
    "            url_list.append(url_tag)                 #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "        seasons_list.append(url_list)                #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "    #에피소드별 url에 접속 후 대본 가져오기\n",
    "    script_list = []\n",
    "    count = 1\n",
    "    print(seasons_list)\n",
    "    for script_url in seasons_list[count]:           #seasons_list 각 인덱스에 있는 url에 접속\n",
    "        script_resp = requests.get(script_url)\n",
    "        script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "        script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #대본 페이지 본문 긁어 오기\n",
    "        \n",
    "        #제한 설정: 시즌 2 대본까지만 긁어오기\n",
    "        count += 2\n",
    "        if count == 5:\n",
    "            break\n",
    "\n",
    "    #텍스트 파일 하나로 몰아넣기\n",
    "    def save_to_txt(series_name):\n",
    "        # 처음에 입력 받은 이름으로 파일 저장하기\n",
    "        file_name = series_name + '_script.txt'\n",
    "        with open(file_name, 'w', -1, 'utf-8') as f:   #'cp949' codec can't encode character '\\xe8' in position 11518: \n",
    "                                                       #illegal multibyte sequence 에러 때문에 'utf-8' 인코딩 수정\n",
    "\n",
    "            f.write('%s\\n' % series_name)         #시즌 몇번째인지\n",
    "            for scripts in script_list:      #해당 시즌 모든 에피소드 텍스트 파일에 입력\n",
    "                f.write('%s\\n' % scripts)\n",
    "        save_to_txt(series_name)                 #텍스트 파일로 저장\n",
    "##script crwaling end \n",
    "\n",
    "#미드 이미지 가져오기/et(img_url)\n",
    "def get_img(series_name):\n",
    "\n",
    "    driver_path = 'C:\\\\Users\\\\15Z970-GA5BK\\\\Downloads\\\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    \n",
    "    driver.get('https://www.google.com')\n",
    "    driver.implicitly_wait(10)\n",
    "    search_input = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "    search_input.send_keys(series_name)\n",
    "\n",
    "    search_btn = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[3]/center/input[1]').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    image_btn = driver.find_element_by_link_text('이미지').click()\n",
    "    \n",
    "    #구글 이미지 페이지 사진 긁어오기\n",
    "    img_url = 'https://www.google.com/search?q=' + series_name + '&source=lnms&tbm=isch&sa=X&ved=0'\n",
    "    resp2 = requests.get(img_url)\n",
    "    soup2 = BeautifulSoup(resp2.content, 'html.parser')\n",
    "    #print(soup2)\n",
    "    find_img = soup2.find('table', class_ = 'images_table')\n",
    "    #print(find_img)\n",
    "    find_img2 = find_img.find_all('tr')\n",
    "    print(len(find_img2))\n",
    "    \n",
    "    img_url_list = []\n",
    "    for td in find_img2:\n",
    "        #a_tag = td.find('td').find('a').find('img')\n",
    "        \n",
    "        #mg_src = a_tag.attrs['src']\n",
    "        #img_url_list.append(img_src)\n",
    "        img_tag = td.find_all('img')\n",
    "        for i in img_tag:\n",
    "            img_src = i.attrs['src']\n",
    "            img_url_list.append(img_src)\n",
    "    \n",
    "    print(img_url_list)\n",
    "    img_url_list2 = []\n",
    "    #image source 가져오기\n",
    "    for i in range(len(img_url_list)):\n",
    "        \n",
    "        img_url_list2.append(img_url_list[i])\n",
    "    \n",
    "    print(img_url_list2)\n",
    "    \n",
    "    #image source에 접근한 후 이미지 다운로드\n",
    "    count = 1\n",
    "    for i in img_url_list2:\n",
    "        img_url = i\n",
    "        resp = requests.get(i)\n",
    "        file_name = series_name  + str(count) + '.jpeg'  #번호 올리면서 이미지 저장\n",
    "        \n",
    "        \n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(resp.content)     \n",
    "        count += 1\n",
    "        if count == 30:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "#replace_space(), script_crawling(), #get_img()\n",
    "series = replace_space(input('원하는 미드를 입력하세요: '))\n",
    "#print(series) \n",
    "script_crawling(series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "#전역 변수\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "\n",
    "#입력 받은 값에서 띄어쓰기가 있는 경우 '-'로 변환\n",
    "def replace_space(series_name):  \n",
    "    \n",
    "    series_name = series_name.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "    series_name.lower()    #소문자로 바꾸기\n",
    "    return series_name\n",
    "\n",
    "\n",
    "\n",
    "#대본 크롤링\n",
    "def script_crawling(series_name):\n",
    "    params = {\n",
    "        'tv-show' : series_name\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params = params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    find_frame = soup.find('div', class_ = 'main-content-left')           #가장 바깥 프레임 설정\n",
    "    find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "\n",
    "    seasons_list = []                                #시즌 태그와 하위 태그 값들을 모두 이곳에 저장\n",
    "\n",
    "    for season in find_seasons:\n",
    "        #시즌 이름 추출                               \n",
    "        seasons_list.append(season.find('h3').text)  # 시즌 numbering   \n",
    "        #에피소드 태그 추출               \n",
    "        a_tags = season.find_all('a')    \n",
    "        url_list = []                                #각 에피소드들의 주소가 담겨 있는 리스트\n",
    "        for a in a_tags:\n",
    "            url_tag = a.attrs['href']                #에피소드 별 주소 추출\n",
    "            url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "            url_tag = url_base + url_tag             # 대본으로 가는 url 합치기\n",
    "        \n",
    "            url_list.append(url_tag)                 #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "        seasons_list.append(url_list)                #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "    #에피소드별 url에 접속 후 대본 가져오기\n",
    "    script_list = []\n",
    "    count = 1\n",
    "    print(seasons_list)\n",
    "    for script_url in seasons_list[count]:           #seasons_list 각 인덱스에 있는 url에 접속\n",
    "        script_resp = requests.get(script_url)\n",
    "        script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "        script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #대본 페이지 본문 긁어 오기\n",
    "        \n",
    "        #제한 설정: 시즌 2 대본까지만 긁어오기\n",
    "        count += 2\n",
    "        if count == 5:\n",
    "            break\n",
    "\n",
    "    #텍스트 파일 하나로 몰아넣기\n",
    "    def save_to_txt(series_name):\n",
    "        # 처음에 입력 받은 이름으로 파일 저장하기\n",
    "        file_name = series_name + '_script.txt'\n",
    "        with open(file_name, 'w', -1, 'utf-8') as f:   #'cp949' codec can't encode character '\\xe8' in position 11518: \n",
    "                                                       #illegal multibyte sequence 에러 때문에 'utf-8' 인코딩 수정\n",
    "\n",
    "            f.write('%s\\n' % series_name)         #시즌 몇번째인지\n",
    "            for scripts in script_list:      #해당 시즌 모든 에피소드 텍스트 파일에 입력\n",
    "                f.write('%s\\n' % scripts)\n",
    "            \n",
    "            save_to_txt(series_name)                 #텍스트 파일로 저장\n",
    "##script crwaling end \n",
    "\n",
    "#미드 이미지 가져오기/et(img_url)\n",
    "def get_img(series_name):\n",
    "\n",
    "    driver_path = 'C:\\\\Users\\\\15Z970-GA5BK\\\\Downloads\\\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    \n",
    "    driver.get('https://www.google.com')\n",
    "    driver.implicitly_wait(10)\n",
    "    search_input = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "    search_input.send_keys(series_name)\n",
    "\n",
    "    search_btn = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[3]/center/input[1]').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    image_btn = driver.find_element_by_link_text('이미지').click()\n",
    "    \n",
    "    #구글 이미지 페이지 사진 긁어오기\n",
    "    img_url = 'https://www.google.com/search?q=' + series_name + '&source=lnms&tbm=isch&sa=X&ved=0'\n",
    "    resp2 = requests.get(img_url)\n",
    "    soup2 = BeautifulSoup(resp2.content, 'html.parser')\n",
    "    #print(soup2)\n",
    "    find_img = soup2.find('table', class_ = 'images_table')\n",
    "    #print(find_img)\n",
    "    find_img2 = find_img.find_all('tr')\n",
    "    print(len(find_img2))\n",
    "    \n",
    "    # img 태그 속 src 속성 리스트에 저장하기\n",
    "    img_url_list = []\n",
    "    for td in find_img2:\n",
    "        img_tag = td.find_all('img')\n",
    "        for i in img_tag:\n",
    "            img_src = i.attrs['src']\n",
    "            img_url_list.append(img_src)\n",
    "    \n",
    "    print(img_url_list)\n",
    "\n",
    "    #image source에 접근한 후 이미지 다운로드\n",
    "    count = 1\n",
    "    for i in img_url_list:\n",
    "        img_url = i\n",
    "        resp = requests.get(i)\n",
    "        file_name = series_name  + str(count) + '.jpeg'  #번호 올리면서 이미지 저장\n",
    "        \n",
    "        \n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(resp.content)     \n",
    "        count += 1\n",
    "        if count == 30:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "#replace_space(), script_crawling(), #get_img()\n",
    "series = replace_space(input('원하는 미드를 입력하세요: '))\n",
    "#print(series) \n",
    "#get_img(series)\n",
    "script_crawling(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
