{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "#전역 변수\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "\n",
    "#입력 받은 값에서 띄어쓰기가 있는 경우 '-'로 변환\n",
    "def replace_space(series_name):  \n",
    "    \n",
    "    series_name = series_name.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "    series_name.lower()    #소문자로 바꾸기\n",
    "    return series_name\n",
    "\n",
    "\n",
    "\n",
    "#대본 크롤링\n",
    "def script_crawling(series_name):\n",
    "    params = {\n",
    "        'tv-show' : series_name\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params = params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    find_frame = soup.find('div', class_ = 'main-content-left')           #가장 바깥 프레임 설정\n",
    "    find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "    season_list = soup.select('.div>.season-episodes')\n",
    "    print(season_list)\n",
    "    seasons_list = []                                #시즌 태그와 하위 태그 값들을 모두 이곳에 저장\n",
    "\n",
    "    for season in find_seasons:\n",
    "        #시즌 이름 추출                               \n",
    "        seasons_list.append(season.find('h3').text)  # 시즌 numbering   \n",
    "        #에피소드 태그 추출               \n",
    "        a_tags = season.find_all('a')    \n",
    "        url_list = []                                #각 에피소드들의 주소가 담겨 있는 리스트\n",
    "        for a in a_tags:\n",
    "            url_tag = a.attrs['href']                #에피소드 별 주소 추출\n",
    "            url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "            url_tag = url_base + url_tag             # 대본으로 가는 url 합치기\n",
    "        \n",
    "            url_list.append(url_tag)                 #에피소드 대본 url a_tag_list에 넣기\n",
    "        \n",
    "        seasons_list.append(url_list)                #시즌별 에피소드 url_list에 추가\n",
    "    \n",
    "    #에피소드별 url에 접속 후 대본 가져오기\n",
    "    script_list = []\n",
    "    count = 1\n",
    "    for script_url in seasons_list[count]:           #seasons_list 각 인덱스에 있는 url에 접속\n",
    "        script_resp = requests.get(script_url)\n",
    "        script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "        script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #대본 페이지 본문 긁어 오기\n",
    "        \n",
    "        #제한 설정: 시즌 2 대본까지만 긁어오기\n",
    "        count += 2\n",
    "        if count == 9:\n",
    "            break\n",
    "    \n",
    "    #텍스트 파일 하나로 몰아넣기\n",
    "    def save_to_txt(series_name):\n",
    "        # 처음에 입력 받은 이름으로 파일 저장하기\n",
    "        file_name = series_name + '_script.txt'\n",
    "        print(file_name)\n",
    "        with open(file_name, 'w', -1, 'utf-8') as f:   #'cp949' codec can't encode character '\\xe8' in position 11518: \n",
    "                                                       #illegal multibyte sequence 에러 때문에 'utf-8' 인코딩 수정\n",
    "            f.write('%s\\n' % series_name)         #시즌 몇번째인지\n",
    "            for scripts in script_list:      #해당 시즌 모든 에피소드 텍스트 파일에 입력\n",
    "                f.write('%s\\n' % scripts)\n",
    "            \n",
    "    save_to_txt(series_name)                 #텍스트 파일로 저장\n",
    "##script crwaling end \n",
    "\n",
    "#미드 이미지 가져오기/et(img_url)\n",
    "def get_img(series_name):\n",
    "\n",
    "    driver_path = 'C:\\\\Users\\\\15Z970-GA5BK\\\\Downloads\\\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    \n",
    "    driver.get('https://www.google.com')\n",
    "    driver.implicitly_wait(10)\n",
    "    search_input = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "    search_input.send_keys(series_name)\n",
    "\n",
    "    search_btn = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[3]/center/input[1]').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    image_btn = driver.find_element_by_link_text('이미지').click()\n",
    "    \n",
    "    #구글 이미지 \n",
    "    # 첫번째 페이지 사진 긁어오기\n",
    "    img_url = 'https://www.google.com/search?q=' + series_name + '&source=lnms&tbm=isch&sa=X&ved=0'\n",
    "    resp2 = requests.get(img_url)\n",
    "    soup2 = BeautifulSoup(resp2.content, 'html.parser')\n",
    "    #print(soup2)\n",
    "    find_img = soup2.find('table', class_ = 'images_table')\n",
    "    #print(find_img)\n",
    "    find_img2 = find_img.find_all('tr')\n",
    "    \n",
    "    # img 태그 속 src 속성 리스트에 저장하기\n",
    "    img_url_list = []\n",
    "    for td in find_img2:\n",
    "        img_tag = td.find_all('img')\n",
    "        for i in img_tag:\n",
    "            img_src = i.attrs['src']\n",
    "            img_url_list.append(img_src)\n",
    "    \n",
    "\n",
    "    #다음 페이지 이미지 불러오기\n",
    "    img_count = 0\n",
    "    img_next_url = 'https://www.google.com/search?q=' + series_name + '&biw=1036&bih=674&gbv=1&tbm=isch&ei=jWv6XKqsLo3VmAXk8ILQBg&start=' + str(img_count)\n",
    "    resp3 = requests.get(img_next_url)\n",
    "    soup3 = BeautifulSoup(resp3.content, 'html.parser')\n",
    "    find_img = soup3.find('table', class_ = 'images_table')\n",
    "    find_next_img = find_img.find_all('tr')\n",
    "    \n",
    "    # url에서 'start=' 값이 20씩 늘어날 때마다 다음 페이지로 넘어감\n",
    "    while img_count <= 100:\n",
    "        for td in find_next_img:\n",
    "            img_tag = td.find_all('img')\n",
    "            for i in img_tag:\n",
    "                img_src = i.attrs['src']\n",
    "                img_url_list.append(img_src) #기존 img_url_list에 추가로 append\n",
    "            img_count += 20\n",
    "        \n",
    "    #image source에 접근한 후 이미지 다운로드\n",
    "    count = 1\n",
    "    while count <= 50:\n",
    "        for i in img_url_list:\n",
    "            img_url = i\n",
    "            resp = requests.get(i)\n",
    "            file_name = series_name  + str(count) + '.jpeg'  #번호 올리면서 이미지 저장\n",
    "        \n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(resp.content)     \n",
    "            count += 1\n",
    "            \n",
    "\n",
    "\n",
    "# 함수= replace_space(), script_crawling(), #get_img()\n",
    "series = replace_space(input('원하는 미드를 입력하세요: '))\n",
    "#print(series) \n",
    "get_img(series)\n",
    "#script_crawling(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원하는 미드를 입력하세요: breaking bad\n",
      "['https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s01e01', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s01e02', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s01e03', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s01e04', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s01e05', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s01e06', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s01e07', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e01', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e02', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e03', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e04', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e05', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e06', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e07', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e08', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e09', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e10', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e11', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e12', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s02e13', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e01', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e02', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e03', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e04', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e05', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e06', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e07', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e08', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e09', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e10', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e11', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e12', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s03e13', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e01', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e02', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e03', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e04', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e05', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e06', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e07', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e08', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e09', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e10', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e11', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e12', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s04e13', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e01', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e02', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e03', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e04', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e05', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e06', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e07', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e08', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e09', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e10', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e11', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e12', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e13', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e14', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e15', 'https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s05e16']\n",
      "['01e01', '01e02', '01e03', '01e04', '01e05', '01e06', '01e07', '02e01', '02e02', '02e03', '02e04', '02e05', '02e06', '02e07', '02e08', '02e09', '02e10', '02e11', '02e12', '02e13', '03e01', '03e02', '03e03', '03e04', '03e05', '03e06', '03e07', '03e08', '03e09', '03e10', '03e11', '03e12', '03e13', '04e01', '04e02', '04e03', '04e04', '04e05', '04e06', '04e07', '04e08', '04e09', '04e10', '04e11', '04e12', '04e13', '05e01', '05e02', '05e03', '05e04', '05e05', '05e06', '05e07', '05e08', '05e09', '05e10', '05e11', '05e12', '05e13', '05e14', '05e15', '05e16']\n",
      "breaking-bad_script.txt\n"
     ]
    }
   ],
   "source": [
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "#전역 변수\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "\n",
    "#입력 받은 값에서 띄어쓰기가 있는 경우 '-'로 변환\n",
    "def replace_space(series_name):  \n",
    "    \n",
    "    series_name = series_name.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "    series_name.lower()    #소문자로 바꾸기\n",
    "    return series_name\n",
    "\n",
    "\n",
    "\n",
    "#대본 크롤링\n",
    "def script_crawling(series_name):\n",
    "    params = {\n",
    "        'tv-show' : series_name\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params = params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    find_frame = soup.find('div', class_ = 'main-content-left')           #가장 바깥 프레임 설정\n",
    "    find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "                                                                          #find_seasons[0] = season1\n",
    "\n",
    "    url_list = []                                     #각 에피소드들의 주소를 담을 리스트\n",
    "    url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "    for season in find_seasons:                            \n",
    "        #에피소드 태그 추출               \n",
    "        a_tags = season.find_all('a')    \n",
    "                                      \n",
    "        for a in a_tags:\n",
    "            url_tag = a.attrs['href']                #에피소드 별 주소 추출            \n",
    "            url_tag = url_base + url_tag             # 대본으로 가는 url 합치기        \n",
    "            url_list.append(url_tag)                 #에피소드 대본 url a_tag_list에 넣기            \n",
    "\n",
    "    print(url_list)\n",
    "\n",
    "    #https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=breaking-bad&episode=s01e01  's01e01' 만 빼오기    \n",
    "    episode_list = []\n",
    "    for i in url_list:    \n",
    "        episode = i[-5:] \n",
    "        episode_list.append(episode)\n",
    "    print(episode_list)\n",
    "    \n",
    "    #에피소드별 url에 접속 후 대본 가져오기\n",
    "    script_list = []\n",
    "    count = 0\n",
    "    for script_url in url_list:           #seasons_list 각 인덱스에 있는 url에 접속\n",
    "        script_resp = requests.get(script_url)\n",
    "        script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "        script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #대본 페이지 본문 긁어 오기\n",
    "        \n",
    "        #제한 설정: 10개 에피소드 긁어오기\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "    \n",
    "    #텍스트 파일 하나로 몰아넣기\n",
    "    def save_to_txt(series_name):\n",
    "        # 처음에 입력 받은 이름으로 파일 저장하기\n",
    "        file_name = series_name + '_script.txt'\n",
    "        print(file_name)\n",
    "        count = 0\n",
    "        with open(file_name, 'w', -1, 'utf-8') as f:   #'cp949' codec can't encode character '\\xe8' in position 11518: \n",
    "                                                       #illegal multibyte sequence 에러 때문에 'utf-8' 인코딩 수정\n",
    "            f.write('%s\\n' % series_name)              # 미드 이름\n",
    "            for scripts in script_list:                #해당 시즌 모든 에피소드 텍스트 파일에 입력\n",
    "                f.write('\\n-----------------------------------%s' % episode_list[count])\n",
    "                f.write('\\n%s' % scripts)\n",
    "                count += 1\n",
    "            \n",
    "    save_to_txt(series_name)                 #텍스트 파일로 저장\n",
    "##script crwaling end \n",
    "\n",
    "\n",
    "#미드 이미지 가져오기/et(img_url)\n",
    "def get_img(series_name):\n",
    "\n",
    "    driver_path = 'C:\\\\Users\\\\15Z970-GA5BK\\\\Downloads\\\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    \n",
    "    driver.get('https://www.google.com')\n",
    "    driver.implicitly_wait(10)\n",
    "    search_input = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[1]/div/div[1]/input')\n",
    "    search_input.send_keys(series_name)\n",
    "\n",
    "    search_btn = driver.find_element_by_xpath('//*[@id=\"tsf\"]/div[2]/div/div[3]/center/input[1]').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    image_btn = driver.find_element_by_link_text('이미지').click()\n",
    "    \n",
    "    #구글 이미지 \n",
    "    #다음 페이지 이미지 불러오기\n",
    "    img_count = 0   \n",
    "    img_url_list = []\n",
    "    while img_count < 40:\n",
    "        img_next_url = 'https://www.google.com/search?q=' + series_name + \\\n",
    "                         '&biw=1036&bih=674&gbv=1&tbm=isch&ei=jWv6XKqsLo3VmAXk8ILQBg&start=' + str(img_count)\n",
    "        # url에서 'start=' 값이 20씩 늘어날 때마다 다음 페이지로 넘어감\n",
    "        resp3 = requests.get(img_next_url)\n",
    "        soup3 = BeautifulSoup(resp3.content, 'html.parser')\n",
    "        find_img = soup3.find('table', class_ = 'images_table')\n",
    "        find_next_img = find_img.find_all('tr')\n",
    "        for td in find_next_img:\n",
    "            img_tag = td.find_all('img')\n",
    "            for i in img_tag:\n",
    "                img_src = i.attrs['src']\n",
    "                img_url_list.append(img_src) #기존 img_url_list에 추가로 append\n",
    "        img_count += 20\n",
    "    #image source에 접근한 후 이미지 다운로드\n",
    "\n",
    "    \n",
    "    # 요기 이상함 count = 1 위치만 바꿧는데 매우 이상해짐\n",
    "    count = 0\n",
    "    while count <= 10:\n",
    "        for i in img_url_list:\n",
    "            count += 1\n",
    "            img_url = i\n",
    "            resp = requests.get(i)\n",
    "            file_name = series_name  + str(count) + '.jpeg'  #번호 올리면서 이미지 저장\n",
    "        \n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(resp.content)     \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "# 함수= replace_space(), script_crawling(), #get_img()\n",
    "series = replace_space(input('원하는 미드를 입력하세요: '))\n",
    "#print(series) \n",
    "get_img(series)\n",
    "script_crawling(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from urllib import parse\n",
    "#전역 변수\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "\n",
    "#입력 받은 값에서 띄어쓰기가 있는 경우 '-'로 변환\n",
    "def replace_space(series_name):  \n",
    "    \n",
    "    series_name = series_name.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "    series_name.lower()    #소문자로 바꾸기\n",
    "    return series_name\n",
    "\n",
    "\n",
    "\n",
    "#대본 크롤링\n",
    "def script_crawling(series_name):\n",
    "    params = {\n",
    "        'tv-show' : series_name\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params = params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    season_list = soup.select('.season-episodes')\n",
    "    print(season_list[1])\n",
    "    seasons_list = []                                #시즌 태그와 하위 태그 값들을 모두 이곳에 저장\n",
    "    season_dict = {}\n",
    "    for season_li in season_list:\n",
    "        link_url = season_li.a['href']\n",
    "        episode = get_param_from_href(link_url, 'episode')\n",
    "    \n",
    "        season_dict[series_name] ={\n",
    "            'episode' : episode,\n",
    "            'link' : link_url\n",
    "        }\n",
    "    print(season_dict)\n",
    "def get_param_from_href(href, param):\n",
    "    url = parse.urlparse(href)\n",
    "    url_query = parse.parse_qs(url.query)\n",
    "    return url_query[param][0]\n",
    "\n",
    "series = replace_space(input('원하는 미드를 입력하세요: '))\n",
    "#print(series) \n",
    "#get_img(series)\n",
    "script_crawling(series)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the simpsons\n",
    "#american horror story\n",
    "#how i met your mother\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "#전역 변수\n",
    "url = 'https://www.springfieldspringfield.co.uk/episode_scripts.php?'\n",
    "\n",
    "#입력 받은 값에서 띄어쓰기가 있는 경우 '-'로 변환\n",
    "def replace_space(series_name):  \n",
    "    \n",
    "    series_name = series_name.replace(\" \", \"-\") #띄어쓰기는 url주소에 쓰이기 위해서 '-'로 변환\n",
    "    series_name.lower()    #소문자로 바꾸기\n",
    "    return series_name\n",
    "\n",
    "\n",
    "\n",
    "#대본 크롤링\n",
    "def script_crawling(series_name):\n",
    "    params = {\n",
    "        'tv-show' : series_name\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params = params)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    find_frame = soup.find('div', class_ = 'main-content-left')           #가장 바깥 프레임 설정\n",
    "    find_seasons = find_frame.find_all('div', class_ = 'season-episodes') #모든 시즌 가져오기\n",
    "\n",
    "\n",
    "    url_list = []                                     #각 에피소드들의 주소가 담겨 있는 리스트\n",
    "    url_base = 'https://www.springfieldspringfield.co.uk/'  \n",
    "    for season in find_seasons:                            \n",
    "        #에피소드 태그 추출               \n",
    "        a_tags = season.find_all('a')    \n",
    "                                      \n",
    "        for a in a_tags:\n",
    "            url_tag = a.attrs['href']                #에피소드 별 주소 추출            \n",
    "            url_tag = url_base + url_tag             # 대본으로 가는 url 합치기        \n",
    "            url_list.append(url_tag)                 #에피소드 대본 url a_tag_list에 넣기            \n",
    "\n",
    "    print(url_list)\n",
    "    \n",
    "    episode_list = []\n",
    "    for i in url_list:    \n",
    "        episode = i[-5:] #https://www.springfieldspringfield.co.uk/view_episode_scripts.php?\n",
    "                            #tv-show=breaking-bad&episode=s01e01  's01e01' 만 빼오기\n",
    "        episode_list.append(episode)\n",
    "    print(episode_list)\n",
    "    #에피소드별 url에 접속 후 대본 가져오기\n",
    "    script_list = []\n",
    "    count = 0\n",
    "    for script_url in url_list:           #seasons_list 각 인덱스에 있는 url에 접속\n",
    "        script_resp = requests.get(script_url)\n",
    "        script_soup = BeautifulSoup(script_resp.content, 'html.parser')\n",
    "        script_list.append(script_soup.find('div', class_ = 'scrolling-script-container').text)  #대본 페이지 본문 긁어 오기\n",
    "        \n",
    "        #제한 설정: 10개 에피소드 긁어오기\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "    \n",
    "    #텍스트 파일 하나로 몰아넣기\n",
    "    def save_to_txt(series_name):\n",
    "        # 처음에 입력 받은 이름으로 파일 저장하기\n",
    "        file_name = series_name + '_script.txt'\n",
    "        print(file_name)\n",
    "        count = 0\n",
    "        with open(file_name, 'w', -1, 'utf-8') as f:   #'cp949' codec can't encode character '\\xe8' in position 11518: \n",
    "                                                       #illegal multibyte sequence 에러 때문에 'utf-8' 인코딩 수정\n",
    "            f.write('%s\\n' % series_name)         # 미드 이름\n",
    "            for scripts in script_list:      #해당 시즌 모든 에피소드 텍스트 파일에 입력\n",
    "                \n",
    "                f.write('%s\\n' % episode_list[count])\n",
    "                f.write('%s\\n' % scripts)\n",
    "                count += 1\n",
    "            \n",
    "    save_to_txt(series_name)                 #텍스트 파일로 저장\n",
    "##script crwaling end \n",
    "\n",
    "# 함수= replace_space(), script_crawling(), #get_img()\n",
    "series = replace_space(input('원하는 미드를 입력하세요: '))\n",
    "#print(series) \n",
    "#get_img(series)\n",
    "script_crawling(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
