{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/2d/d4ad0ef5159e87d66339f1dfaa4736687bf2699c3b8ce9e57998f28bb5a1/gym-0.13.0.tar.gz (1.6MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\15z970-ga5bk\\anaconda3\\lib\\site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\15z970-ga5bk\\anaconda3\\lib\\site-packages (from gym) (1.16.2)\n",
      "Requirement already satisfied: six in c:\\users\\15z970-ga5bk\\anaconda3\\lib\\site-packages (from gym) (1.12.0)\n",
      "Collecting pyglet>=1.2.0 (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "Collecting cloudpickle~=1.2.0 (from gym)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/f4/4a080c349c1680a2086196fcf0286a65931708156f39568ed7051e42ff6a/cloudpickle-1.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: future in c:\\users\\15z970-ga5bk\\anaconda3\\lib\\site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\15Z970-GA5BK\\AppData\\Local\\pip\\Cache\\wheels\\00\\b8\\b2\\e5dfb6be621560717e719734293eee9fe3c66b668fdc334b1a\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, cloudpickle, gym\n",
      "  Found existing installation: cloudpickle 0.8.0\n",
      "    Uninstalling cloudpickle-0.8.0:\n",
      "      Successfully uninstalled cloudpickle-0.8.0\n",
      "Successfully installed cloudpickle-1.2.1 gym-0.13.0 pyglet-1.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spyder 3.3.3 requires pyqt5<=5.12; python_version >= \"3\", which is not installed.\n"
     ]
    }
   ],
   "source": [
    "# https://gym.openai.com/\n",
    "# https://gym.openai.com/docs/\n",
    "try:\n",
    "    import gym\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install gym\n",
    "    import gym\n",
    "finally:\n",
    "    from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym 환경설정\n",
    "# https://gym.openai.com/envs/ 에 들어가면 다양한 환경을 살펴볼 수 있음\n",
    "# Frozen-Lake : https://gym.openai.com/envs/FrozenLake-v0/ \n",
    "register(\n",
    "    id = 'FrozenLake-v1', \n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv', \n",
    "    kwargs={\n",
    "        # 'is_slippery': False 이면 Deteministic 환경.\n",
    "        # 'is_slippery': True 이면 Stochastic 환경.\n",
    "        'map_name' : '4x4', # 4x4 크기의 맵\n",
    "        'is_slippery' : False # 미끄러질 가능성 없음 (나중에 다룰 것)   #내가 원하는대로 갈 확률 33% \n",
    "                              #is slippery가 false면 무조건 내가 원하는 대로 움직임\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(16), Discrete(4), 16, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space, env.observation_space.n, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3\n",
    "\n",
    "# 임의로 방향 하나를 선택함.\n",
    "action = env.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위치:  5\n",
      "보상:  0\n",
      "Game Over?:  True\n",
      "정보(확률):  {'prob': 1.0}\n",
      "행동:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"위치: \", observation)\n",
    "print(\"보상: \",reward)\n",
    "print(\"Game Over?: \",done)\n",
    "print(\"정보(확률): \", info) # 내가 의도한대로 갈 확률\n",
    "print(\"행동: \", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np      ####  http://yujuwon.tistory.com/entry/NumPy\n",
    "import random\n",
    "\n",
    "\n",
    "# np.zeros((16, 4))\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))    # (16,4) : 4*4 map + 상하좌우 4개\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000 #게임 총 몇번 할거야?\n",
    "learning_rate = 0.3 #보상을 받았을 때 얼마의 비율로 업데이트를 할것인가\n",
    "discount_rate = 0.99 #할인율: 바로 직전 뿐만 아니라 과거에 했떤 모든 경험들이 얼마나 큰 비중을 경험에 차지할 것인가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번 진행중..\n",
      "500 번 진행중..\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "for epi in range(num_episodes):\n",
    "    observation = env.reset() # 초기화\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    #게임 시작점\n",
    "    for t in range(100):\n",
    "        # 1턴 실행 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # 학습을 빨리 하기 위해서 넣는 코드\n",
    "        # 없으면 계속 LEFT로 가기 때문\n",
    "        # 전체가 0이면 처리\n",
    "        if not Q[current_state, :].any():\n",
    "            action = env.action_space.sample()\n",
    "        elif np.random.rand() < 0.5:\n",
    "            # 랜덤 선택\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # 최대값\n",
    "            action = np.argmax(Q[current_state])\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 성공 사례를 만들어야 측정이 가능함.\n",
    "        # 현 위치의 최대값 = 성공 가능성\n",
    "        # 이전 위치의 현 방향 선택 = 기존의 성공 가능성\n",
    "        Q[current_state, action] += learning_rate * \\ #최댓값 1\n",
    "        (reward + discount_rate * \\ #최솟값 -1\n",
    "         np.max(Q[observation, :]) - \\\n",
    "         # reward가 1이면 observation은 0이 된다. \n",
    "         Q[current_state, action]) #최댓값 0.3\n",
    "        #최댓값을 빼었기 때문에 기댓값\n",
    "        if done:\n",
    "            episode_reward +=reward\n",
    "            break\n",
    "    if epi % 500 == 0:\n",
    "        print(epi, \"번 진행중..\")\n",
    "    total_reward += episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.364"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward/ num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94148015, 0.95099002, 0.95099005, 0.94148015],\n",
       "       [0.94148015, 0.        , 0.96059601, 0.95099005],\n",
       "       [0.95099005, 0.970299  , 0.95099005, 0.96059601],\n",
       "       [0.96059601, 0.        , 0.95050301, 0.95098723],\n",
       "       [0.94986082, 0.960596  , 0.        , 0.94148015],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9801    , 0.        , 0.96059601],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.9210285 , 0.        , 0.970299  , 0.87193565],\n",
       "       [0.94998446, 0.9797854 , 0.9801    , 0.        ],\n",
       "       [0.970299  , 0.99      , 0.        , 0.970299  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.97129604, 0.99      , 0.93666834],\n",
       "       [0.9801    , 0.99      , 1.        , 0.9801    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q\n",
    "#랜덤으로 판을 돌다가 G에 골인하게 되면 그제서야 최초 학습이 적용됨. \n",
    "#G에 골인하면 뒤에서부터, 16번째에 골인이면 15번째의 선택을 학습하면서 거꾸로 스타트까지 올라감.\n",
    "# 그렇게 학습률 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.94148015, 0.95099002, 0.95099005, 0.94148015],\n",
       "        [0.94148015, 0.        , 0.96059601, 0.95099005],\n",
       "        [0.95099005, 0.970299  , 0.95099005, 0.96059601],\n",
       "        [0.96059601, 0.        , 0.95050301, 0.95098723]],\n",
       "\n",
       "       [[0.94986082, 0.960596  , 0.        , 0.94148015],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.9801    , 0.        , 0.96059601],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.9210285 , 0.        , 0.970299  , 0.87193565],\n",
       "        [0.94998446, 0.9797854 , 0.9801    , 0.        ],\n",
       "        [0.970299  , 0.99      , 0.        , 0.970299  ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.97129604, 0.99      , 0.93666834],\n",
       "        [0.9801    , 0.99      , 1.        , 0.9801    ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.reshape(4,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n",
      "34.0\n",
      "35.0\n",
      "36.0\n",
      "37.0\n",
      "38.0\n",
      "39.0\n",
      "40.0\n",
      "41.0\n",
      "42.0\n",
      "43.0\n",
      "44.0\n",
      "45.0\n",
      "46.0\n",
      "47.0\n",
      "48.0\n",
      "49.0\n",
      "50.0\n",
      "51.0\n",
      "52.0\n",
      "53.0\n",
      "54.0\n",
      "55.0\n",
      "56.0\n",
      "57.0\n",
      "58.0\n",
      "59.0\n",
      "60.0\n",
      "61.0\n",
      "62.0\n",
      "63.0\n",
      "64.0\n",
      "65.0\n",
      "66.0\n",
      "67.0\n",
      "68.0\n",
      "69.0\n",
      "70.0\n",
      "71.0\n",
      "72.0\n",
      "73.0\n",
      "74.0\n",
      "75.0\n",
      "76.0\n",
      "77.0\n",
      "78.0\n",
      "79.0\n",
      "80.0\n",
      "81.0\n",
      "82.0\n",
      "83.0\n",
      "84.0\n",
      "85.0\n",
      "86.0\n",
      "87.0\n",
      "88.0\n",
      "89.0\n",
      "90.0\n",
      "91.0\n",
      "92.0\n",
      "93.0\n",
      "94.0\n",
      "95.0\n",
      "96.0\n",
      "97.0\n",
      "98.0\n",
      "99.0\n",
      "100.0\n",
      "101.0\n",
      "102.0\n",
      "103.0\n",
      "104.0\n",
      "105.0\n",
      "106.0\n",
      "107.0\n",
      "108.0\n",
      "109.0\n",
      "110.0\n",
      "111.0\n",
      "112.0\n",
      "113.0\n",
      "114.0\n",
      "115.0\n",
      "116.0\n",
      "117.0\n",
      "118.0\n",
      "119.0\n",
      "120.0\n",
      "121.0\n",
      "122.0\n",
      "123.0\n",
      "124.0\n",
      "125.0\n",
      "126.0\n",
      "127.0\n",
      "128.0\n",
      "129.0\n",
      "130.0\n",
      "131.0\n",
      "132.0\n",
      "133.0\n",
      "134.0\n",
      "135.0\n",
      "136.0\n",
      "137.0\n",
      "138.0\n",
      "139.0\n",
      "140.0\n",
      "141.0\n",
      "142.0\n",
      "143.0\n",
      "144.0\n",
      "145.0\n",
      "146.0\n",
      "147.0\n",
      "148.0\n",
      "149.0\n",
      "150.0\n",
      "151.0\n",
      "152.0\n",
      "153.0\n",
      "154.0\n",
      "155.0\n",
      "156.0\n",
      "157.0\n",
      "158.0\n",
      "159.0\n",
      "160.0\n",
      "161.0\n",
      "162.0\n",
      "163.0\n",
      "164.0\n",
      "165.0\n",
      "166.0\n",
      "167.0\n",
      "168.0\n",
      "169.0\n",
      "170.0\n",
      "171.0\n",
      "172.0\n",
      "173.0\n",
      "174.0\n",
      "175.0\n",
      "176.0\n",
      "177.0\n",
      "178.0\n",
      "179.0\n",
      "180.0\n",
      "181.0\n",
      "182.0\n",
      "183.0\n",
      "184.0\n",
      "185.0\n",
      "186.0\n",
      "187.0\n",
      "188.0\n",
      "189.0\n",
      "190.0\n",
      "191.0\n",
      "192.0\n",
      "193.0\n",
      "194.0\n",
      "195.0\n",
      "196.0\n",
      "197.0\n",
      "198.0\n",
      "199.0\n",
      "200.0\n",
      "201.0\n",
      "202.0\n",
      "203.0\n",
      "204.0\n",
      "205.0\n",
      "206.0\n",
      "207.0\n",
      "208.0\n",
      "209.0\n",
      "210.0\n",
      "211.0\n",
      "212.0\n",
      "213.0\n",
      "214.0\n",
      "215.0\n",
      "216.0\n",
      "217.0\n",
      "218.0\n",
      "219.0\n",
      "220.0\n",
      "221.0\n",
      "222.0\n",
      "223.0\n",
      "224.0\n",
      "225.0\n",
      "226.0\n",
      "227.0\n",
      "228.0\n",
      "229.0\n",
      "230.0\n",
      "231.0\n",
      "232.0\n",
      "233.0\n",
      "234.0\n",
      "235.0\n",
      "236.0\n",
      "237.0\n",
      "238.0\n",
      "239.0\n",
      "240.0\n",
      "241.0\n",
      "242.0\n",
      "243.0\n",
      "244.0\n",
      "245.0\n",
      "246.0\n",
      "247.0\n",
      "248.0\n",
      "249.0\n",
      "250.0\n",
      "251.0\n",
      "252.0\n",
      "253.0\n",
      "254.0\n",
      "255.0\n",
      "256.0\n",
      "257.0\n",
      "258.0\n",
      "259.0\n",
      "260.0\n",
      "261.0\n",
      "262.0\n",
      "263.0\n",
      "264.0\n",
      "265.0\n",
      "266.0\n",
      "267.0\n",
      "268.0\n",
      "269.0\n",
      "270.0\n",
      "271.0\n",
      "272.0\n",
      "273.0\n",
      "274.0\n",
      "275.0\n",
      "276.0\n",
      "277.0\n",
      "278.0\n",
      "279.0\n",
      "280.0\n",
      "281.0\n",
      "282.0\n",
      "283.0\n",
      "284.0\n",
      "285.0\n",
      "286.0\n",
      "287.0\n",
      "288.0\n",
      "289.0\n",
      "290.0\n",
      "291.0\n",
      "292.0\n",
      "293.0\n",
      "294.0\n",
      "295.0\n",
      "296.0\n",
      "297.0\n",
      "298.0\n",
      "299.0\n",
      "300.0\n",
      "301.0\n",
      "302.0\n",
      "303.0\n",
      "304.0\n",
      "305.0\n",
      "306.0\n",
      "307.0\n",
      "308.0\n",
      "309.0\n",
      "310.0\n",
      "311.0\n",
      "312.0\n",
      "313.0\n",
      "314.0\n",
      "315.0\n",
      "316.0\n",
      "317.0\n",
      "318.0\n",
      "319.0\n",
      "320.0\n",
      "321.0\n",
      "322.0\n",
      "323.0\n",
      "324.0\n",
      "325.0\n",
      "326.0\n",
      "327.0\n",
      "328.0\n",
      "329.0\n",
      "330.0\n",
      "331.0\n",
      "332.0\n",
      "333.0\n",
      "334.0\n",
      "335.0\n",
      "336.0\n",
      "337.0\n",
      "338.0\n",
      "339.0\n",
      "340.0\n",
      "341.0\n",
      "342.0\n",
      "343.0\n",
      "344.0\n",
      "345.0\n",
      "346.0\n",
      "347.0\n",
      "348.0\n",
      "349.0\n",
      "350.0\n",
      "351.0\n",
      "352.0\n",
      "353.0\n",
      "354.0\n",
      "355.0\n",
      "356.0\n",
      "357.0\n",
      "358.0\n",
      "359.0\n",
      "360.0\n",
      "361.0\n",
      "362.0\n",
      "363.0\n",
      "364.0\n",
      "365.0\n",
      "366.0\n",
      "367.0\n",
      "368.0\n",
      "369.0\n",
      "370.0\n",
      "371.0\n",
      "372.0\n",
      "373.0\n",
      "374.0\n",
      "375.0\n",
      "376.0\n",
      "377.0\n",
      "378.0\n",
      "379.0\n",
      "380.0\n",
      "381.0\n",
      "382.0\n",
      "383.0\n",
      "384.0\n",
      "385.0\n",
      "386.0\n",
      "387.0\n",
      "388.0\n",
      "389.0\n",
      "390.0\n",
      "391.0\n",
      "392.0\n",
      "393.0\n",
      "394.0\n",
      "395.0\n",
      "396.0\n",
      "397.0\n",
      "398.0\n",
      "399.0\n",
      "400.0\n",
      "401.0\n",
      "402.0\n",
      "403.0\n",
      "404.0\n",
      "405.0\n",
      "406.0\n",
      "407.0\n",
      "408.0\n",
      "409.0\n",
      "410.0\n",
      "411.0\n",
      "412.0\n",
      "413.0\n",
      "414.0\n",
      "415.0\n",
      "416.0\n",
      "417.0\n",
      "418.0\n",
      "419.0\n",
      "420.0\n",
      "421.0\n",
      "422.0\n",
      "423.0\n",
      "424.0\n",
      "425.0\n",
      "426.0\n",
      "427.0\n",
      "428.0\n",
      "429.0\n",
      "430.0\n",
      "431.0\n",
      "432.0\n",
      "433.0\n",
      "434.0\n",
      "435.0\n",
      "436.0\n",
      "437.0\n",
      "438.0\n",
      "439.0\n",
      "440.0\n",
      "441.0\n",
      "442.0\n",
      "443.0\n",
      "444.0\n",
      "445.0\n",
      "446.0\n",
      "447.0\n",
      "448.0\n",
      "449.0\n",
      "450.0\n",
      "451.0\n",
      "452.0\n",
      "453.0\n",
      "454.0\n",
      "455.0\n",
      "456.0\n",
      "457.0\n",
      "458.0\n",
      "459.0\n",
      "460.0\n",
      "461.0\n",
      "462.0\n",
      "463.0\n",
      "464.0\n",
      "465.0\n",
      "466.0\n",
      "467.0\n",
      "468.0\n",
      "469.0\n",
      "470.0\n",
      "471.0\n",
      "472.0\n",
      "473.0\n",
      "474.0\n",
      "475.0\n",
      "476.0\n",
      "477.0\n",
      "478.0\n",
      "479.0\n",
      "480.0\n",
      "481.0\n",
      "482.0\n",
      "483.0\n",
      "484.0\n",
      "485.0\n",
      "486.0\n",
      "487.0\n",
      "488.0\n",
      "489.0\n",
      "490.0\n",
      "491.0\n",
      "492.0\n",
      "493.0\n",
      "494.0\n",
      "495.0\n",
      "496.0\n",
      "497.0\n",
      "498.0\n",
      "499.0\n",
      "500.0\n",
      "501.0\n",
      "502.0\n",
      "503.0\n",
      "504.0\n",
      "505.0\n",
      "506.0\n",
      "507.0\n",
      "508.0\n",
      "509.0\n",
      "510.0\n",
      "511.0\n",
      "512.0\n",
      "513.0\n",
      "514.0\n",
      "515.0\n",
      "516.0\n",
      "517.0\n",
      "518.0\n",
      "519.0\n",
      "520.0\n",
      "521.0\n",
      "522.0\n",
      "523.0\n",
      "524.0\n",
      "525.0\n",
      "526.0\n",
      "527.0\n",
      "528.0\n",
      "529.0\n",
      "530.0\n",
      "531.0\n",
      "532.0\n",
      "533.0\n",
      "534.0\n",
      "535.0\n",
      "536.0\n",
      "537.0\n",
      "538.0\n",
      "539.0\n",
      "540.0\n",
      "541.0\n",
      "542.0\n",
      "543.0\n",
      "544.0\n",
      "545.0\n",
      "546.0\n",
      "547.0\n",
      "548.0\n",
      "549.0\n",
      "550.0\n",
      "551.0\n",
      "552.0\n",
      "553.0\n",
      "554.0\n",
      "555.0\n",
      "556.0\n",
      "557.0\n",
      "558.0\n",
      "559.0\n",
      "560.0\n",
      "561.0\n",
      "562.0\n",
      "563.0\n",
      "564.0\n",
      "565.0\n",
      "566.0\n",
      "567.0\n",
      "568.0\n",
      "569.0\n",
      "570.0\n",
      "571.0\n",
      "572.0\n",
      "573.0\n",
      "574.0\n",
      "575.0\n",
      "576.0\n",
      "577.0\n",
      "578.0\n",
      "579.0\n",
      "580.0\n",
      "581.0\n",
      "582.0\n",
      "583.0\n",
      "584.0\n",
      "585.0\n",
      "586.0\n",
      "587.0\n",
      "588.0\n",
      "589.0\n",
      "590.0\n",
      "591.0\n",
      "592.0\n",
      "593.0\n",
      "594.0\n",
      "595.0\n",
      "596.0\n",
      "597.0\n",
      "598.0\n",
      "599.0\n",
      "600.0\n",
      "601.0\n",
      "602.0\n",
      "603.0\n",
      "604.0\n",
      "605.0\n",
      "606.0\n",
      "607.0\n",
      "608.0\n",
      "609.0\n",
      "610.0\n",
      "611.0\n",
      "612.0\n",
      "613.0\n",
      "614.0\n",
      "615.0\n",
      "616.0\n",
      "617.0\n",
      "618.0\n",
      "619.0\n",
      "620.0\n",
      "621.0\n",
      "622.0\n",
      "623.0\n",
      "624.0\n",
      "625.0\n",
      "626.0\n",
      "627.0\n",
      "628.0\n",
      "629.0\n",
      "630.0\n",
      "631.0\n",
      "632.0\n",
      "633.0\n",
      "634.0\n",
      "635.0\n",
      "636.0\n",
      "637.0\n",
      "638.0\n",
      "639.0\n",
      "640.0\n",
      "641.0\n",
      "642.0\n",
      "643.0\n",
      "644.0\n",
      "645.0\n",
      "646.0\n",
      "647.0\n",
      "648.0\n",
      "649.0\n",
      "650.0\n",
      "651.0\n",
      "652.0\n",
      "653.0\n",
      "654.0\n",
      "655.0\n",
      "656.0\n",
      "657.0\n",
      "658.0\n",
      "659.0\n",
      "660.0\n",
      "661.0\n",
      "662.0\n",
      "663.0\n",
      "664.0\n",
      "665.0\n",
      "666.0\n",
      "667.0\n",
      "668.0\n",
      "669.0\n",
      "670.0\n",
      "671.0\n",
      "672.0\n",
      "673.0\n",
      "674.0\n",
      "675.0\n",
      "676.0\n",
      "677.0\n",
      "678.0\n",
      "679.0\n",
      "680.0\n",
      "681.0\n",
      "682.0\n",
      "683.0\n",
      "684.0\n",
      "685.0\n",
      "686.0\n",
      "687.0\n",
      "688.0\n",
      "689.0\n",
      "690.0\n",
      "691.0\n",
      "692.0\n",
      "693.0\n",
      "694.0\n",
      "695.0\n",
      "696.0\n",
      "697.0\n",
      "698.0\n",
      "699.0\n",
      "700.0\n",
      "701.0\n",
      "702.0\n",
      "703.0\n",
      "704.0\n",
      "705.0\n",
      "706.0\n",
      "707.0\n",
      "708.0\n",
      "709.0\n",
      "710.0\n",
      "711.0\n",
      "712.0\n",
      "713.0\n",
      "714.0\n",
      "715.0\n",
      "716.0\n",
      "717.0\n",
      "718.0\n",
      "719.0\n",
      "720.0\n",
      "721.0\n",
      "722.0\n",
      "723.0\n",
      "724.0\n",
      "725.0\n",
      "726.0\n",
      "727.0\n",
      "728.0\n",
      "729.0\n",
      "730.0\n",
      "731.0\n",
      "732.0\n",
      "733.0\n",
      "734.0\n",
      "735.0\n",
      "736.0\n",
      "737.0\n",
      "738.0\n",
      "739.0\n",
      "740.0\n",
      "741.0\n",
      "742.0\n",
      "743.0\n",
      "744.0\n",
      "745.0\n",
      "746.0\n",
      "747.0\n",
      "748.0\n",
      "749.0\n",
      "750.0\n",
      "751.0\n",
      "752.0\n",
      "753.0\n",
      "754.0\n",
      "755.0\n",
      "756.0\n",
      "757.0\n",
      "758.0\n",
      "759.0\n",
      "760.0\n",
      "761.0\n",
      "762.0\n",
      "763.0\n",
      "764.0\n",
      "765.0\n",
      "766.0\n",
      "767.0\n",
      "768.0\n",
      "769.0\n",
      "770.0\n",
      "771.0\n",
      "772.0\n",
      "773.0\n",
      "774.0\n",
      "775.0\n",
      "776.0\n",
      "777.0\n",
      "778.0\n",
      "779.0\n",
      "780.0\n",
      "781.0\n",
      "782.0\n",
      "783.0\n",
      "784.0\n",
      "785.0\n",
      "786.0\n",
      "787.0\n",
      "788.0\n",
      "789.0\n",
      "790.0\n",
      "791.0\n",
      "792.0\n",
      "793.0\n",
      "794.0\n",
      "795.0\n",
      "796.0\n",
      "797.0\n",
      "798.0\n",
      "799.0\n",
      "800.0\n",
      "801.0\n",
      "802.0\n",
      "803.0\n",
      "804.0\n",
      "805.0\n",
      "806.0\n",
      "807.0\n",
      "808.0\n",
      "809.0\n",
      "810.0\n",
      "811.0\n",
      "812.0\n",
      "813.0\n",
      "814.0\n",
      "815.0\n",
      "816.0\n",
      "817.0\n",
      "818.0\n",
      "819.0\n",
      "820.0\n",
      "821.0\n",
      "822.0\n",
      "823.0\n",
      "824.0\n",
      "825.0\n",
      "826.0\n",
      "827.0\n",
      "828.0\n",
      "829.0\n",
      "830.0\n",
      "831.0\n",
      "832.0\n",
      "833.0\n",
      "834.0\n",
      "835.0\n",
      "836.0\n",
      "837.0\n",
      "838.0\n",
      "839.0\n",
      "840.0\n",
      "841.0\n",
      "842.0\n",
      "843.0\n",
      "844.0\n",
      "845.0\n",
      "846.0\n",
      "847.0\n",
      "848.0\n",
      "849.0\n",
      "850.0\n",
      "851.0\n",
      "852.0\n",
      "853.0\n",
      "854.0\n",
      "855.0\n",
      "856.0\n",
      "857.0\n",
      "858.0\n",
      "859.0\n",
      "860.0\n",
      "861.0\n",
      "862.0\n",
      "863.0\n",
      "864.0\n",
      "865.0\n",
      "866.0\n",
      "867.0\n",
      "868.0\n",
      "869.0\n",
      "870.0\n",
      "871.0\n",
      "872.0\n",
      "873.0\n",
      "874.0\n",
      "875.0\n",
      "876.0\n",
      "877.0\n",
      "878.0\n",
      "879.0\n",
      "880.0\n",
      "881.0\n",
      "882.0\n",
      "883.0\n",
      "884.0\n",
      "885.0\n",
      "886.0\n",
      "887.0\n",
      "888.0\n",
      "889.0\n",
      "890.0\n",
      "891.0\n",
      "892.0\n",
      "893.0\n",
      "894.0\n",
      "895.0\n",
      "896.0\n",
      "897.0\n",
      "898.0\n",
      "899.0\n",
      "900.0\n",
      "901.0\n",
      "902.0\n",
      "903.0\n",
      "904.0\n",
      "905.0\n",
      "906.0\n",
      "907.0\n",
      "908.0\n",
      "909.0\n",
      "910.0\n",
      "911.0\n",
      "912.0\n",
      "913.0\n",
      "914.0\n",
      "915.0\n",
      "916.0\n",
      "917.0\n",
      "918.0\n",
      "919.0\n",
      "920.0\n",
      "921.0\n",
      "922.0\n",
      "923.0\n",
      "924.0\n",
      "925.0\n",
      "926.0\n",
      "927.0\n",
      "928.0\n",
      "929.0\n",
      "930.0\n",
      "931.0\n",
      "932.0\n",
      "933.0\n",
      "934.0\n",
      "935.0\n",
      "936.0\n",
      "937.0\n",
      "938.0\n",
      "939.0\n",
      "940.0\n",
      "941.0\n",
      "942.0\n",
      "943.0\n",
      "944.0\n",
      "945.0\n",
      "946.0\n",
      "947.0\n",
      "948.0\n",
      "949.0\n",
      "950.0\n",
      "951.0\n",
      "952.0\n",
      "953.0\n",
      "954.0\n",
      "955.0\n",
      "956.0\n",
      "957.0\n",
      "958.0\n",
      "959.0\n",
      "960.0\n",
      "961.0\n",
      "962.0\n",
      "963.0\n",
      "964.0\n",
      "965.0\n",
      "966.0\n",
      "967.0\n",
      "968.0\n",
      "969.0\n",
      "970.0\n",
      "971.0\n",
      "972.0\n",
      "973.0\n",
      "974.0\n",
      "975.0\n",
      "976.0\n",
      "977.0\n",
      "978.0\n",
      "979.0\n",
      "980.0\n",
      "981.0\n",
      "982.0\n",
      "983.0\n",
      "984.0\n",
      "985.0\n",
      "986.0\n",
      "987.0\n",
      "988.0\n",
      "989.0\n",
      "990.0\n",
      "991.0\n",
      "992.0\n",
      "993.0\n",
      "994.0\n",
      "995.0\n",
      "996.0\n",
      "997.0\n",
      "998.0\n",
      "999.0\n",
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    for t in range(100):\n",
    "        #1턴 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # Q값이 최대가 되는 행동을 선택함\n",
    "        action = np.argmax(Q[current_state])\n",
    "        \n",
    "        # 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #종료\n",
    "        if done:\n",
    "            episode_reward += reward\n",
    "            \n",
    "    # 총 reward\n",
    "    total_reward += episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stochastic 환경."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gym 환경설정\n",
    "# https://gym.openai.com/envs/ 에 들어가면 다양한 환경을 살펴볼 수 있음\n",
    "# Frozen-Lake : https://gym.openai.com/envs/FrozenLake-v0/ \n",
    "register(\n",
    "    id = 'FrozenLake-v2', \n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv', \n",
    "    kwargs={\n",
    "        # 'is_slippery': False 이면 Deteministic 환경.\n",
    "        # 'is_slippery': True 이면 Stochastic 환경.\n",
    "        'map_name' : '4x4', # 4x4 크기의 맵\n",
    "        'is_slippery' : True # 미끄러질 가능성 있음\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v2\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위치:  1\n",
      "보상:  0.0\n",
      "Game Over?:  False\n",
      "정보(확률):  {'prob': 0.3333333333333333}\n",
      "행동:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"위치: \", observation)\n",
    "print(\"보상: \",reward)\n",
    "print(\"Game Over?: \",done)\n",
    "print(\"정보(확률): \", info) # 내가 의도한대로 갈 확률\n",
    "print(\"행동: \", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.zeros((16, 4))\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))    # (16,4) : 4*4 map + 상하좌우 4개\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 100000\n",
    "learning_rate = 0.3\n",
    "discount_rate = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번 진행중..\n",
      "500 번 진행중..\n",
      "1000 번 진행중..\n",
      "1500 번 진행중..\n",
      "2000 번 진행중..\n",
      "2500 번 진행중..\n",
      "3000 번 진행중..\n",
      "3500 번 진행중..\n",
      "4000 번 진행중..\n",
      "4500 번 진행중..\n",
      "5000 번 진행중..\n",
      "5500 번 진행중..\n",
      "6000 번 진행중..\n",
      "6500 번 진행중..\n",
      "7000 번 진행중..\n",
      "7500 번 진행중..\n",
      "8000 번 진행중..\n",
      "8500 번 진행중..\n",
      "9000 번 진행중..\n",
      "9500 번 진행중..\n",
      "10000 번 진행중..\n",
      "10500 번 진행중..\n",
      "11000 번 진행중..\n",
      "11500 번 진행중..\n",
      "12000 번 진행중..\n",
      "12500 번 진행중..\n",
      "13000 번 진행중..\n",
      "13500 번 진행중..\n",
      "14000 번 진행중..\n",
      "14500 번 진행중..\n",
      "15000 번 진행중..\n",
      "15500 번 진행중..\n",
      "16000 번 진행중..\n",
      "16500 번 진행중..\n",
      "17000 번 진행중..\n",
      "17500 번 진행중..\n",
      "18000 번 진행중..\n",
      "18500 번 진행중..\n",
      "19000 번 진행중..\n",
      "19500 번 진행중..\n",
      "20000 번 진행중..\n",
      "20500 번 진행중..\n",
      "21000 번 진행중..\n",
      "21500 번 진행중..\n",
      "22000 번 진행중..\n",
      "22500 번 진행중..\n",
      "23000 번 진행중..\n",
      "23500 번 진행중..\n",
      "24000 번 진행중..\n",
      "24500 번 진행중..\n",
      "25000 번 진행중..\n",
      "25500 번 진행중..\n",
      "26000 번 진행중..\n",
      "26500 번 진행중..\n",
      "27000 번 진행중..\n",
      "27500 번 진행중..\n",
      "28000 번 진행중..\n",
      "28500 번 진행중..\n",
      "29000 번 진행중..\n",
      "29500 번 진행중..\n",
      "30000 번 진행중..\n",
      "30500 번 진행중..\n",
      "31000 번 진행중..\n",
      "31500 번 진행중..\n",
      "32000 번 진행중..\n",
      "32500 번 진행중..\n",
      "33000 번 진행중..\n",
      "33500 번 진행중..\n",
      "34000 번 진행중..\n",
      "34500 번 진행중..\n",
      "35000 번 진행중..\n",
      "35500 번 진행중..\n",
      "36000 번 진행중..\n",
      "36500 번 진행중..\n",
      "37000 번 진행중..\n",
      "37500 번 진행중..\n",
      "38000 번 진행중..\n",
      "38500 번 진행중..\n",
      "39000 번 진행중..\n",
      "39500 번 진행중..\n",
      "40000 번 진행중..\n",
      "40500 번 진행중..\n",
      "41000 번 진행중..\n",
      "41500 번 진행중..\n",
      "42000 번 진행중..\n",
      "42500 번 진행중..\n",
      "43000 번 진행중..\n",
      "43500 번 진행중..\n",
      "44000 번 진행중..\n",
      "44500 번 진행중..\n",
      "45000 번 진행중..\n",
      "45500 번 진행중..\n",
      "46000 번 진행중..\n",
      "46500 번 진행중..\n",
      "47000 번 진행중..\n",
      "47500 번 진행중..\n",
      "48000 번 진행중..\n",
      "48500 번 진행중..\n",
      "49000 번 진행중..\n",
      "49500 번 진행중..\n",
      "50000 번 진행중..\n",
      "50500 번 진행중..\n",
      "51000 번 진행중..\n",
      "51500 번 진행중..\n",
      "52000 번 진행중..\n",
      "52500 번 진행중..\n",
      "53000 번 진행중..\n",
      "53500 번 진행중..\n",
      "54000 번 진행중..\n",
      "54500 번 진행중..\n",
      "55000 번 진행중..\n",
      "55500 번 진행중..\n",
      "56000 번 진행중..\n",
      "56500 번 진행중..\n",
      "57000 번 진행중..\n",
      "57500 번 진행중..\n",
      "58000 번 진행중..\n",
      "58500 번 진행중..\n",
      "59000 번 진행중..\n",
      "59500 번 진행중..\n",
      "60000 번 진행중..\n",
      "60500 번 진행중..\n",
      "61000 번 진행중..\n",
      "61500 번 진행중..\n",
      "62000 번 진행중..\n",
      "62500 번 진행중..\n",
      "63000 번 진행중..\n",
      "63500 번 진행중..\n",
      "64000 번 진행중..\n",
      "64500 번 진행중..\n",
      "65000 번 진행중..\n",
      "65500 번 진행중..\n",
      "66000 번 진행중..\n",
      "66500 번 진행중..\n",
      "67000 번 진행중..\n",
      "67500 번 진행중..\n",
      "68000 번 진행중..\n",
      "68500 번 진행중..\n",
      "69000 번 진행중..\n",
      "69500 번 진행중..\n",
      "70000 번 진행중..\n",
      "70500 번 진행중..\n",
      "71000 번 진행중..\n",
      "71500 번 진행중..\n",
      "72000 번 진행중..\n",
      "72500 번 진행중..\n",
      "73000 번 진행중..\n",
      "73500 번 진행중..\n",
      "74000 번 진행중..\n",
      "74500 번 진행중..\n",
      "75000 번 진행중..\n",
      "75500 번 진행중..\n",
      "76000 번 진행중..\n",
      "76500 번 진행중..\n",
      "77000 번 진행중..\n",
      "77500 번 진행중..\n",
      "78000 번 진행중..\n",
      "78500 번 진행중..\n",
      "79000 번 진행중..\n",
      "79500 번 진행중..\n",
      "80000 번 진행중..\n",
      "80500 번 진행중..\n",
      "81000 번 진행중..\n",
      "81500 번 진행중..\n",
      "82000 번 진행중..\n",
      "82500 번 진행중..\n",
      "83000 번 진행중..\n",
      "83500 번 진행중..\n",
      "84000 번 진행중..\n",
      "84500 번 진행중..\n",
      "85000 번 진행중..\n",
      "85500 번 진행중..\n",
      "86000 번 진행중..\n",
      "86500 번 진행중..\n",
      "87000 번 진행중..\n",
      "87500 번 진행중..\n",
      "88000 번 진행중..\n",
      "88500 번 진행중..\n",
      "89000 번 진행중..\n",
      "89500 번 진행중..\n",
      "90000 번 진행중..\n",
      "90500 번 진행중..\n",
      "91000 번 진행중..\n",
      "91500 번 진행중..\n",
      "92000 번 진행중..\n",
      "92500 번 진행중..\n",
      "93000 번 진행중..\n",
      "93500 번 진행중..\n",
      "94000 번 진행중..\n",
      "94500 번 진행중..\n",
      "95000 번 진행중..\n",
      "95500 번 진행중..\n",
      "96000 번 진행중..\n",
      "96500 번 진행중..\n",
      "97000 번 진행중..\n",
      "97500 번 진행중..\n",
      "98000 번 진행중..\n",
      "98500 번 진행중..\n",
      "99000 번 진행중..\n",
      "99500 번 진행중..\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "recent_reward = []\n",
    "recent_size = 100\n",
    "\n",
    "for epi in range(num_episodes):\n",
    "    observation = env.reset() # 초기화\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    for t in range(100):\n",
    "        # 1턴 실행 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # 학습을 빨리 하기 위해서 넣는 코드\n",
    "        # 없으면 계속 LEFT로 가기 때문\n",
    "        # 전체가 0이면 처리\n",
    "        \n",
    "        # if not Q[current_state, :].any():\n",
    "        # action = env.action_space.sample()\n",
    "        if np.random.rand() < 0.1: # 변경 된걸 확인하세요.\n",
    "            # 랜덤 선택\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # 최대값\n",
    "            action = np.argmax(Q[current_state])\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 성공 사례를 만들어야 측정이 가능함.\n",
    "        # 현 위치의 최대값 = 성공 가능성\n",
    "        # 이전 위치의 현 방향 선택 = 기존의 성공 가능성\n",
    "        Q[current_state, action] += learning_rate * (reward + discount_rate * np.max(Q[observation, :]) - Q[current_state, action]) \n",
    "            #                        0.3 - 내결정 * 0.3\n",
    "           #내 결정 = 내 결정 + 0.3 -0.3(내결정)\n",
    "        if done:\n",
    "            episode_reward +=reward\n",
    "            break\n",
    "    if epi % 500 == 0:\n",
    "        print(epi, \"번 진행중..\")\n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    recent_reward.append(episode_reward)\n",
    "    if len(recent_reward) >= recent_size:\n",
    "        # 최근 100개에 대해서 검사\n",
    "        recent_reward.pop(0)\n",
    "        \n",
    "        if sum(recent_reward) / recent_size > 0.7:\n",
    "            print(\"0.7이 넘었습니다. 멈추겠습니다.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위치:  12\n",
      "보상:  0.0\n",
      "Game Over?:  True\n",
      "정보(확률):  {'prob': 0.3333333333333333}\n",
      "행동:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"위치: \", observation)\n",
    "print(\"보상: \", reward)\n",
    "print(\"Game Over?: \",done)\n",
    "print(\"정보(확률): \", info) # 내가 의도한대로 갈 확률\n",
    "print(\"행동: \", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33052"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward/ num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recent_reward)/recent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54563052, 0.53520556, 0.53789376, 0.53006509],\n",
       "       [0.48143402, 0.23540841, 0.41164338, 0.51543244],\n",
       "       [0.44502576, 0.34403206, 0.42390005, 0.47301338],\n",
       "       [0.28290285, 0.15411191, 0.26094122, 0.46396364],\n",
       "       [0.567145  , 0.50290285, 0.14548786, 0.47285774],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.46247832, 0.1603602 , 0.14698439, 0.08120084],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.31468133, 0.42458191, 0.42681909, 0.59623797],\n",
       "       [0.24919672, 0.64846043, 0.39465703, 0.52345708],\n",
       "       [0.33927144, 0.62277014, 0.2613628 , 0.19848275],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.45746611, 0.45017864, 0.71341276, 0.66585751],\n",
       "       [0.77280702, 0.77579568, 0.76395012, 0.74593983],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q테이블로 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000\n",
    "\n",
    "total_reward = 0.0\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    for t in range(100):\n",
    "        #1턴 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # Q값이 최대가 되는 행동을 선택함\n",
    "        action = np.argmax(Q[current_state])\n",
    "        \n",
    "        # 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #종료\n",
    "        if done:\n",
    "            episode_reward += reward\n",
    "            \n",
    "    # 총 reward\n",
    "    total_reward += episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618.0\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Stochastic 환경.\n",
    "env = gym.make('FrozenLake-v2')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 96)\n",
    "        self.fc4 = nn.Linear(96, 96)\n",
    "        self.fc5 = nn.Linear(96, 64)\n",
    "        self.fc6 = nn.Linear(64, 64)\n",
    "        self.fc7 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def onehot2tensor(state):\n",
    "    tmp = np.zeros(16)\n",
    "    tmp[state] = 1\n",
    "    vector = np.array(tmp, dtype='float32')\n",
    "    tensor = torch.from_numpy(vector).float()\n",
    "    return tensor\n",
    "\n",
    "def applymodel(tensor):\n",
    "    output_tensor = model(tensor)\n",
    "    output_array = output_tensor.data.numpy()\n",
    "    return output_tensor, output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([45.0314, 47.9362, 45.1100, 43.5702], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([46.4968, 49.5300, 46.5825, 45.0658], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([48.7093, 51.9264, 48.8049, 47.2945], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([49.8167, 53.1310, 49.9177, 48.4261], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([50.6564, 54.0452, 50.7616, 49.2872], grad_fn=<AddBackward0>)\n",
      "500번: total_loss: 0.4183310456573963, total_reward: 18.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6698, 0.5731, 0.6698, 0.6697], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6697, 0.5731, 0.6697, 0.6697], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6697, 0.5733, 0.6697, 0.6697], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6697, 0.5734, 0.6696, 0.6696], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6696, 0.5735, 0.6696, 0.6696], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6696, 0.5736, 0.6696, 0.6696], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6695, 0.5737, 0.6695, 0.6695], grad_fn=<AddBackward0>)\n",
      "1000번: total_loss: 0.002088979661493795, total_reward: 42.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6130, 0.6134, 0.6483, 0.6132], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6129, 0.6133, 0.6547, 0.6131], grad_fn=<AddBackward0>)\n",
      "1500번: total_loss: 2.122232490364695e-05, total_reward: 58.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5946, 0.6258, 0.5922, 0.6259], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5959, 0.6250, 0.5923, 0.6254], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5970, 0.6243, 0.5923, 0.6245], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5991, 0.6237, 0.5924, 0.6238], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6010, 0.6231, 0.5924, 0.6227], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6028, 0.6225, 0.5924, 0.6218], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6043, 0.6218, 0.5925, 0.6209], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6057, 0.6211, 0.5925, 0.6201], grad_fn=<AddBackward0>)\n",
      "2000번: total_loss: 0.0003274979744674056, total_reward: 73.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2723, 0.2735, 0.2734, 0.2722], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2721, 0.2729, 0.2732, 0.2719], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2718, 0.2724, 0.2728, 0.2717], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2716, 0.2720, 0.2724, 0.2715], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2714, 0.2716, 0.2720, 0.2713], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2713, 0.2712, 0.2716, 0.2712], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2711, 0.2709, 0.2711, 0.2710], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2710, 0.2706, 0.2706, 0.2706], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2705, 0.2703, 0.2702, 0.2702], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2697, 0.2701, 0.2699, 0.2699], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2690, 0.2697, 0.2696, 0.2696], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2684, 0.2693, 0.2693, 0.2693], grad_fn=<AddBackward0>)\n",
      "2500번: total_loss: 2.1993712152834632e-05, total_reward: 79.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5378, 0.5380, 0.4542, 0.5375], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5375, 0.5372, 0.4542, 0.5369], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5367, 0.5365, 0.4542, 0.5364], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5357, 0.5358, 0.4542, 0.5359], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5347, 0.5352, 0.4542, 0.5353], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5334, 0.5347, 0.4542, 0.5348], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5323, 0.5342, 0.4542, 0.5341], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5313, 0.5336, 0.4542, 0.5336], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5304, 0.5329, 0.4542, 0.5330], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5296, 0.5323, 0.4542, 0.5324], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5289, 0.5318, 0.4542, 0.5317], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5282, 0.5311, 0.4542, 0.5311], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5276, 0.5304, 0.4542, 0.5305], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5271, 0.5297, 0.4542, 0.5298], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5266, 0.5291, 0.4542, 0.5290], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5262, 0.5284, 0.4542, 0.5284], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5258, 0.5276, 0.4542, 0.5277], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5254, 0.5269, 0.4542, 0.5270], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5251, 0.5263, 0.4542, 0.5262], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5248, 0.5256, 0.4542, 0.5255], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5242, 0.5249, 0.4542, 0.5249], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5237, 0.5242, 0.4542, 0.5243], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5232, 0.5235, 0.4565, 0.5238], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5228, 0.5229, 0.4585, 0.5231], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5224, 0.5224, 0.4604, 0.5224], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5220, 0.5219, 0.4621, 0.5216], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5213, 0.5215, 0.4636, 0.5209], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5206, 0.5209, 0.4650, 0.5203], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5200, 0.5203, 0.4662, 0.5197], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5195, 0.5196, 0.4673, 0.5191], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5190, 0.5188, 0.4683, 0.5187], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5182, 0.5180, 0.4692, 0.5182], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5175, 0.5174, 0.4700, 0.5177], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5168, 0.5168, 0.4707, 0.5171], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5162, 0.5163, 0.4714, 0.5163], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5156, 0.5158, 0.4720, 0.5155], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5151, 0.5152, 0.4725, 0.5148], grad_fn=<AddBackward0>)\n",
      "3000번: total_loss: 0.0012987912109565514, total_reward: 90.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5050, 0.5062, 0.5060, 0.5078], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5041, 0.5057, 0.5054, 0.5074], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5032, 0.5053, 0.5048, 0.5069], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5024, 0.5049, 0.5043, 0.5063], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5018, 0.5045, 0.5038, 0.5055], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5011, 0.5042, 0.5034, 0.5046], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5006, 0.5039, 0.5031, 0.5035], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5001, 0.5035, 0.5027, 0.5026], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4996, 0.5031, 0.5024, 0.5018], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4992, 0.5026, 0.5021, 0.5010], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4989, 0.5020, 0.5019, 0.5004], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4985, 0.5014, 0.5017, 0.4997], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4982, 0.5008, 0.5011, 0.4992], grad_fn=<AddBackward0>)\n",
      "3500번: total_loss: 8.199844069167739e-05, total_reward: 97.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0027, 0.0027, 0.0027, 0.0027], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0027, 0.0027, 0.0027, 0.0027], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0027, 0.0027, 0.0027, 0.0027], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0027, 0.0027, 0.0027, 0.0027], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0027, 0.0027, 0.0027, 0.0027], grad_fn=<AddBackward0>)\n",
      "4000번: total_loss: 8.698449877053704e-10, total_reward: 100.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6189, 0.6219, 0.6221, 0.5743], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6176, 0.6212, 0.6211, 0.5744], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6164, 0.6200, 0.6203, 0.5745], grad_fn=<AddBackward0>)\n",
      "4500번: total_loss: 2.8940972697455436e-05, total_reward: 106.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.3105, 1.3719, 1.1197, 1.2698], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.3153, 1.3722, 1.1197, 1.2698], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.3196, 1.3722, 1.1198, 1.2698], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.3234, 1.3722, 1.1231, 1.2698], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.3269, 1.3720, 1.1260, 1.2698], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.3300, 1.3716, 1.1286, 1.2699], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.3329, 1.3711, 1.1310, 1.2699], grad_fn=<AddBackward0>)\n",
      "5000번: total_loss: 0.014526690949423937, total_reward: 131.0 \n"
     ]
    }
   ],
   "source": [
    "# cost function을 사용했으나 어느 지점부터 변화가 없음\n",
    "\n",
    "total_reward = 0.0\n",
    "num_episodes = 5000\n",
    "discount_rate = 0.99\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # 현재 게임 reward\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    # 누적 오차\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for t in range(100):\n",
    "        current_state = observation\n",
    "        \n",
    "        # 경사 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 현위치 onehot encoding \n",
    "        current_tensor = onehot2tensor(current_state)\n",
    "\n",
    "        if (i_episode+1) % 500 == 0:\n",
    "            print(current_tensor)\n",
    "            current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "            print(current_output_tensor)\n",
    "            # print(\"{}번: total_loss: {}, total_reward: {} \".format(i_episode+1, total_loss, total_reward))\n",
    "            \n",
    "            \n",
    "        # 모형에 입력\n",
    "        current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "        # 행동 선택\n",
    "        if np.random.rand() < 0.1:\n",
    "            #무작위 선택\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Q값이 최대가 되도록\n",
    "            action = np.argmax(current_output_array)\n",
    "\n",
    "        ## 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # onehot 벡터 텐서로 변환\n",
    "        observation_tensor = onehot2tensor(observation)\n",
    "        \n",
    "        # 모형에 입력\n",
    "        observation_output_tensor, observation_output_array = applymodel(observation_tensor)\n",
    "        \n",
    "        # Q값 업데이트\n",
    "        # y값\n",
    "        q = reward + discount_rate * np.max(observation_output_array) # reward는 마지막 value를 위해 더함\n",
    "        \n",
    "        # 기존 보드의 q값 복사\n",
    "        q_array = np.copy(current_output_array) # 이전 값\n",
    "        q_array[action] = q\n",
    "        q_variable = torch.from_numpy(q_array)\n",
    "        \n",
    "        # 오차 계산 - q_array와 q\n",
    "        # 현재 아웃풋 값과, 기존 q어레이\n",
    "        # y값은 올바른 선택일 때의 최대값,\n",
    "        # q^은 현재 선택의 결과\n",
    "        \n",
    "        # current_output_tensor: 현재 측정값\n",
    "        # q_variable: 최고의 선택을 했을 때 y값\n",
    "        loss = criterion(current_output_tensor, q_variable)\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        \n",
    "        # 웨이트 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 오차 누적 계산\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if done:\n",
    "            # 종료\n",
    "            episode_reward += reward\n",
    "            \n",
    "            break\n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    # 누적오차 및 보상률\n",
    "    if (i_episode+1) % 500 == 0:\n",
    "        print(\"{}번: total_loss: {}, total_reward: {} \".format(i_episode+1, total_loss, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500번:\n",
      "38.0\n",
      "------------------------------\n",
      "1000번:\n",
      "63.0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "num_episodes = 1000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # 현재 게임 reward\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    \n",
    "    for t in range(100):\n",
    "        current_state = observation\n",
    "        \n",
    "        \n",
    "        # 현위치 onehot encoding \n",
    "        current_tensor = onehot2tensor(current_state)\n",
    "        # 모형에 입력\n",
    "        current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "        \n",
    "        # Q테이블에서 제일 큰거 선택\n",
    "        action = np.argmax(current_output_array)\n",
    "\n",
    "        ## 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # 종료\n",
    "            episode_reward += reward\n",
    "            break\n",
    "            \n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    # 누적오차 및 보상률\n",
    "    if (i_episode+1) % 500 == 0:\n",
    "        print(\"{}번:\".format(i_episode+1))\n",
    "        print(total_reward)\n",
    "        print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0\n",
      "0.063\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)  \n",
    "print(total_reward/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
