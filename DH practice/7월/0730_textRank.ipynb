{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank \n",
    "어느 하이퍼링크가 다양한 곳에서 참조가 되었는가? 많이 참조 될 수록 상위 rank. \n",
    "# ***언어에 구애받지 않는다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph = directed / undirected graphs\n",
    "방향이 없는 그래프를 썼을 때 수렴하는 속도가 빨라서 사용.\n",
    "## weighted graphs\n",
    "각 노드별로 연결된 엣지에 가중치를 포함시킨다.\n",
    "## 반복하는 이유? \n",
    "반복할 수록 거리가 점점 멀어져서 중요도가 높은 것만 남는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"The FAANG stocks won’t see much more growth in the near future, according to Bill Studebaker, founder and Chief Investment Officer of Robo Global. \\\n",
    "Studebaker argues we are seeing a 'reallocation' that will continue from large-cap tech stocks into market-weight stocks. \\\n",
    "The FAANG stocks have had a rough few weeks, and have been hit hard since March 12. \\\n",
    "One FAANG to look out for, in the midst of all this, is Amazon, according to Studebaker. \\\n",
    "The stock market is seeing a 'reallocation' out of FAANG stocks, which are not where the smart money is, founder and Chief Investment Officer of Robo Global Bill Studebaker told Business Insider. \\\n",
    "The FAANG stocks (Facebook, Apple, Amazon, Netflix, Google) are all down considerably since March 12, a trend that accelerated when news of a massive Facebook data scandal broke, sending the tech-heavy Nasdaq into a downward frenzy. \\\n",
    "Investors are wondering what’s next. \\\n",
    "And what’s next isn’t good news for FAANG stock optimists, Studebaker thinks. 'This is a dead trade' for the next several months, he said. 'I wouldn’t expect there to be a lot of performance attribution coming from the FAANG stocks,' he added. That is, if the stock market is to see gains in the next several months, they will largely not come from the big tech companies. \\\n",
    "The market is seeing a 'reallocation out of large-cap technology, into other parts of the market,' he said. And this trend could continue for the foreseeable future. 'When you get these reallocation trades, a de-risking, this can go on for months and months.' The FAANG’s are pricey stocks, he said, pointing out that investors will 'factor in the law of big numbers,' he said. 'Just because they’re big cap doesn’t mean they’re safe,' he added. \\\n",
    "Still, he doesn’t necessarily think that investors are going to shift drastically into value stocks. 'With an increasingly favorable macro backdrop, you have strong growth demand.' \\\n",
    "Studebaker, who runs an artificial intelligence and robotics exchange-traded fund with $4 billion in assets under management, thinks that AI and robotics are better areas of growth. His ETF is up 27% in the past year, while the FAANG stocks are also largely up over that same span, even if they are down since March 12. \\\n",
    "While many point to artificial intelligence as an area that will be a boost to Google and Amazon, Studebaker doesn’t see that as a sign of significant growth for the FAANGs. He pointed out that 'eighty to ninety percent of their businesses are still search,' and that 'AI doesn’t really move the needle on the business.' He also said 'the revenue mix [attributable to AI] in those businesses are insignificant.' \\\n",
    "And while he’s not bullish on FAANG’s, he does say that the one FAANG to still watch out for is Amazon, simply because ecommerce still represents a small portion of the global retail market, giving the company room to grow.\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['The', 'FAANG', 'stocks', 'won', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'according', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global.', 'Studebaker', 'argues', 'we', 'are', 'seeing', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stocks', 'into', 'market-weight', 'stocks.', 'The', 'FAANG', 'stocks', 'have', 'had', 'a', 'rough', 'few', 'weeks', ',', 'and', 'have', 'been', 'hit', 'hard', 'since', 'March', '12.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'is', 'Amazon', ',', 'according', 'to', 'Studebaker.', 'The', 'stock', 'market', 'is', 'seeing', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stocks', ',', 'which', 'are', 'not', 'where', 'the', 'smart', 'money', 'is', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'told', 'Business', 'Insider.', 'The', 'FAANG', 'stocks', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'are', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerated', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'broke', ',', 'sending', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy.', 'Investors', 'are', 'wondering', 'what', '’', 's', 'next.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimists', ',', 'Studebaker', 'thinks.', \"'This\", 'is', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'months', ',', 'he', 'said.', \"'\", 'I', 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'coming', 'from', 'the', 'FAANG', 'stocks', ',', \"'\", 'he', 'added.', 'That', 'is', ',', 'if', 'the', 'stock', 'market', 'is', 'to', 'see', 'gains', 'in', 'the', 'next', 'several', 'months', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'companies.', 'The', 'market', 'is', 'seeing', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'parts', 'of', 'the', 'market', ',', \"'\", 'he', 'said.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trades', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'months', 'and', 'months.', \"'\", 'The', 'FAANG', '’', 's', 'are', 'pricey', 'stocks', ',', 'he', 'said', ',', 'pointing', 'out', 'that', 'investors', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'numbers', ',', \"'\", 'he', 'said.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investors', 'are', 'going', 'to', 'shift', 'drastically', 'into', 'value', 'stocks.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand.', \"'\", 'Studebaker', ',', 'who', 'runs', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'assets', 'under', 'management', ',', 'thinks', 'that', 'AI', 'and', 'robotics', 'are', 'better', 'areas', 'of', 'growth.', 'His', 'ETF', 'is', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stocks', 'are', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'are', 'down', 'since', 'March', '12.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'as', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'as', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs.', 'He', 'pointed', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'businesses', 'are', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business.', \"'\", 'He', 'also', 'said', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'businesses', 'are', 'insignificant.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'does', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'is', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represents', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'giving', 'the', 'company', 'room', 'to', 'grow', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = TreebankWordTokenizer().tokenize(Text)\n",
    "\n",
    "print('Tokenized Text: \\n')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 품사부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('won', 'VBD'), ('’', 'JJ'), ('t', 'NN'), ('see', 'VBP'), ('much', 'RB'), ('more', 'JJR'), ('growth', 'NN'), ('in', 'IN'), ('the', 'DT'), ('near', 'JJ'), ('future', 'NN'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Bill', 'NNP'), ('Studebaker', 'NNP'), (',', ','), ('founder', 'NN'), ('and', 'CC'), ('Chief', 'NNP'), ('Investment', 'NNP'), ('Officer', 'NNP'), ('of', 'IN'), ('Robo', 'NNP'), ('Global.', 'NNP'), ('Studebaker', 'NNP'), ('argues', 'VBZ'), ('we', 'PRP'), ('are', 'VBP'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), (\"'\", 'POS'), ('that', 'WDT'), ('will', 'MD'), ('continue', 'VB'), ('from', 'IN'), ('large-cap', 'JJ'), ('tech', 'NN'), ('stocks', 'NNS'), ('into', 'IN'), ('market-weight', 'JJ'), ('stocks.', 'NN'), ('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('have', 'VBP'), ('had', 'VBD'), ('a', 'DT'), ('rough', 'JJ'), ('few', 'JJ'), ('weeks', 'NNS'), (',', ','), ('and', 'CC'), ('have', 'VBP'), ('been', 'VBN'), ('hit', 'VBN'), ('hard', 'JJ'), ('since', 'IN'), ('March', 'NNP'), ('12.', 'CD'), ('One', 'NNP'), ('FAANG', 'NNP'), ('to', 'TO'), ('look', 'VB'), ('out', 'RP'), ('for', 'IN'), (',', ','), ('in', 'IN'), ('the', 'DT'), ('midst', 'NN'), ('of', 'IN'), ('all', 'PDT'), ('this', 'DT'), (',', ','), ('is', 'VBZ'), ('Amazon', 'NNP'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Studebaker.', 'NNP'), ('The', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), (\"'\", 'POS'), ('out', 'IN'), ('of', 'IN'), ('FAANG', 'NNP'), ('stocks', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('not', 'RB'), ('where', 'WRB'), ('the', 'DT'), ('smart', 'JJ'), ('money', 'NN'), ('is', 'VBZ'), (',', ','), ('founder', 'NN'), ('and', 'CC'), ('Chief', 'NNP'), ('Investment', 'NNP'), ('Officer', 'NNP'), ('of', 'IN'), ('Robo', 'NNP'), ('Global', 'NNP'), ('Bill', 'NNP'), ('Studebaker', 'NNP'), ('told', 'VBD'), ('Business', 'NNP'), ('Insider.', 'NNP'), ('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('(', '('), ('Facebook', 'NNP'), (',', ','), ('Apple', 'NNP'), (',', ','), ('Amazon', 'NNP'), (',', ','), ('Netflix', 'NNP'), (',', ','), ('Google', 'NNP'), (')', ')'), ('are', 'VBP'), ('all', 'DT'), ('down', 'RB'), ('considerably', 'RB'), ('since', 'IN'), ('March', 'NNP'), ('12', 'CD'), (',', ','), ('a', 'DT'), ('trend', 'NN'), ('that', 'WDT'), ('accelerated', 'VBD'), ('when', 'WRB'), ('news', 'NN'), ('of', 'IN'), ('a', 'DT'), ('massive', 'JJ'), ('Facebook', 'NNP'), ('data', 'NN'), ('scandal', 'NN'), ('broke', 'VBD'), (',', ','), ('sending', 'VBG'), ('the', 'DT'), ('tech-heavy', 'JJ'), ('Nasdaq', 'NNP'), ('into', 'IN'), ('a', 'DT'), ('downward', 'JJ'), ('frenzy.', 'NN'), ('Investors', 'NNS'), ('are', 'VBP'), ('wondering', 'VBG'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('next.', 'DT'), ('And', 'CC'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('next', 'JJ'), ('isn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('good', 'JJ'), ('news', 'NN'), ('for', 'IN'), ('FAANG', 'NNP'), ('stock', 'NN'), ('optimists', 'NNS'), (',', ','), ('Studebaker', 'NNP'), ('thinks.', 'NN'), (\"'This\", 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('dead', 'JJ'), ('trade', 'NN'), (\"'\", \"''\"), ('for', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('he', 'PRP'), ('said.', 'VBD'), (\"'\", \"''\"), ('I', 'PRP'), ('wouldn', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('expect', 'VBP'), ('there', 'EX'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('performance', 'NN'), ('attribution', 'NN'), ('coming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('added.', 'VBZ'), ('That', 'DT'), ('is', 'VBZ'), (',', ','), ('if', 'IN'), ('the', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('see', 'VB'), ('gains', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('they', 'PRP'), ('will', 'MD'), ('largely', 'RB'), ('not', 'RB'), ('come', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('big', 'JJ'), ('tech', 'NN'), ('companies.', 'VBZ'), ('The', 'DT'), ('market', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), ('out', 'IN'), ('of', 'IN'), ('large-cap', 'JJ'), ('technology', 'NN'), (',', ','), ('into', 'IN'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('market', 'NN'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('said.', 'VBZ'), ('And', 'CC'), ('this', 'DT'), ('trend', 'NN'), ('could', 'MD'), ('continue', 'VB'), ('for', 'IN'), ('the', 'DT'), ('foreseeable', 'JJ'), ('future.', 'NN'), (\"'When\", 'POS'), ('you', 'PRP'), ('get', 'VBP'), ('these', 'DT'), ('reallocation', 'NN'), ('trades', 'NNS'), (',', ','), ('a', 'DT'), ('de-risking', 'NN'), (',', ','), ('this', 'DT'), ('can', 'MD'), ('go', 'VB'), ('on', 'IN'), ('for', 'IN'), ('months', 'NNS'), ('and', 'CC'), ('months.', 'NN'), (\"'\", \"''\"), ('The', 'DT'), ('FAANG', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('are', 'VBP'), ('pricey', 'JJ'), ('stocks', 'NNS'), (',', ','), ('he', 'PRP'), ('said', 'VBD'), (',', ','), ('pointing', 'VBG'), ('out', 'RP'), ('that', 'IN'), ('investors', 'NNS'), ('will', 'MD'), (\"'factor\", 'VB'), ('in', 'IN'), ('the', 'DT'), ('law', 'NN'), ('of', 'IN'), ('big', 'JJ'), ('numbers', 'NNS'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('said.', 'VBZ'), (\"'Just\", 'CC'), ('because', 'IN'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('big', 'JJ'), ('cap', 'NN'), ('doesn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('mean', 'NN'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('safe', 'JJ'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('added.', 'VBZ'), ('Still', 'RB'), (',', ','), ('he', 'PRP'), ('doesn', 'VBZ'), ('’', 'JJ'), ('t', 'NNS'), ('necessarily', 'RB'), ('think', 'VBP'), ('that', 'IN'), ('investors', 'NNS'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('shift', 'VB'), ('drastically', 'RB'), ('into', 'IN'), ('value', 'NN'), ('stocks.', 'NN'), (\"'With\", 'CD'), ('an', 'DT'), ('increasingly', 'RB'), ('favorable', 'JJ'), ('macro', 'NN'), ('backdrop', 'NN'), (',', ','), ('you', 'PRP'), ('have', 'VBP'), ('strong', 'JJ'), ('growth', 'NN'), ('demand.', 'NN'), (\"'\", \"''\"), ('Studebaker', 'NNP'), (',', ','), ('who', 'WP'), ('runs', 'VBZ'), ('an', 'DT'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('robotics', 'NNS'), ('exchange-traded', 'JJ'), ('fund', 'NN'), ('with', 'IN'), ('$', '$'), ('4', 'CD'), ('billion', 'CD'), ('in', 'IN'), ('assets', 'NNS'), ('under', 'IN'), ('management', 'NN'), (',', ','), ('thinks', 'VBZ'), ('that', 'IN'), ('AI', 'NNP'), ('and', 'CC'), ('robotics', 'NNS'), ('are', 'VBP'), ('better', 'JJR'), ('areas', 'NNS'), ('of', 'IN'), ('growth.', 'NN'), ('His', 'PRP$'), ('ETF', 'NN'), ('is', 'VBZ'), ('up', 'RP'), ('27', 'CD'), ('%', 'NN'), ('in', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('year', 'NN'), (',', ','), ('while', 'IN'), ('the', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('are', 'VBP'), ('also', 'RB'), ('largely', 'RB'), ('up', 'IN'), ('over', 'IN'), ('that', 'DT'), ('same', 'JJ'), ('span', 'NN'), (',', ','), ('even', 'RB'), ('if', 'IN'), ('they', 'PRP'), ('are', 'VBP'), ('down', 'JJ'), ('since', 'IN'), ('March', 'NNP'), ('12.', 'CD'), ('While', 'IN'), ('many', 'JJ'), ('point', 'NN'), ('to', 'TO'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('as', 'IN'), ('an', 'DT'), ('area', 'NN'), ('that', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('a', 'DT'), ('boost', 'NN'), ('to', 'TO'), ('Google', 'NNP'), ('and', 'CC'), ('Amazon', 'NNP'), (',', ','), ('Studebaker', 'NNP'), ('doesn', 'VBZ'), ('’', 'JJ'), ('t', 'NNS'), ('see', 'VBP'), ('that', 'IN'), ('as', 'IN'), ('a', 'DT'), ('sign', 'NN'), ('of', 'IN'), ('significant', 'JJ'), ('growth', 'NN'), ('for', 'IN'), ('the', 'DT'), ('FAANGs.', 'NNP'), ('He', 'PRP'), ('pointed', 'VBD'), ('out', 'RP'), ('that', 'IN'), (\"'eighty\", 'VBZ'), ('to', 'TO'), ('ninety', 'VB'), ('percent', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('businesses', 'NNS'), ('are', 'VBP'), ('still', 'RB'), ('search', 'RB'), (',', ','), (\"'\", \"''\"), ('and', 'CC'), ('that', 'IN'), (\"'AI\", 'NNP'), ('doesn', 'VBD'), ('’', 'NNP'), ('t', 'NN'), ('really', 'RB'), ('move', 'VB'), ('the', 'DT'), ('needle', 'NN'), ('on', 'IN'), ('the', 'DT'), ('business.', 'NN'), (\"'\", 'POS'), ('He', 'PRP'), ('also', 'RB'), ('said', 'VBD'), (\"'the\", 'JJ'), ('revenue', 'NN'), ('mix', 'NN'), ('[', 'NNP'), ('attributable', 'NN'), ('to', 'TO'), ('AI', 'NNP'), (']', 'NNP'), ('in', 'IN'), ('those', 'DT'), ('businesses', 'NNS'), ('are', 'VBP'), ('insignificant.', 'JJ'), (\"'\", 'POS'), ('And', 'CC'), ('while', 'IN'), ('he', 'PRP'), ('’', 'VBZ'), ('s', 'PRP'), ('not', 'RB'), ('bullish', 'VB'), ('on', 'IN'), ('FAANG', 'NNP'), ('’', 'NNP'), ('s', 'NN'), (',', ','), ('he', 'PRP'), ('does', 'VBZ'), ('say', 'VB'), ('that', 'IN'), ('the', 'DT'), ('one', 'CD'), ('FAANG', 'NNP'), ('to', 'TO'), ('still', 'RB'), ('watch', 'VB'), ('out', 'RP'), ('for', 'IN'), ('is', 'VBZ'), ('Amazon', 'NNP'), (',', ','), ('simply', 'RB'), ('because', 'IN'), ('ecommerce', 'NN'), ('still', 'RB'), ('represents', 'VBZ'), ('a', 'DT'), ('small', 'JJ'), ('portion', 'NN'), ('of', 'IN'), ('the', 'DT'), ('global', 'JJ'), ('retail', 'JJ'), ('market', 'NN'), (',', ','), ('giving', 'VBG'), ('the', 'DT'), ('company', 'NN'), ('room', 'NN'), ('to', 'TO'), ('grow', 'VB'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "print(POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 표제어추출 (lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'FAANG', 'stock', 'win', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'accord', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global.', 'Studebaker', 'argue', 'we', 'be', 'see', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stock', 'into', 'market-weight', 'stocks.', 'The', 'FAANG', 'stock', 'have', 'have', 'a', 'rough', 'few', 'week', ',', 'and', 'have', 'be', 'hit', 'hard', 'since', 'March', '12.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'be', 'Amazon', ',', 'accord', 'to', 'Studebaker.', 'The', 'stock', 'market', 'be', 'see', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stock', ',', 'which', 'be', 'not', 'where', 'the', 'smart', 'money', 'be', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.', 'The', 'FAANG', 'stock', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'be', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerate', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'break', ',', 'send', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy.', 'Investors', 'be', 'wonder', 'what', '’', 's', 'next.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimist', ',', 'Studebaker', 'thinks.', \"'This\", 'be', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'month', ',', 'he', 'said.', \"'\", 'I', 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'come', 'from', 'the', 'FAANG', 'stock', ',', \"'\", 'he', 'added.', 'That', 'be', ',', 'if', 'the', 'stock', 'market', 'be', 'to', 'see', 'gain', 'in', 'the', 'next', 'several', 'month', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'companies.', 'The', 'market', 'be', 'see', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'part', 'of', 'the', 'market', ',', \"'\", 'he', 'said.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trade', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'month', 'and', 'months.', \"'\", 'The', 'FAANG', '’', 's', 'be', 'pricey', 'stock', ',', 'he', 'say', ',', 'point', 'out', 'that', 'investor', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'number', ',', \"'\", 'he', 'said.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investor', 'be', 'go', 'to', 'shift', 'drastically', 'into', 'value', 'stocks.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand.', \"'\", 'Studebaker', ',', 'who', 'run', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'asset', 'under', 'management', ',', 'think', 'that', 'AI', 'and', 'robotics', 'be', 'good', 'area', 'of', 'growth.', 'His', 'ETF', 'be', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stock', 'be', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'be', 'down', 'since', 'March', '12.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'a', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'a', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs.', 'He', 'point', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'business', 'be', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business.', \"'\", 'He', 'also', 'say', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'business', 'be', 'insignificant.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'do', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'be', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represent', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'give', 'the', 'company', 'room', 'to', 'grow', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_text = []\n",
    "\n",
    "for word in POS_tag:\n",
    "    # 품사를 유지한 채로 lemmatized 원형 복원을 하라.\n",
    "    lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0], pos = get_wordnet_pos(word[1]))))\n",
    "    \n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 처리 및 불필요한 품사 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAANG', 'stock', 'win', 'more', 'growth', 'near', 'future', 'accord', 'Bill', 'Studebaker', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global.', 'Studebaker', 'argue', \"'reallocation\", 'large-cap', 'tech', 'stock', 'market-weight', 'stocks.', 'FAANG', 'stock', 'rough', 'few', 'week', 'hard', 'March', 'One', 'FAANG', 'midst', 'Amazon', 'accord', 'Studebaker.', 'stock', 'market', \"'reallocation\", 'FAANG', 'stock', 'smart', 'money', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.', 'FAANG', 'stock', 'Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'March', 'trend', 'accelerate', 'news', 'massive', 'Facebook', 'data', 'scandal', 'break', 'send', 'tech-heavy', 'Nasdaq', 'downward', 'frenzy.', 'Investors', 'wonder', 'next', 'good', 'news', 'FAANG', 'stock', 'optimist', 'Studebaker', 'thinks.', \"'This\", 'dead', 'trade', 'next', 'several', 'month', 'lot', 'performance', 'attribution', 'FAANG', 'stock', 'stock', 'market', 'gain', 'next', 'several', 'month', 'big', 'tech', 'market', \"'reallocation\", 'large-cap', 'technology', 'other', 'part', 'market', 'trend', 'foreseeable', 'future.', 'reallocation', 'trade', 'de-risking', 'month', 'months.', 'FAANG', 'pricey', 'stock', 'point', 'investor', 'law', 'big', 'number', 're', 'big', 'cap', 'mean', 're', 'safe', 'investor', 'value', 'stocks.', 'favorable', 'macro', 'backdrop', 'strong', 'growth', 'demand.', 'Studebaker', 'run', 'artificial', 'intelligence', 'robotics', 'exchange-traded', 'fund', 'asset', 'management', 'AI', 'robotics', 'good', 'area', 'growth.', 'ETF', 'past', 'year', 'FAANG', 'stock', 'same', 'span', 'March', 'many', 'point', 'artificial', 'intelligence', 'area', 'boost', 'Google', 'Amazon', 'Studebaker', 'sign', 'significant', 'growth', 'FAANGs.', 'point', 'percent', 'business', \"'AI\", 'needle', 'business.', \"'the\", 'revenue', 'mix', 'attributable', 'AI', 'business', 'insignificant.', 'FAANG', 'do', 'FAANG', 'Amazon', 'ecommerce', 'represent', 'small', 'portion', 'global', 'retail', 'market', 'give', 'company', 'room']\n"
     ]
    }
   ],
   "source": [
    "stopwords = []\n",
    "\n",
    "wanted_POS = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "        \n",
    "punctuations = list(str(string.punctuation))\n",
    "stopwords = stopwords + punctuations\n",
    "\n",
    "stopwords_plus = ['t', 'isn']\n",
    "stopwords = stopwords + stopwords_plus\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "processed_text = []\n",
    "\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords:\n",
    "        processed_text.append(word)\n",
    "        \n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'win', \"'reallocation\", 'market', 'investor', 'Bill', 'downward', 'foreseeable', 'FAANGs.', 'business', 'retail', 'large-cap', 'news', 'Nasdaq', 'optimist', 'global', 'cap', 'thinks.', 'big', 'data', 'accelerate', 'AI', 'backdrop', 'Amazon', 'year', 'near', 'more', 'trade', 'Netflix', 'needle', 'growth.', 'break', 'attribution', 'few', 'de-risking', 'accord', 'Facebook', 'boost', 'pricey', 'tech-heavy', 'smart', 'significant', \"'AI\", 'future.', 'Global', 'tell', 'month', 'run', 'Officer', 'Robo', 'lot', 'March', 'wonder', 'business.', 'law', 'founder', 'exchange-traded', 'revenue', 'insignificant.', 'money', 'asset', 'small', 'rough', 'attributable', 'Studebaker.', 'fund', 'demand.', 'Business', 'value', 'do', 'same', 'intelligence', 'point', 'technology', 'Studebaker', 'Global.', 'reallocation', 'send', 'strong', \"'This\", 'other', 'performance', 'Insider.', 'sign', 'ETF', 'Investment', 'past', 'week', 'area', 'mix', 'good', 'argue', 'market-weight', \"'the\", 'stock', 'trend', 'gain', 'Investors', 'robotics', 'many', 'give', 'macro', 'room', 'next', 're', 'massive', 'One', 'favorable', 'percent', 'represent', 'several', 'portion', 'hard', 'ecommerce', 'management', 'part', 'frenzy.', 'growth', 'tech', 'FAANG', 'midst', 'stocks.', 'dead', 'mean', 'safe', 'number', 'span', 'Chief', 'Google', 'months.', 'company', 'scandal', 'future', 'Apple']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(processed_text))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# windown_size = 3 -> 3-gram과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "#토큰별로 그래프 edge를 Matrix 형태로 생성\n",
    "weighted_edge = np.zeros((vocab_len, vocab_len), dtype = np.float32)\n",
    "\n",
    "# 각 토큰 노트별로 점수 계싼을 위한 배열 생성\n",
    "score = np.zeros((vocab_len), dtype = np.float32)\n",
    "\n",
    "# coocurrence를 판단하기 위한 window 사이즈 설정\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0, vocab_len):\n",
    "    score[i] = 1\n",
    "    for j in range(0, vocab_len):\n",
    "        if j == i:\n",
    "            continue\n",
    "        else:\n",
    "            for window_start in range(0, (len(processed_text) - window_size)):\n",
    "                # window_end = 0 + 3, 1 + 3, 2+ 3...\n",
    "                window_end = window_start + window_size\n",
    "                \n",
    "                window = processed_text[window_start: window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'win', \"'reallocation\", 'market', 'investor', 'Bill', 'downward', 'foreseeable', 'FAANGs.', 'business', 'retail', 'large-cap', 'news', 'Nasdaq', 'optimist', 'global', 'cap', 'thinks.', 'big', 'data', 'accelerate', 'AI', 'backdrop', 'Amazon', 'year', 'near', 'more', 'trade', 'Netflix', 'needle', 'growth.', 'break', 'attribution', 'few', 'de-risking', 'accord', 'Facebook', 'boost', 'pricey', 'tech-heavy', 'smart', 'significant', \"'AI\", 'future.', 'Global', 'tell', 'month', 'run', 'Officer', 'Robo', 'lot', 'March', 'wonder', 'business.', 'law', 'founder', 'exchange-traded', 'revenue', 'insignificant.', 'money', 'asset', 'small', 'rough', 'attributable', 'Studebaker.', 'fund', 'demand.', 'Business', 'value', 'do', 'same', 'intelligence', 'point', 'technology', 'Studebaker', 'Global.', 'reallocation', 'send', 'strong', \"'This\", 'other', 'performance', 'Insider.', 'sign', 'ETF', 'Investment', 'past', 'week', 'area', 'mix', 'good', 'argue', 'market-weight', \"'the\", 'stock', 'trend', 'gain', 'Investors', 'robotics', 'many', 'give', 'macro', 'room', 'next', 're', 'massive', 'One', 'favorable', 'percent', 'represent', 'several', 'portion', 'hard', 'ecommerce', 'management', 'part', 'frenzy.', 'growth', 'tech', 'FAANG', 'midst', 'stocks.', 'dead', 'mean', 'safe', 'number', 'span', 'Chief', 'Google', 'months.', 'company', 'scandal', 'future', 'Apple']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각 노드의 score 계싼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2242525 , 0.66527504, 1.6234951 , 2.8719614 , 1.2568297 ,\n",
       "       1.1811248 , 0.91442543, 0.731136  , 0.70616627, 1.3837832 ,\n",
       "       0.77580357, 1.1581424 , 1.2199225 , 0.92881674, 0.63703525,\n",
       "       0.80521816, 0.70529735, 0.71065885, 1.787719  , 0.8023953 ,\n",
       "       0.6905883 , 1.4112239 , 0.78940916, 2.308997  , 0.6961058 ,\n",
       "       0.7143598 , 0.69716895, 1.3174993 , 0.6657063 , 0.8598148 ,\n",
       "       0.7404819 , 0.89812934, 0.67318463, 0.73916477, 0.7023294 ,\n",
       "       1.2062559 , 1.284316  , 0.6726134 , 0.63107026, 0.93079394,\n",
       "       0.64137965, 0.70571345, 0.81954134, 0.75365865, 0.6416816 ,\n",
       "       0.6838331 , 1.7559868 , 0.6702931 , 1.135585  , 1.1578162 ,\n",
       "       0.6927791 , 1.8376529 , 0.7714824 , 0.89048195, 0.6786046 ,\n",
       "       1.1443967 , 0.7679509 , 0.890721  , 0.68854123, 0.64852583,\n",
       "       0.8088296 , 0.8288284 , 0.6926921 , 0.81820357, 0.6342842 ,\n",
       "       0.8019854 , 0.69562376, 0.68982774, 0.7146721 , 0.6346033 ,\n",
       "       0.6561458 , 1.2497158 , 1.8010771 , 0.6905084 , 3.3837745 ,\n",
       "       0.64441377, 0.7532566 , 0.9206491 , 0.7660756 , 0.7313726 ,\n",
       "       0.70761573, 0.69626546, 0.6700934 , 0.682693  , 0.75970316,\n",
       "       1.1161134 , 0.7437016 , 0.7531246 , 1.2786304 , 0.8614473 ,\n",
       "       1.252743  , 0.6364465 , 0.6333566 , 0.89851964, 5.3028994 ,\n",
       "       1.267172  , 0.6428498 , 0.8346329 , 1.3243495 , 0.67677486,\n",
       "       0.6859135 , 0.7871578 , 0.15      , 1.8072842 , 1.29127   ,\n",
       "       0.7162731 , 0.67223024, 0.76342684, 0.7322796 , 0.7922535 ,\n",
       "       1.1900194 , 0.8351213 , 0.736889  , 0.7317639 , 0.78651446,\n",
       "       0.7008157 , 0.88644224, 1.9307556 , 1.1218808 , 5.3873506 ,\n",
       "       0.6468713 , 1.2516756 , 0.7298886 , 0.71833277, 0.71398985,\n",
       "       0.6823258 , 0.6804649 , 1.1231216 , 1.2083    , 0.6573831 ,\n",
       "       0.46458283, 0.853213  , 0.6981258 , 0.66493154], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of artificial: 1.2242525\n",
      "Score of win: 0.66527504\n",
      "Score of 'reallocation: 1.6234951\n",
      "Score of market: 2.8719614\n",
      "Score of investor: 1.2568297\n",
      "Score of Bill: 1.1811248\n",
      "Score of downward: 0.91442543\n",
      "Score of foreseeable: 0.731136\n",
      "Score of FAANGs.: 0.70616627\n",
      "Score of business: 1.3837832\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    for i in range(0,vocab_len):\n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "        break\n",
    "for i in range(0,10) : #vocab_len):\n",
    "    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 핵심 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "FAANG : 5.3873506\n",
      "stock : 5.3028994\n",
      "Studebaker : 3.3837745\n",
      "market : 2.8719614\n",
      "Amazon : 2.308997\n",
      "growth : 1.9307556\n",
      "March : 1.8376529\n",
      "next : 1.8072842\n",
      "point : 1.8010771\n",
      "big : 1.787719\n"
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(score),0)\n",
    "keywords_num = 10\n",
    "print(\"Keywords:\\n\")\n",
    "for i in range(0,keywords_num):\n",
    "    print(str(vocabulary[sorted_index[i]])+\" : \" + str(score[sorted_index[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank 핵심 구 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 불용어를 기준으로 구 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock', 'win'], ['more', 'growth'], ['near', 'future'], ['accord'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker', 'argue'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['FAANG', 'stock'], ['rough', 'few', 'week'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['accord'], ['Studebaker.'], ['stock', 'market'], [\"'reallocation\"], ['FAANG', 'stock'], ['smart', 'money'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.'], ['FAANG', 'stock'], ['Facebook'], ['Apple'], ['Amazon'], ['Netflix'], ['Google'], ['March'], ['trend'], ['accelerate'], ['news'], ['massive', 'Facebook', 'data', 'scandal', 'break'], ['send'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['wonder'], ['next'], ['good', 'news'], ['FAANG', 'stock', 'optimist'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'month'], ['lot'], ['performance', 'attribution'], ['FAANG', 'stock'], ['stock', 'market'], ['gain'], ['next', 'several', 'month'], ['big', 'tech'], ['market'], [\"'reallocation\"], ['large-cap', 'technology'], ['other', 'part'], ['market'], ['trend'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['month'], ['months.'], ['FAANG'], ['pricey', 'stock'], ['point'], ['investor'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['investor'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['Studebaker'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['asset'], ['management'], ['AI'], ['robotics'], ['good', 'area'], ['growth.'], ['ETF'], ['past', 'year'], ['FAANG', 'stock'], ['same', 'span'], ['March'], ['many', 'point'], ['artificial', 'intelligence'], ['area'], ['boost'], ['Google'], ['Amazon'], ['Studebaker'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['point'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['AI'], ['business'], ['insignificant.'], ['FAANG'], ['do'], ['FAANG'], ['Amazon'], ['ecommerce'], ['represent'], ['small', 'portion'], ['global', 'retail', 'market'], ['give'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    if word in stopwords:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock', 'win'], ['more', 'growth'], ['near', 'future'], ['accord'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker', 'argue'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['FAANG', 'stock'], ['rough', 'few', 'week'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['Studebaker.'], ['stock', 'market'], ['smart', 'money'], ['Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.'], ['Facebook'], ['Apple'], ['Netflix'], ['Google'], ['trend'], ['accelerate'], ['news'], ['massive', 'Facebook', 'data', 'scandal', 'break'], ['send'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['wonder'], ['next'], ['good', 'news'], ['FAANG', 'stock', 'optimist'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'month'], ['lot'], ['performance', 'attribution'], ['gain'], ['big', 'tech'], ['market'], ['large-cap', 'technology'], ['other', 'part'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['month'], ['months.'], ['FAANG'], ['pricey', 'stock'], ['point'], ['investor'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['Studebaker'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['asset'], ['management'], ['AI'], ['robotics'], ['good', 'area'], ['growth.'], ['ETF'], ['past', 'year'], ['same', 'span'], ['many', 'point'], ['area'], ['boost'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['insignificant.'], ['do'], ['ecommerce'], ['represent'], ['small', 'portion'], ['global', 'retail', 'market'], ['give'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "unique_phrases = []\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock', 'win'], ['more', 'growth'], ['near', 'future'], ['accord'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker', 'argue'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['FAANG', 'stock'], ['rough', 'few', 'week'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['Studebaker.'], ['stock', 'market'], ['smart', 'money'], ['Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.'], ['Apple'], ['Netflix'], ['Google'], ['trend'], ['accelerate'], ['massive', 'Facebook', 'data', 'scandal', 'break'], ['send'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['wonder'], ['good', 'news'], ['FAANG', 'stock', 'optimist'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'month'], ['lot'], ['performance', 'attribution'], ['gain'], ['big', 'tech'], ['large-cap', 'technology'], ['other', 'part'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['months.'], ['pricey', 'stock'], ['investor'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['asset'], ['management'], ['AI'], ['good', 'area'], ['growth.'], ['ETF'], ['past', 'year'], ['same', 'span'], ['many', 'point'], ['boost'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['insignificant.'], ['do'], ['ecommerce'], ['represent'], ['small', 'portion'], ['global', 'retail', 'market'], ['give'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    \n",
    "    \n",
    "    for phrase in unique_phrases:\n",
    "        \n",
    "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "            \n",
    "            unique_phrases.remove([word])\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각 구의 Score 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: 'FAANG stock win', Score: 11.355524957180023\n",
      "Keyword: 'more growth', Score: 2.6279245615005493\n",
      "Keyword: 'near future', Score: 1.4124855995178223\n",
      "Keyword: 'accord', Score: 1.2062559127807617\n",
      "Keyword: 'Bill Studebaker', Score: 4.564899325370789\n",
      "Keyword: 'founder', Score: 1.1443966627120972\n",
      "Keyword: 'Chief Investment Officer', Score: 3.3748199939727783\n",
      "Keyword: 'Robo Global. Studebaker argue', Score: 5.822450935840607\n",
      "Keyword: ''reallocation', Score: 1.623495101928711\n",
      "Keyword: 'large-cap tech stock', Score: 7.582922577857971\n",
      "Keyword: 'market-weight stocks.', Score: 1.8850321769714355\n",
      "Keyword: 'FAANG stock', Score: 10.690249919891357\n",
      "Keyword: 'rough few week', Score: 2.1849814653396606\n",
      "Keyword: 'hard', Score: 0.7368890047073364\n",
      "Keyword: 'March', Score: 1.8376529216766357\n",
      "Keyword: 'One FAANG', Score: 6.0595808029174805\n",
      "Keyword: 'midst', Score: 0.6468713283538818\n",
      "Keyword: 'Amazon', Score: 2.3089969158172607\n",
      "Keyword: 'Studebaker.', Score: 0.6342841982841492\n",
      "Keyword: 'stock market', Score: 8.174860715866089\n",
      "Keyword: 'smart money', Score: 1.2899054884910583\n",
      "Keyword: 'Robo Global Bill Studebaker tell Business Insider.', Score: 8.408151388168335\n",
      "Keyword: 'Apple', Score: 0.6649315357208252\n",
      "Keyword: 'Netflix', Score: 0.6657062768936157\n",
      "Keyword: 'Google', Score: 1.208299994468689\n",
      "Keyword: 'trend', Score: 1.2671719789505005\n",
      "Keyword: 'accelerate', Score: 0.6905882954597473\n",
      "Keyword: 'massive Facebook data scandal break', Score: 4.554326713085175\n",
      "Keyword: 'send', Score: 0.9206491112709045\n",
      "Keyword: 'tech-heavy Nasdaq', Score: 1.859610676765442\n",
      "Keyword: 'downward frenzy. Investors', Score: 2.6355005502700806\n",
      "Keyword: 'wonder', Score: 0.7714824080467224\n",
      "Keyword: 'good news', Score: 2.472665548324585\n",
      "Keyword: 'FAANG stock optimist', Score: 11.327285170555115\n",
      "Keyword: 'Studebaker thinks. 'This', Score: 4.825805962085724\n",
      "Keyword: 'dead trade', Score: 2.0473878979682922\n",
      "Keyword: 'next several month', Score: 4.753290414810181\n",
      "Keyword: 'lot', Score: 0.6927791237831116\n",
      "Keyword: 'performance attribution', Score: 1.3694500923156738\n",
      "Keyword: 'gain', Score: 0.6428498029708862\n",
      "Keyword: 'big tech', Score: 2.909599781036377\n",
      "Keyword: 'large-cap technology', Score: 1.848650872707367\n",
      "Keyword: 'other part', Score: 1.4084314107894897\n",
      "Keyword: 'foreseeable future.', Score: 1.4847946763038635\n",
      "Keyword: 'reallocation trade', Score: 2.070755898952484\n",
      "Keyword: 'de-risking', Score: 0.7023293972015381\n",
      "Keyword: 'months.', Score: 0.6573830842971802\n",
      "Keyword: 'pricey stock', Score: 5.933969616889954\n",
      "Keyword: 'investor', Score: 1.2568297386169434\n",
      "Keyword: 'law', Score: 0.6786046028137207\n",
      "Keyword: 'big number', Score: 2.470044791698456\n",
      "Keyword: 're big cap', Score: 3.784286379814148\n",
      "Keyword: 'mean', Score: 0.7183327674865723\n",
      "Keyword: 're safe', Score: 2.005259871482849\n",
      "Keyword: 'value stocks.', Score: 1.9663476943969727\n",
      "Keyword: 'favorable macro backdrop', Score: 2.3399937748908997\n",
      "Keyword: 'strong growth demand.', Score: 3.392454981803894\n",
      "Keyword: 'run', Score: 0.6702930927276611\n",
      "Keyword: 'artificial intelligence', Score: 2.473968267440796\n",
      "Keyword: 'robotics exchange-traded fund', Score: 2.894285798072815\n",
      "Keyword: 'asset', Score: 0.8088296055793762\n",
      "Keyword: 'management', Score: 0.7865144610404968\n",
      "Keyword: 'AI', Score: 1.4112238883972168\n",
      "Keyword: 'good area', Score: 2.531373381614685\n",
      "Keyword: 'growth.', Score: 0.7404819130897522\n",
      "Keyword: 'ETF', Score: 0.7597031593322754\n",
      "Keyword: 'past year', Score: 1.4398073554039001\n",
      "Keyword: 'same span', Score: 1.336610734462738\n",
      "Keyword: 'many point', Score: 2.477851986885071\n",
      "Keyword: 'boost', Score: 0.6726133823394775\n",
      "Keyword: 'sign', Score: 0.6826930046081543\n",
      "Keyword: 'significant growth', Score: 2.636469066143036\n",
      "Keyword: 'FAANGs.', Score: 0.7061662673950195\n",
      "Keyword: 'percent', Score: 0.7322795987129211\n",
      "Keyword: 'business', Score: 1.383783221244812\n",
      "Keyword: ''AI', Score: 0.819541335105896\n",
      "Keyword: 'needle', Score: 0.8598148226737976\n",
      "Keyword: 'business.', Score: 0.8904819488525391\n",
      "Keyword: ''the revenue mix', Score: 2.6506879329681396\n",
      "Keyword: 'attributable', Score: 0.8182035684585571\n",
      "Keyword: 'insignificant.', Score: 0.6885412335395813\n",
      "Keyword: 'do', Score: 0.6346033215522766\n",
      "Keyword: 'ecommerce', Score: 0.7317638993263245\n",
      "Keyword: 'represent', Score: 0.7922534942626953\n",
      "Keyword: 'small portion', Score: 1.6639496684074402\n",
      "Keyword: 'global retail market', Score: 4.45298308134079\n",
      "Keyword: 'give', Score: 0.6859135031700134\n",
      "Keyword: 'company room', Score: 0.6145828366279602\n"
     ]
    }
   ],
   "source": [
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score=0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += \" \"\n",
    "        phrase_score+=score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "i=0\n",
    "for keyword in keywords:\n",
    "    print(\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66527504"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각 구를 Score로 정렬하여 핵심 구 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "FAANG stock win, \n",
      "FAANG stock optimist, \n",
      "FAANG stock, \n",
      "Robo Global Bill Studebaker tell Business Insider., \n",
      "stock market, \n",
      "large-cap tech stock, \n",
      "One FAANG, \n",
      "pricey stock, \n",
      "Robo Global. Studebaker argue, \n",
      "Studebaker thinks. 'This, \n"
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "keywords_num = 10\n",
    "print(\"Keywords:\\n\")\n",
    "for i in range(0,keywords_num):\n",
    "    print(str(keywords[sorted_index[i]])+\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stocks',\n",
       " 'stock',\n",
       " 'studebaker',\n",
       " 'trade',\n",
       " 'trades',\n",
       " 'amazon',\n",
       " 'tech',\n",
       " 'attribution',\n",
       " 'attributable',\n",
       " 'cap',\n",
       " 'facebook',\n",
       " 'market',\n",
       " 'future',\n",
       " 'growth',\n",
       " 'thinks',\n",
       " 'think',\n",
       " 'frenzy',\n",
       " 'investment',\n",
       " 'big',\n",
       " 'global',\n",
       " 'favorable macro',\n",
       " 'mix',\n",
       " 'exchange',\n",
       " 'broke',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'scandal',\n",
       " 'netflix',\n",
       " 'artificial intelligence',\n",
       " 'ninety percent',\n",
       " 'shift drastically',\n",
       " 'said',\n",
       " 'hit hard',\n",
       " 'small portion',\n",
       " 'news',\n",
       " 'pointing',\n",
       " 'point',\n",
       " 'area',\n",
       " 'pointed',\n",
       " 'good',\n",
       " 'better areas',\n",
       " 'smart money',\n",
       " 'past',\n",
       " 'year']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "keywords(Text, ratio = 0.5).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succinctly encapsulates, text is data in that it is a useful source of\n",
      "has increased, text mining analysis has evolved from a labor-intensive\n",
      "transparency in conducting monetary policy, it is important to develop and\n",
      "usefulness of the text mining approach, saying that “A more rigorous and\n",
      "constructive means of judging the effects of the Fed’s new transcript policy\n",
      "With all this in mind, we use text mining to extract the quantitative\n",
      "information about monetary policy decision making from 232,658\n",
      "qualitative contents of the Bank of Korea’s MPB (Monetary Policy\n",
      "1) We use 151 sets of minutes from the MPB (Monetary Policy Board), 206,223 news articles related to\n",
      "Board) minutes into quantitative indicators, we measure the sentiment\n",
      "We find that our lexicon-based indicators help to\n",
      "explain the current and future monetary policy when considering an\n",
      "outperform English text-based indicators, a media-based measure of\n",
      "economic policy uncertainty measure based on Baker, Bloom, and Davis\n",
      "a field-specific dictionary and the original Korean text, not a translated\n",
      "monetary policy decision making at the BOK (Bank of Korea).\n",
      "demonstrates that lexicon-based indicators have ample information about\n",
      "monetary policy, beyond information contained in macroeconomic\n",
      "example, one may interpret our indicators as a latent variable of the BOK\n",
      "analyze the effect of monetary policy.\n",
      "future direction of monetary policy.\n",
      "important to analyze what kinds of information the BOK documents\n",
      "In this regard, our study demonstrates the usefulness of text\n",
      "sentiment of a single word (a uni-gram) or a bi-gram phrase that\n",
      "5-gram) as features, we consider the context and can better capture the\n",
      "polarity – hawkish, neutral, or dovish, in our case – of these n-grams, we\n",
      "develop two kinds of sentiment indicators based on two contrasting but\n",
      "not rely on the researchers’ subjective selection of seed words and uses\n",
      "word, or of an n-gram in our case, based on its statistical association with\n",
      "market information (e.g., stock returns), indicators based on this approach\n",
      "decides the polarity based on the proximity to the pre-determined seed\n",
      "seed words, we use a state-ofthe-art domain-specific sentiment induction\n",
      "Third, we use our own natural language processing (NLP) tool called\n",
      "eKoNLPy (Korean NLP Python Library for Economic Analysis) to address\n",
      "the difficulties associated with the Korean language such as field-specific\n",
      "non-Korean loan words, irregular conjugation of verbs and adjectives, and\n",
      "the coauthors, and is specifically designed for text mining research in the\n",
      "information, too, such as macroeconomic uncertainty and stock market\n",
      "text-based indicators that capture the sentiment of monetary policy.\n",
      "text mining applied to economics, we limit our discussion relevant to\n",
      "central banking.2) Earlier studies that use text mining approach rely more\n",
      "methods that measure tones or sentiments.\n",
      "(2016) construct economic policy uncertainty (EPU) measure by counting\n",
      "the number of news articles that contain specific words such as\n",
      "(2017) for a survey of text mining approach to economic research and Bholat et al.\n",
      "innovations in policy uncertainty are indicators for declines in\n",
      "Concerning monetary policy, several studies attempt to extract\n",
      "measure the sentiment of monetary policy, Picault and Renault (2017)\n",
      "monetary policy sentiment helps explain the current and future ECB\n",
      "high-frequency sentiment from Bank of England’s daily market\n",
      "While we reckon that text mining approach that uses the Korean\n",
      "research direction by comparing the methods of bag-of-words and\n",
      "sentiment measures are statistically associated with asset prices such as\n",
      "(2017) and our study use the sentiment analysis, there are several points\n",
      "First, we use n-grams to capture the subtlety of texts by\n",
      "Second, we use a field-specific dictionary specifically\n",
      "designed for text mining in economics and finance.\n",
      "construct the sentiment index based on market approach, in addition to\n",
      "3) We explain our application of text mining approach in the following section, including the market and\n",
      "Sentiment analysis generally takes the following processes: (i) preparing\n",
      "the corpus of interests, (ii) pre-processing texts, (iii) feature selection, (iv)\n",
      "polarity or sentiment classification of features and (v) measuring\n",
      "While our target texts are the MPB minutes, we use a large\n",
      "amount of other documents to build field-specific lexicons.\n",
      "5) More examples and details of text mining for central banks can be found on Bholat et al.\n",
      "international finance, financial markets, and monetary policy.\n",
      "• Discussion concerning monetary policy decision records the views of\n",
      "• Result of deliberation on monetary policy\n",
      "2017 (151 minutes) from the BOK website.7) We use only the second\n",
      "We collect news articles that include the word ‘interest rates (금리)’\n",
      "contain the information on the general economy, monetary policy, financial\n",
      "market, and public perception on the BOK’s future monetary policy\n",
      "The number of news articles for our final use is 206,223.\n",
      "perform event studies that attempt to guage the market impact of monetary policy.\n",
      "analyst reports show the experts’ view on the monetary policy and the\n",
      "2.2 Korean NLP Python Library for Economic Analysis (eKoNLPy)\n",
      "To convert Korean text into numerical expressions (e.g., bag of words,\n",
      "be important when one uses n-grams.\n",
      "n-gram models, which hinders polarity classification.\n",
      "economics and finance and uses its own morphologocial analyzer.\n",
      "domain-specific terms (i.e., jargon and foreign words), eKoNLPy is\n",
      "since lemmatization consider the morphological analysis of words.\n",
      "Since eKoNLPy is developed for the purpose of text mining for economic\n",
      "analysis, compared to KoNLPy. For example, consider the following sentence:\n",
      "KoNLPy, which uses a general-purpose dictionary, cannot.13) Consider\n",
      "To address this problem, we decide to use n-grams.15) However,\n",
      "sample; as the lexicons are too much specific to the target documents, it\n",
      "To avoid the explosion of dimension, we use the limited word set in\n",
      "forming n-grams by limiting the part-of-speech tag of words to nouns\n",
      "14) Apel and Grimaldi (2014) use two-word combinations (bi-grams) of a noun and an adjective, such as\n",
      "15) Picault and Renault (2017) define the field-specific lexicon by considering n-grams (from 1-gram to\n",
      "They classify the polarity of n-grams by calculating the\n",
      "field-specific lexicon is composed of 34,052 n-grams.\n",
      "n-gram, which is a contiguous sequence of n words in a text, this problem is aggravated.\n",
      "step is to classify polarity of these n-grams to use them for measuring\n",
      "If there are no well-known lists of polarity words like Harvard-IV or\n",
      "LM dictionary, we need to classify the polarity of our selected features\n",
      "similarity between words and polarity proto types.21)\n",
      "18) From the perspective of n-grams being effective opinion-bearing features for sentiment analysis, ideal\n",
      "lowers out-of-sample performance in terms of the accuracy of polarity classification, which we discuss in\n",
      "McDonald (2011), which has lists of single words by category (Negative, Positive, Uncertainty, Litigious,\n",
      "Their research indicates that the LM dictionary has a better correlation with\n",
      "lexical-based approach, there are three methods to obtain the polarity\n",
      "word list: manual, dictionary-based, and corpus-based.\n",
      "disadvantage of this approach is its inability to find field-specific polarity\n",
      "The corpus-based method finds polarity words by searching\n",
      "context-specific sentiment words and their polarities using a corpus of that\n",
      "economics or finance area where jargon or words with different\n",
      "We classify polarity of n-grams in two ways.\n",
      "approach that classifies polarity from market information using machine\n",
      "The other is the corpus-based approach that classifies polarity\n",
      "associated with the sentiments of monetary policy (dovish or hawkish) using Google Search.\n",
      "24) Word embedding is the collective name for a set of language modeling and feature learning techniques in natural\n",
      "classify the polarity of features using market information and call it\n",
      "use words as the dependent variable and stock returns as the explanatory\n",
      "That is, they estimate market reactions and use it as\n",
      "approach is that they start from the known word lists like Harvard IV-4 or\n",
      "LM dictionary and use regressionbased approach to determine the relative\n",
      "Our market approach also uses market information\n",
      "large corpus and classify their polarity, we use the machine learning\n",
      "For our market approach, we use the Na¨ıve Bayes classifier (NBC), a\n",
      "The former is not available for monetary policy.\n",
      "we label news articles and reports in our corpus as hawkish (dovish) if the\n",
      "probability of each feature given the class (hawkish/dovish), which we use\n",
      "We classify the polarity of our lexicon as hawkish (dovish) if the\n",
      "stance in that it does not use market information at all.\n",
      "the seed words, which we call lexical approach.\n",
      "Lexical approach is based on an intuitive observation: if two words\n",
      "Then the polarity of an unknown word can be determined\n",
      "We place the seed set of words (n-grams in our case) and our n-grams in\n",
      "a vector space (lexical graph) and measure the proximity of our n-grams to\n",
      "The parameters we use for training are 5-gram for center words, 5-gram for\n",
      "Likewise the market approach, we classify polarity of our lexicon as\n",
      "To see if the market and lexical approach give the similar result, we\n",
      "market approach and 23,956 n-grams from the lexical approach, there are\n",
      "n-grams) have the same polarity.\n",
      "Governor’s news conference about monetary policy decisions.\n",
      "number of hawkish and dovish features (n-grams) in each sentence.\n",
      "indicator of quantifying the sentiment of monetary policy.\n",
      "indicator based on market approach and lexical approach by   and\n",
      "power of the current and future monetary policy decisions in the following\n",
      "1. Can our lexicon-based indicators (  and   ) explain the\n",
      "BOK’s current and future monetary policy decisions?\n",
      "2. Is it important to use a field-specific dictionary?\n",
      "3. Is it important to use the original Korean text, not Korean-to-English\n",
      "indicators, macroeconomic variables, and other proxies related to economic\n",
      "text and use a dictionary specific to economics and finance terminologies.\n",
      "lexicon-based indicators that capture the sentiment (or tone) of the\n",
      "The former uses the market\n",
      "BOK policy rate, other measures of economic uncertainty.\n",
      "Panel (b) shows that our indicator \n",
      "captures the movements of the BOK policy rate.\n",
      "compare our indicator with other measures such as economic policy\n",
      "financial crisis, our indicator and economic policy uncertainty index do\n",
      "indicator is negatively associated with uncertainty index.\n",
      "34) For both measures of economic policy uncertainty and uncertainty, we use the Korean versions.\n",
      "August 2018, one can use economic policy uncertainty indexes of 25 countries at\n",
      "Figure 5 compares our indicator   with monetary policy decisions\n",
      "monetary policy,   takes a value of zero when there is no change in\n",
      "increase of policy rate by 25 basis points), −1 for a dovish monetary\n",
      "a non-standard policy.35) Output gap (   ) is defined as the difference\n",
      "Panel (a) clearly shows that our indicator well tracks the changes in\n",
      "Panel (b)–(d) shows the time-series of output gap, CPI, and stock market\n",
      "Correlation coefficient of   with monetary policy\n",
      "It clearly shows that the BOK policy rate and industrial\n",
      "BOK policy rate has a\n",
      "correlation with economic policy uncertainty measure of US and Korea.\n",
      "In order to assess the relation between the BOK MPB’s policy rate\n",
      "explanatory power of our lexicon-based indicators (\n",
      "35) Appendix A shows the chronological list of the BOK’s non-stanard monetary policy.\n",
      "and compare the explanatory powers of their own lexicon-based indicators\n",
      "where   is the BOK’s monetary policy stance and (   ) is the\n",
      "monetary policy stance in addition to macroeconomic variables.\n",
      "we consider our lexicon-based indicators (\n",
      "policy uncertainty index (  ) by Baker et al.\n",
      "For changes in monetary policy\n",
      "The leading index is based on nine indicators that are closely related to future\n",
      "stance (  ), we use two variables following Picault and Renault\n",
      "sample period, the BOK MPB increases its policy rate by 25 basis points\n",
      "The other is a variable of monetary policy decision\n",
      "(  ) to account for the BOK’s non-standard monetary policy.\n",
      "previously released macroeconomic data, then our indicators of  \n",
      "expect a positive coefficient: more hawkish (dovish) sentiment should be\n",
      "result when we measure the change in monetary policy stance (  ) as\n",
      "the change in policy rate (  ).\n",
      "It shows that our indicators  \n",
      "suggests that our lexicon-based indicators contain additional information\n",
      "38) Since we use the monthly data for our empirical analysis, we measure the changes in policy rate on a\n",
      "Table 8 shows the result when the dependent variable is   that\n",
      "Apel and Grimaldi (2014) use the following specification and\n",
      "contain the relevant information on the future monetary policy stance\n",
      "3. Comparison with Other Text-Based Indicators\n",
      "one translates Korean text into English and applies the standard\n",
      "Further, is it really important to use a field-specific\n",
      "and uses a general-purpose dictionary (  ), and three English-based\n",
      "the most popular tools for analyzing Korean texts.41) But it uses a\n",
      "general-purpose dictionary, not like   and   that uses a\n",
      "dictionary specific to the field of economics and finance.\n",
      "text analysis, we translate all the MPB’s minutes into English using Google\n",
      "dictionary and  is based on the field-specific dictionary of\n",
      "While English-based indicators\n",
      "We compare the performance of these indicators based on equation (5)\n",
      "current and future BOK policy rate.\n",
      "field-specific dictionary and (iii) is to compare the results from Korean\n",
      "the current change in BOK policy rate (  ), column (1) and (2)\n",
      "show that,   fails to capture the change in BOK policy rate, while it\n",
      "is based on the most popular tool for the Korean text analysis.\n",
      "suggest that it is important to use a field-specific dictionary.\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "Deciphering Monetary Policy Board Minutes through Text Mining\n",
      "BOK policy rate     ,   outperforms other indicators in terms\n",
      "figure 7 and table 9, it is desirable to use the original Korean text with a\n",
      "We develop text-based indicators that quantify the sentiment of\n",
      "monetary policy using a field-specific Korean dictionary and n-grams.\n",
      "show that our indicators help explain the current and future monetary\n",
      "policy decisions and perform better compared to other indicators.\n",
      "show that using a field-specific dictionary and the original Korean text is\n",
      "important to examine what kinds of information our indicators do (or do\n",
      "not) have compared to the BOK policy rate and macroeconomic variables.\n",
      "If we interpret the BOK policy rate as a threshold variable or a latent\n",
      "variable of our indicator of monetary policy sentiment, it would be\n",
      "DSGE models that analyze the effect of monetary policy.\n",
      "indicators based on the minutes help explain the future decision of\n",
      "monetary policy.\n",
      "sentiment indicator before and after the BOK’s announcements, it can be\n",
      "rates in Korea, a text-based indicator would be a decent alternative.\n",
      "indicators to measure macroeconomic uncertainty, public expectation\n",
      "about future monetary policy stance, stock market sentiment, and so on.\n",
      "mining approach can be a useful addition to the BOK’s and researchers’\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "print(summarize(Text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
