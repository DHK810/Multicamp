{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Namsan/NNP)\n",
      "  (PERSON Botanical/NNP Garden/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  well/RB\n",
      "  known/VBN\n",
      "  botanical/JJ\n",
      "  gardenin/NN\n",
      "  (GPE Seoul/NNP)\n",
      "  ,/,\n",
      "  Korea.n/NNP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "text = \"Namsan Botanical Garden is a well known botanical gardenin Seoul, Korea.n\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    chunks = nltk.ne_chunk(tags)\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple chunker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Ravi/NNP)\n",
      "  is/VBZ\n",
      "  (NP the/DT CEO/NNP)\n",
      "  of/IN\n",
      "  (NP a/DT Company/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  very/RB\n",
      "  (NP powerful/JJ public/JJ speaker/NN)\n",
      "  also/RB\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text = \"Ravi is the CEO of a Company. He is very powerful public speaker also.\"\n",
    "grammar = '\\n'.join([\n",
    "    'NP: {<DT>*<NNP>}', #DT : 한정사 , NNP : 고유명사 => DT가 0번 이상 출현\n",
    "    'NP: {<JJ>*<NN>}', #JJ : 형용사 , NN : 명사 => JJ가 0번 이상 출현\n",
    "    'NP: {<NNP>+}',\n",
    "    ])\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    chunkparser = nltk.RegexpParser(grammar)\n",
    "    result = chunkparser.parse(tags)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Chunker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  38.6%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  48.2%%\n",
      "    Precision:     71.1%%\n",
      "    Recall:        17.2%%\n",
      "    F-Measure:     27.7%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  45.0%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  50.7%%\n",
      "    Precision:     51.9%%\n",
      "    Recall:         8.8%%\n",
      "    F-Measure:     15.1%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\15Z970-GA5BK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('conll2000')\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.corpus import treebank_chunk\n",
    "def mySimpleChunker() :\n",
    "    grammar = 'NP: {<NNP>+}'\n",
    "    return nltk.RegexpParser(grammar)\n",
    "def test_nothing(data) : # 정규표현식 없이 베이스라인을 확인\n",
    "    cp = nltk.RegexpParser(\"\")\n",
    "    print(cp.evaluate(data))\n",
    "def test_mysimplechunker(data) : #test_nothing과 비교하는 함수\n",
    "    schunker = mySimpleChunker()\n",
    "    print(schunker.evaluate(data))\n",
    "datasets = [\n",
    "    conll2000.chunked_sents('test.txt', chunk_types=['NP']),\n",
    "    treebank_chunk.chunked_sents()\n",
    "    ]\n",
    "for dataset in datasets :\n",
    "    test_nothing(dataset[:50])\n",
    "    test_mysimplechunker(dataset[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_chunk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NNP Tajmahal) (VBZ is)) (VP (IN in) (NNP Agra)))\n",
      "(S\n",
      "  (NP (NNP Bangalore) (VBZ is))\n",
      "  (VP (DT the) (NN capital) (IN of) (NNP Karnataka)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'B'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-aaf8c091190b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m ]\n\u001b[0;32m     23\u001b[0m \u001b[0mRDParserExample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mRDParserExample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-aaf8c091190b>\u001b[0m in \u001b[0;36mRDParserExample\u001b[1;34m(grammar, textlist)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtextlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\parse\\recursivedescent.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# Start a recursive descent parse, with an initial tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             raise ValueError(\n\u001b[1;32m--> 684\u001b[1;33m                 \u001b[1;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[1;34m\"input words: %r.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m             )\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'B'\"."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def RDParserExample(grammar, textlist):\n",
    "    parser = nltk.parse.RecursiveDescentParser(grammar)\n",
    "    for text in textlist:\n",
    "        sentence = nltk.word_tokenize(text)\n",
    "        for tree in parser.parse(sentence):\n",
    "            print(tree)\n",
    "            tree.draw()\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> NNP VBZ\n",
    "VP -> IN NNP | DT NN IN NNP\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")\n",
    "text = [\n",
    "\"Tajmahal is in Agra\",\n",
    "\"Bangalore is the capital of Karnataka\",\n",
    "]\n",
    "RDParserExample(grammar, text)\n",
    "RDParserExample(grammar, text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 쉬프트 변환 구문 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9일', '뉴욕환시에서', '미국', '달러화는', '일본', '고', '위', '관계자의', '엔', '강세', '용인', '발언으로', '엔화', '.']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'9일', '뉴욕환시에서', '미국', '달러화는', '일본', '고', '위', '관계자의', '엔', '강세', '용인', '발언으로', '엔화', '.'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7618fb73214d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mNN\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'capital'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \"\"\")\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mSRParserExample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-7618fb73214d>\u001b[0m in \u001b[0;36mSRParserExample\u001b[1;34m(grammar, textlist)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\parse\\shiftreduce.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# initialize the stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             raise ValueError(\n\u001b[1;32m--> 684\u001b[1;33m                 \u001b[1;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[1;34m\"input words: %r.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m             )\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'9일', '뉴욕환시에서', '미국', '달러화는', '일본', '고', '위', '관계자의', '엔', '강세', '용인', '발언으로', '엔화', '.'\"."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def SRParserExample(grammar, textlist):\n",
    "    parser = nltk.parse.ShiftReduceParser(grammar)\n",
    "    for text in textlist:\n",
    "        sentence = nltk.word_tokenize(text)\n",
    "        print(sentence)\n",
    "        for tree in parser.parse(sentence):\n",
    "            print(tree)\n",
    "            tree.draw()\n",
    "text = [\n",
    "\"Bangalore is the capital of Karnataka\",\n",
    "\"Tajmahal is in Agra\",\n",
    "]\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> NNP VBZ\n",
    "VP -> IN NNP | DT NN IN NNP\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")\n",
    "SRParserExample(grammar, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의존 문법과 투사 의존성구문분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(yield (savings small) (gains large))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "grammar = nltk.grammar.DependencyGrammar.fromstring(\"\"\"\n",
    "'savings' -> 'small'\n",
    "'yield' -> 'savings'\n",
    "'gains' -> 'large'\n",
    "'yield' -> 'gains'\n",
    "\"\"\")\n",
    "sentence = 'small savings yield large gains'\n",
    "dp = nltk.parse.ProjectiveDependencyParser(grammar)\n",
    "for t in sorted(dp.parse(sentence.split())):\n",
    "    print(t)\n",
    "    t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autopep8\n",
      "  Downloading https://files.pythonhosted.org/packages/45/f3/24b437da561b6af4840c871fbbda32889ca304fc1f7b6cc3ada8b09f394a/autopep8-1.4.4.tar.gz (114kB)\n",
      "Requirement already satisfied: pycodestyle>=2.4.0 in c:\\users\\15z970-ga5bk\\anaconda3\\lib\\site-packages (from autopep8) (2.5.0)\n",
      "Building wheels for collected packages: autopep8\n",
      "  Building wheel for autopep8 (setup.py): started\n",
      "  Building wheel for autopep8 (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\15Z970-GA5BK\\AppData\\Local\\pip\\Cache\\wheels\\7e\\f5\\4b\\c19e6276126325eb8071b273347c05a830c37a82b9b3b81510\n",
      "Successfully built autopep8\n",
      "Installing collected packages: autopep8\n",
      "Successfully installed autopep8-1.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autopep8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차트구문분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Bangal.  is  . the  .capita.  of  .Karnat.|\n",
      "|[------]      .      .      .      .      .| [0:1] 'Bangalore'\n",
      "|.      [------]      .      .      .      .| [1:2] 'is'\n",
      "|.      .      [------]      .      .      .| [2:3] 'the'\n",
      "|.      .      .      [------]      .      .| [3:4] 'capital'\n",
      "|.      .      .      .      [------]      .| [4:5] 'of'\n",
      "|.      .      .      .      .      [------]| [5:6] 'Karnataka'\n",
      "|[------]      .      .      .      .      .| [0:1] NNP -> 'Bangalore' *\n",
      "|[------>      .      .      .      .      .| [0:1] T1 -> NNP * VBZ\n",
      "|.      [------]      .      .      .      .| [1:2] VBZ -> 'is' *\n",
      "|[-------------]      .      .      .      .| [0:2] T1 -> NNP VBZ *\n",
      "|[------------->      .      .      .      .| [0:2] S  -> T1 * T4\n",
      "|.      .      [------]      .      .      .| [2:3] DT -> 'the' *\n",
      "|.      .      [------>      .      .      .| [2:3] T2 -> DT * NN\n",
      "|.      .      .      [------]      .      .| [3:4] NN -> 'capital' *\n",
      "|.      .      [-------------]      .      .| [2:4] T2 -> DT NN *\n",
      "|.      .      [------------->      .      .| [2:4] T4 -> T2 * T3\n",
      "|.      .      .      .      [------]      .| [4:5] IN -> 'of' *\n",
      "|.      .      .      .      [------>      .| [4:5] T3 -> IN * NNP\n",
      "|.      .      .      .      .      [------]| [5:6] NNP -> 'Karnataka' *\n",
      "|.      .      .      .      .      [------>| [5:6] T1 -> NNP * VBZ\n",
      "|.      .      .      .      [-------------]| [4:6] T3 -> IN NNP *\n",
      "|.      .      .      .      [-------------]| [4:6] T4 -> T3 *\n",
      "|.      .      [---------------------------]| [2:6] T4 -> T2 T3 *\n",
      "|[=========================================]| [0:6] S  -> T1 T4 *\n",
      "Total Edges : 24\n",
      "(S\n",
      "  (T1 (NNP Bangalore) (VBZ is))\n",
      "  (T4 (T2 (DT the) (NN capital)) (T3 (IN of) (NNP Karnataka))))\n"
     ]
    }
   ],
   "source": [
    "from nltk.grammar import CFG\n",
    "from nltk.parse.chart import ChartParser, BU_LC_STRATEGY\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> T1 T4\n",
    "T1 -> NNP VBZ\n",
    "T2 -> DT NN\n",
    "T3 -> IN NNP\n",
    "T4 -> T3 | T2 T3\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")\n",
    "cp = ChartParser(grammar, BU_LC_STRATEGY, trace=True)\n",
    "sentence = \"Bangalore is the capital of Karnataka\"\n",
    "tokens = sentence.split()\n",
    "chart = cp.chart_parse(tokens)\n",
    "parses = list(chart.parses(grammar.start()))\n",
    "print(\"Total Edges :\", len(chart.edges()))\n",
    "for tree in parses: print(tree)\n",
    "tree.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
