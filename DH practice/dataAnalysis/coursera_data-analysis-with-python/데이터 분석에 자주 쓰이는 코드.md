# 데이터 분석에 자주 쓰이는 코드

### 변수 간 상관관계 파악

```
df[[]].corr()
```

### 변수 간 선형회귀분석

```
sns.regplot(x="engine-size", y="price", data=df)
plt.ylim(0,)
```

### 카테고리 변수들n

```
sns.boxplot(x="body-style", y="price", data=df)
```

### pivot table

```
grouped_pivot = grouped_test1.pivot(index='drive-wheels',columns='body-style')
grouped_pivot
```

### 독립변수와 종속변수간의 상관분석

```
from scipy import stats
pearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)  
```

### 회기그래프 & residual 그래프

```
sns.regplot(x = 'highway-mpg', y = 'price', data=df)
plt.ylim(0,)


sns.residplot(df['highway-mpg'], df['price'])
```

### binning

```
bins = np.linspace(min(df["horsepower"]), max(df["horsepower"]), 4)
group_names = ['Low', 'Medium', 'High']

df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )
df[['horsepower','horsepower-binned']].head(20)

%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, df["horsepower-binned"].value_counts())

# set x/y labels and plot title
plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")

# bin visualization
%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot

a = (0,1,2)

# draw historgram of attribute "horsepower" with bins = 3
plt.pyplot.hist(df["horsepower"], bins = 3)

# set x/y labels and plot title
plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")
```

### one-hot-encoder

```
dummy_variable_1 = pd.get_dummies(df["fuel-type"])
```

# Model Development

### Linear Regression

```
from sklearn.linear_model import LinearRegression

# linearregression 객체화
lm = LinearRegression()

# 변수 선언
X = df[['highway-mpg']]
Y = df['price']

# 최적의 그래프 그리기
lm.fit(X,Y)

# 예측
Yhat=lm.predict(X)
Yhat[0:5]  

#y 절편값
lm.intercept_

# slope값
lm.coef_
```

### Multiple Linear Regression

```
Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]

lm.fit(Z, df['price'])

lm.intercept_

lm.coef_
```

### Model Evaluation using Visualization

### Regression plot

- scatter plot + fitted linear regression line

```
width = 12
height = 10
plt.figure(figsize=(width, height))
sns.regplot(x="highway-mpg", y="price", data=df)
plt.ylim(0,)
```

### Residual plot

- The difference between the observed value (y) and the predicted value (Yhat) is called the residual (e)
- If the points in a residual plot are **randomly spread out around the x-axis**, then a **linear model is appropriate** for the data. Why is that? Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.

### Multiple Linear Regression

```
Y_hat = lm.predict(Z)

plt.figure(figsize=(width, height))

# actual price
ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")

# predicted values
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values" , ax=ax1)


plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price (in dollars)')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()
```

### Polynomial Regression

```
from sklearn.preprocessing import PolynomialFeatures

pr=PolynomialFeatures(degree=2)

Z_pr=pr.fit_transform(Z)

Z.shape
Z_pr.shape
```

## Evaluation of accuracy of the model

### R-squared

- R squared, also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line.

- ```
  #highway_mpg_fit
  lm.fit(X, Y)
  # Find the R^2
  print('The R-square is: ', lm.score(X, Y))
  ```

### Mean Squared Error (MSE)

-  the difference between actual value (y) and the estimated value (ŷ)

- ```
  from sklearn.metrics import mean_squared_error
  
  Y_pred =lm.predict(X)
  print('The output of the first four predicted value is: ', Y_pred[0:4])
  
  mse = mean_squared_error(df['price'], Y_pred)
  print('The mean square error of price and predicted value is: ', mse)
  ```

- 























